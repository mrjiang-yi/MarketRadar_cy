# scrape_economy_selenium.py
# -----------------------------------------------------------------------------
# DeepSeek Finance Project - Macro Data Scraper (Concurrent Version)
# åŠŸèƒ½æè¿°:
# 1. ä½¿ç”¨ Selenium (Headless Chrome) å¹¶å‘æŠ“å–ä¸œæ–¹è´¢å¯Œç½‘çš„å¤šé¡¹å®è§‚ç»æµæ•°æ®ã€‚
# 2. [å¯¹å¤–æ¥å£] æä¾› get_macro_data() ä¾› MarketRadar ä¸»ç¨‹åºè°ƒç”¨ã€‚
# 3. [ç¨³å®šæ€§] å¢åŠ é‡è¯•æœºåˆ¶ï¼šå•ä¸ªä»»åŠ¡å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•5æ¬¡ã€‚
# 4. [å®šåˆ¶é€»è¾‘] é’ˆå¯¹â€œä¸­å›½_å—å‘èµ„é‡‘â€ä»…è·å–è¿‘30å¤©æ•°æ®ï¼›å…¶ä»–æ•°æ®ä¿æŒè¿‘180å¤©ã€‚
# 5. [ä¿®å¤] å¢åŠ é¡µé¢åŠ è½½è¶…æ—¶é™åˆ¶(45s)å’Œé™ä½å¹¶å‘æ•°(2)ã€‚
# 6. [ä¿®å¤] ä¿®å¤å—å‘èµ„é‡‘ .dt æŠ¥é”™ã€‚
# 7. [æ–°å¢] è¿”å›è¯¦ç»†çŠ¶æ€æ—¥å¿—ä¾›ä¸»ç¨‹åºç”Ÿæˆ Log æ–‡ä»¶ã€‚
# -----------------------------------------------------------------------------

import time
import json
import pandas as pd
import datetime
import os
import numpy as np
from io import StringIO
from concurrent.futures import ThreadPoolExecutor, as_completed

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class MacroDataScraper:
    def __init__(self):
        # ç›®æ ‡æ•°æ®æºé…ç½®
        self.targets = {
            "ä¸­å›½_CPI": "https://data.eastmoney.com/cjsj/cpi.html",
            "ä¸­å›½_PMI": "https://data.eastmoney.com/cjsj/pmi.html",
            "ä¸­å›½_PPI": "https://data.eastmoney.com/cjsj/ppi.html",
            "ä¸­å›½_è´§å¸ä¾›åº”é‡": "https://data.eastmoney.com/cjsj/hbgyl.html",
            "ä¸­å›½_LPR": "https://data.eastmoney.com/cjsj/globalRateLPR.html",
            "ä¸­å›½_å—å‘èµ„é‡‘": "https://data.eastmoney.com/hsgtV2/hsgtDetail/scgkDetail_nx.html", 
            "ç¾å›½_ISMåˆ¶é€ ä¸šPMI": "https://data.eastmoney.com/cjsj/foreign_0_0.html",
            "ç¾å›½_ISMéåˆ¶é€ ä¸šæŒ‡æ•°": "https://data.eastmoney.com/cjsj/foreign_0_1.html",
            "ç¾å›½_éå†œå°±ä¸š": "https://data.eastmoney.com/cjsj/foreign_0_2.html",
            "ç¾å›½_æ ¸å¿ƒé›¶å”®é”€å”®æœˆç‡": "https://data.eastmoney.com/cjsj/foreign_0_9.html",
            "ç¾å›½_åˆ©ç‡å†³è®®": "https://data.eastmoney.com/cjsj/foreign_8_0.html",
            "æ—¥æœ¬_å¤®è¡Œåˆ©ç‡å†³è®®": "https://data.eastmoney.com/cjsj/foreign_3_0.html",
        }

        # è¾“å‡ºç»“æ„æ˜ å°„è¡¨
        self.key_mapping = {
            "ä¸­å›½_CPI": ("china", "CPI"),
            "ä¸­å›½_PMI": ("china", "PMI_åˆ¶é€ ä¸š"),
            "ä¸­å›½_PPI": ("china", "PPI"),
            "ä¸­å›½_è´§å¸ä¾›åº”é‡": ("china", "è´§å¸ä¾›åº”é‡"),
            "ä¸­å›½_LPR": ("china", "LPR"),
            "ä¸­å›½_å—å‘èµ„é‡‘": ("china", "å—å‘èµ„é‡‘å‡€æµå…¥"), 
            "ç¾å›½_ISMåˆ¶é€ ä¸šPMI": ("usa", "ISM_åˆ¶é€ ä¸šPMI"),
            "ç¾å›½_ISMéåˆ¶é€ ä¸šæŒ‡æ•°": ("usa", "ISM_éåˆ¶é€ ä¸šPMI"),
            "ç¾å›½_éå†œå°±ä¸š": ("usa", "éå†œå°±ä¸šäººæ•°"),
            "ç¾å›½_æ ¸å¿ƒé›¶å”®é”€å”®æœˆç‡": ("usa", "é›¶å”®é”€å”®æœˆç‡"),
            "ç¾å›½_åˆ©ç‡å†³è®®": ("usa", "åˆ©ç‡å†³è®®"),
            "æ—¥æœ¬_å¤®è¡Œåˆ©ç‡å†³è®®": ("japan", "å¤®è¡Œåˆ©ç‡")
        }
        
        # ç»“æœå­˜å‚¨å­—å…¸
        self.results = {}
        self.status_logs = []
        
        # Chrome æµè§ˆå™¨é…ç½®
        self.chrome_options = Options()
        self.chrome_options.add_argument("--headless")
        self.chrome_options.add_argument("--disable-gpu")
        self.chrome_options.add_argument("--log-level=3")
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36')
        
        # ç‹¬ç«‹è¿è¡Œæ—¶è¾“å‡ºæ–‡ä»¶è·¯å¾„
        self.output_path = "OnlineReport.json"

    def clean_date(self, date_str):
        try:
            date_str = str(date_str).strip()
            if "å¹´" in date_str and "æœˆ" in date_str:
                clean_str = date_str.replace("æœˆä»½", "").replace("æœˆ", "").replace("å¹´", "-")
                if clean_str.count("-") == 1:
                    clean_str += "-01"
                return pd.to_datetime(clean_str)
            return pd.to_datetime(date_str)
        except Exception:
            return pd.NaT

    def fetch_single_source(self, name, url):
        max_retries = 5
        # å—å‘èµ„é‡‘ä»…éœ€30å¤©ï¼Œå…¶ä»–æ•°æ®ä¿æŒ180å¤©
        days_to_keep = 30 if "å—å‘èµ„é‡‘" in name else 180
        last_error = None

        for attempt in range(1, max_retries + 1):
            print(f"ğŸŒ [{name}] ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯• (Selenium)...")
            driver = None
            try:
                driver = webdriver.Chrome(options=self.chrome_options)
                
                # å¼ºåˆ¶è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢é¡µé¢åŠ è½½å¡æ­»
                driver.set_page_load_timeout(45) 
                driver.set_script_timeout(45)
                
                driver.get(url)
                # ç­‰å¾…è¡¨æ ¼åŠ è½½
                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "table")))
                html = driver.page_source
                dfs = pd.read_html(StringIO(html))
                
                if not dfs:
                    raise ValueError("é¡µé¢è§£æä¸ºç©ºï¼Œæœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®")

                df = max(dfs, key=lambda x: len(x))
                
                # å¤„ç† MultiIndex
                if isinstance(df.columns, pd.MultiIndex):
                    new_cols = []
                    for col in df.columns:
                        valid_parts = [str(c) for c in col if "Unnamed" not in str(c) and str(c).strip() != ""]
                        seen = set()
                        unique_parts = [x for x in valid_parts if not (x in seen or seen.add(x))]
                        new_cols.append("".join(unique_parts))
                    df.columns = new_cols
                
                df.columns = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                
                possible_date_cols = ['æœˆä»½', 'æ—¶é—´', 'æ—¥æœŸ', 'å‘å¸ƒæ—¥æœŸ', 'å…¬å¸ƒæ—¥æœŸ']
                date_col = next((col for col in df.columns if any(x in str(col) for x in possible_date_cols)), None)
                
                if date_col:
                    df['_std_date'] = df[date_col].apply(self.clean_date)
                    df = df.dropna(subset=['_std_date'])
                    
                    # [ä¿®å¤] å¼ºåˆ¶è½¬æ¢ä¸º datetime ç±»å‹
                    df['_std_date'] = pd.to_datetime(df['_std_date'])
                    
                    cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days_to_keep)
                    df = df[df['_std_date'] >= cutoff_date]
                    
                    df['_std_date'] = df['_std_date'].dt.strftime('%Y-%m-%d')
                    df = df.replace({'-': None, 'nan': None})
                    df = df.where(pd.notnull(df), None)

                    if name == "ä¸­å›½_å—å‘èµ„é‡‘":
                        keep_cols = ['_std_date']
                        for c in df.columns:
                            if "å‡€ä¹°é¢" in c and "å½“æ—¥" in c:
                                keep_cols.append(c)
                            elif "æˆäº¤ç¬”æ•°" in c:
                                keep_cols.append(c)
                        df = df[keep_cols]
                        df.rename(columns={'_std_date': 'æ—¥æœŸ'}, inplace=True)
                    
                    # ç»Ÿä¸€å¢åŠ  'æ—¥æœŸ' å­—æ®µç”¨äºåˆå¹¶
                    if 'æ—¥æœŸ' not in df.columns and '_std_date' in df.columns:
                        df['æ—¥æœŸ'] = df['_std_date']

                    records = df.to_dict('records')
                    print(f"âœ… [{name}] æŠ“å–æˆåŠŸ! è·å¾— {len(records)} æ¡è®°å½•")
                    return name, records, None # Success
                else:
                    raise ValueError(f"æœªæ‰¾åˆ°æ—¥æœŸåˆ—: {df.columns.tolist()}")

            except Exception as e:
                last_error = str(e)
                print(f"âŒ [{name}] å¤±è´¥: {last_error[:200]}") 
                if attempt < max_retries:
                    time.sleep(2)
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        # æœ€ç»ˆå¤±è´¥
        return name, [], last_error

    def run_concurrent(self):
        print("ğŸš€ [Scraper] æ­£åœ¨å¹¶å‘æŠ“å–å®è§‚æ•°æ® (Workers=2)...")
        # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—
        self.status_logs = []
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_name = {
                executor.submit(self.fetch_single_source, name, url): name 
                for name, url in self.targets.items()
            }
            for future in as_completed(future_to_name):
                name, data, error_msg = future.result()
                if not error_msg:
                    self.results[name] = data
                    self.status_logs.append({'name': name, 'status': True, 'error': None})
                else:
                    self.results[name] = []
                    self.status_logs.append({'name': name, 'status': False, 'error': error_msg})
                    
        return self.results, self.status_logs

    def organize_data(self):
        """
        å°†æ‰å¹³çš„ results è½¬æ¢ä¸ºåµŒå¥—çš„å­—å…¸ç»“æ„
        """
        nested_data = {
            "china": {},
            "usa": {},
            "japan": {}
        }
        
        for old_key, data_list in self.results.items():
            if not data_list:
                continue
            if old_key in self.key_mapping:
                country_key, metric_key = self.key_mapping[old_key]
                nested_data[country_key][metric_key] = data_list
        
        return nested_data

    def get_data_dict(self):
        """
        å¯¹å¤–ä¸»æ¥å£ï¼šè¿è¡ŒæŠ“å–å¹¶è¿”å› (data_dict, status_logs)
        """
        self.run_concurrent()
        return self.organize_data(), self.status_logs

    def save_custom_json(self):
        data, _ = self.get_data_dict() # ç‹¬ç«‹è¿è¡Œæ—¶å¿½ç•¥æ—¥å¿—è¿”å›
        try:
            with open(self.output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ ç‹¬ç«‹è¿è¡Œæ•°æ®å·²å†™å…¥: {self.output_path}")
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

# å¯¹å¤–æš´éœ²çš„ä¾¿æ·å‡½æ•°
def get_macro_data():
    scraper = MacroDataScraper()
    return scraper.get_data_dict()

if __name__ == "__main__":
    scraper = MacroDataScraper()
    scraper.save_custom_json()