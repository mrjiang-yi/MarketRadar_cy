# scrape_economy_selenium.py
# -----------------------------------------------------------------------------
# DeepSeek Finance Project - Macro Data Scraper (Concurrent Version)
# åŠŸèƒ½æè¿°:
# 1. ä½¿ç”¨ Selenium (Headless Chrome) å¹¶å‘æŠ“å–ä¸œæ–¹è´¢å¯Œç½‘åŠ Investing.com çš„æ•°æ®ã€‚
# 2. [å¯¹å¤–æ¥å£] æä¾› get_macro_data() ä¾› MarketRadar ä¸»ç¨‹åºè°ƒç”¨ã€‚
# 3. [ç¨³å®šæ€§] å¢åŠ é‡è¯•æœºåˆ¶ï¼šå•ä¸ªä»»åŠ¡å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•5æ¬¡ã€‚
# 4. [å®šåˆ¶é€»è¾‘] é’ˆå¯¹â€œä¸­å›½_å—å‘èµ„é‡‘â€ä»…è·å–è¿‘30å¤©æ•°æ®ï¼›å…¶ä»–æ•°æ®ä¿æŒè¿‘180å¤©ã€‚
# 5. [æ–°å¢] æ”¯æŒ Investing.com æ•°æ®æ ¼å¼æ¸…æ´—ï¼ˆå¤„ç† K/M äº¤æ˜“é‡å•ä½åŠä¸­æ–‡æ—¥æœŸï¼‰ã€‚
# 6. [åçˆ¬] å¢åŠ é˜²æ£€æµ‹å‚æ•°ä»¥åº”å¯¹ Investing.comã€‚
# 7. [ä¿®å¤] è§£å†³ Investing.com å¡æ­»é—®é¢˜ï¼šé‡‡ç”¨ eager åŠ è½½ç­–ç•¥ + ç¦ç”¨å›¾ç‰‡ã€‚
# -----------------------------------------------------------------------------

import time
import json
import pandas as pd
import datetime
import os
import re
import numpy as np
from io import StringIO
from concurrent.futures import ThreadPoolExecutor, as_completed

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class MacroDataScraper:
    def __init__(self):
        # ç›®æ ‡æ•°æ®æºé…ç½®
        self.targets = {
            "ä¸­å›½_CPI": "https://data.eastmoney.com/cjsj/cpi.html",
            "ä¸­å›½_PMI": "https://data.eastmoney.com/cjsj/pmi.html",
            "ä¸­å›½_PPI": "https://data.eastmoney.com/cjsj/ppi.html",
            "ä¸­å›½_è´§å¸ä¾›åº”é‡": "https://data.eastmoney.com/cjsj/hbgyl.html",
            "ä¸­å›½_LPR": "https://data.eastmoney.com/cjsj/globalRateLPR.html",
            "ä¸­å›½_å—å‘èµ„é‡‘": "https://data.eastmoney.com/hsgtV2/hsgtDetail/scgkDetail_nx.html", 
            "ç¾å›½_ISMåˆ¶é€ ä¸šPMI": "https://data.eastmoney.com/cjsj/foreign_0_0.html",
            "ç¾å›½_ISMéåˆ¶é€ ä¸šæŒ‡æ•°": "https://data.eastmoney.com/cjsj/foreign_0_1.html",
            "ç¾å›½_éå†œå°±ä¸š": "https://data.eastmoney.com/cjsj/foreign_0_2.html",
            "ç¾å›½_æ ¸å¿ƒé›¶å”®é”€å”®æœˆç‡": "https://data.eastmoney.com/cjsj/foreign_0_9.html",
            "ç¾å›½_åˆ©ç‡å†³è®®": "https://data.eastmoney.com/cjsj/foreign_8_0.html",
            "æ—¥æœ¬_å¤®è¡Œåˆ©ç‡å†³è®®": "https://data.eastmoney.com/cjsj/foreign_3_0.html",
            # [æ–°å¢] Investing.com æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°
            "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°": "https://cn.investing.com/indices/hang-seng-healthcare-historical-data"
        }

        # è¾“å‡ºç»“æ„æ˜ å°„è¡¨
        self.key_mapping = {
            "ä¸­å›½_CPI": ("china", "CPI"),
            "ä¸­å›½_PMI": ("china", "PMI_åˆ¶é€ ä¸š"),
            "ä¸­å›½_PPI": ("china", "PPI"),
            "ä¸­å›½_è´§å¸ä¾›åº”é‡": ("china", "è´§å¸ä¾›åº”é‡"),
            "ä¸­å›½_LPR": ("china", "LPR"),
            "ä¸­å›½_å—å‘èµ„é‡‘": ("china", "å—å‘èµ„é‡‘å‡€æµå…¥"), 
            "ç¾å›½_ISMåˆ¶é€ ä¸šPMI": ("usa", "ISM_åˆ¶é€ ä¸šPMI"),
            "ç¾å›½_ISMéåˆ¶é€ ä¸šæŒ‡æ•°": ("usa", "ISM_éåˆ¶é€ ä¸šPMI"),
            "ç¾å›½_éå†œå°±ä¸š": ("usa", "éå†œå°±ä¸šäººæ•°"),
            "ç¾å›½_æ ¸å¿ƒé›¶å”®é”€å”®æœˆç‡": ("usa", "é›¶å”®é”€å”®æœˆç‡"),
            "ç¾å›½_åˆ©ç‡å†³è®®": ("usa", "åˆ©ç‡å†³è®®"),
            "æ—¥æœ¬_å¤®è¡Œåˆ©ç‡å†³è®®": ("japan", "å¤®è¡Œåˆ©ç‡"),
            "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°": ("hk", "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°")
        }
        
        # ç»“æœå­˜å‚¨å­—å…¸
        self.results = {}
        self.status_logs = []
        
        # Chrome æµè§ˆå™¨é…ç½®
        self.chrome_options = Options()
        self.chrome_options.add_argument("--headless")
        self.chrome_options.add_argument("--disable-gpu")
        self.chrome_options.add_argument("--log-level=3")
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled") # é˜²æ­¢è¢«è¯†åˆ«ä¸ºè‡ªåŠ¨åŒ–
        self.chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36')
        
        # [å…³é”®ä¿®å¤] è®¾ç½®é¡µé¢åŠ è½½ç­–ç•¥ä¸º 'eager'
        # 'normal': ç­‰å¾…æ‰€æœ‰èµ„æºï¼ˆcss, images, scriptsï¼‰åŠ è½½å®Œæˆ -> ä¼šå¡æ­»
        # 'eager': DOM è§£æå®Œå°±ç»§ç»­ -> æå¤§æå‡é€Ÿåº¦å¹¶é˜²æ­¢å› å¹¿å‘Šè„šæœ¬å¡æ­»
        self.chrome_options.page_load_strategy = 'eager'

        # [å…³é”®ä¿®å¤] ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œè¿›ä¸€æ­¥æé€Ÿ
        prefs = {"profile.managed_default_content_settings.images": 2}
        self.chrome_options.add_experimental_option("prefs", prefs)
        
        # ç‹¬ç«‹è¿è¡Œæ—¶è¾“å‡ºæ–‡ä»¶è·¯å¾„
        self.output_path = "OnlineReport.json"

    def clean_date(self, date_str):
        """
        [åŸå‡½æ•°ä¿ç•™] æ¸…æ´—æ—¥æœŸæ ¼å¼ï¼Œä¸»è¦ç”¨äºä¸œæ–¹è´¢å¯Œç½‘æ•°æ®
        """
        try:
            date_str = str(date_str).strip()
            # ç§»é™¤ 'æ—¥', 'æœˆä»½', 'æœˆ'ï¼Œå°† 'å¹´' æ›¿æ¢ä¸º '-'
            if "å¹´" in date_str:
                clean_str = date_str.replace("æœˆä»½", "").replace("æœˆ", "").replace("æ—¥", "").replace("å¹´", "-")
                # å¤„ç†ç±»ä¼¼ "2023-5" è¿™ç§åªæœ‰å¹´æœˆçš„æƒ…å†µï¼Œè¡¥å…¨ä¸º1å·
                if clean_str.count("-") == 1:
                    clean_str += "-01"
                return pd.to_datetime(clean_str)
            return pd.to_datetime(date_str)
        except Exception:
            return pd.NaT

    def _clean_investing_date(self, date_str):
        """
        [æ–°å¢ä¸“ç”¨å‡½æ•°] ä¸“é—¨ç”¨äºæ¸…æ´— Investing.com çš„ä¸­æ–‡æ—¥æœŸæ ¼å¼
        æ”¯æŒ: '2026å¹´01æœˆ02æ—¥' -> '2026-01-02'
        ä¿®å¤äº†åŸ clean_date å¯èƒ½ä¼šé”™è¯¯åˆ é™¤â€˜æœˆâ€™å­—å¯¼è‡´æ ¼å¼é”™è¯¯çš„é—®é¢˜
        """
        try:
            date_str = str(date_str).strip()
            if "å¹´" in date_str:
                # å…³é”®ä¿®å¤: å°† 'æœˆ' æ›¿æ¢ä¸º '-' è€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                clean_str = date_str.replace("å¹´", "-").replace("æœˆä»½", "").replace("æœˆ", "-").replace("æ—¥", "")
                
                # å¤„ç†å¯èƒ½å‡ºç°çš„å¤šä½™æ¨ªæ 
                clean_str = re.sub(r'-+', '-', clean_str)
                
                if clean_str.count("-") == 1:
                    clean_str += "-01"
                return pd.to_datetime(clean_str)
            return pd.to_datetime(date_str)
        except Exception:
            return pd.NaT

    def parse_volume(self, vol_str):
        """
        è§£æå¸¦å•ä½çš„äº¤æ˜“é‡ (e.g., '4.00K', '618.89M')
        """
        if not isinstance(vol_str, str):
            return vol_str
        
        vol_str = vol_str.upper().strip()
        if vol_str in ['-', '', 'NAN', 'NONE']:
            return 0.0
            
        multiplier = 1.0
        if 'K' in vol_str:
            multiplier = 1000.0
            vol_str = vol_str.replace('K', '')
        elif 'M' in vol_str:
            multiplier = 1000000.0
            vol_str = vol_str.replace('M', '')
        elif 'B' in vol_str:
            multiplier = 1000000000.0
            vol_str = vol_str.replace('B', '')
            
        try:
            return float(vol_str) * multiplier
        except:
            return 0.0

    def parse_percentage(self, pct_str):
        """
        è§£æç™¾åˆ†æ¯”å­—ç¬¦ä¸²
        """
        if not isinstance(pct_str, str):
            return pct_str
        try:
            return float(pct_str.replace('%', '').replace(',', ''))
        except:
            return 0.0

    def fetch_investing_source(self, name, url):
        """
        [æ–°å¢ç‹¬ç«‹å‡½æ•°] ä¸“é—¨æŠ“å– Investing.com æ•°æ®
        åŒ…å«ç‹¬ç«‹çš„è¶…æ—¶è®¾ç½®ã€æ—¥æœŸè§£æé€»è¾‘å’Œè¡¨æ ¼å®šä½é€»è¾‘ï¼Œäº’ä¸å½±å“ã€‚
        """
        max_retries = 5
        last_error = None
        
        # æ’ç”ŸæŒ‡æ•°å¯èƒ½éœ€è¦æ›´é•¿çš„æ—¶é—´æ¥åŠ è½½å¹¿å‘Šè„šæœ¬ï¼ˆå³ä½¿æ˜¯ eager æ¨¡å¼ï¼‰
        # è¿™é‡Œä½¿ç”¨ç‹¬ç«‹çš„è¶…æ—¶è®¾ç½®
        
        for attempt in range(1, max_retries + 1):
            print(f"ğŸŒ [{name}] ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯• (Selenium - Investingä¸“çº¿)...")
            driver = None
            try:
                driver = webdriver.Chrome(options=self.chrome_options)
                
                # åçˆ¬è™«å¤„ç†
                driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                    "source": """Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"""
                })

                # [é’ˆå¯¹æ€§è°ƒæ•´] Investing.com æœ‰æ—¶å“åº”è¾ƒæ…¢ï¼Œç»™äºˆæ›´å®½å®¹çš„è¶…æ—¶æ—¶é—´ (60s)
                driver.set_page_load_timeout(60)
                driver.set_script_timeout(60)
                
                driver.get(url)
                
                # ç­‰å¾…è¡¨æ ¼åŠ è½½
                try:
                    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "table")))
                except:
                    print(f"âš ï¸ [{name}] ç­‰å¾…è¡¨æ ¼è¶…æ—¶ï¼Œå°è¯•ç»§ç»­è§£ææºç ...")
                
                html = driver.page_source
                dfs = pd.read_html(StringIO(html))
                
                if not dfs:
                    raise ValueError("é¡µé¢è§£æä¸ºç©ºï¼Œæœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®")

                # å®šä½ Investing è¡¨æ ¼ (å¿…é¡»åŒ…å« æ—¥æœŸ/æ”¶ç›˜/äº¤æ˜“é‡)
                target_df = None
                for df in dfs:
                    # ä¸´æ—¶æ¸…æ´—åˆ—åä»¥ä¾¿åŒ¹é…
                    cols = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                    if all(k in cols for k in ['æ—¥æœŸ', 'æ”¶ç›˜', 'äº¤æ˜“é‡']):
                        df.columns = cols # åº”ç”¨æ¸…æ´—åçš„åˆ—å
                        target_df = df
                        break
                
                if target_df is None:
                    # å¤‡é€‰æ–¹æ¡ˆï¼šæŒ‰åˆ—ç‰¹å¾åŒ¹é…
                    for df in dfs:
                        cols = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                        if 'æ—¥æœŸ' in cols and 'æ”¶ç›˜' in cols:
                            df.columns = cols
                            target_df = df
                            break

                if target_df is None:
                     raise ValueError(f"æœªæ‰¾åˆ°ç¬¦åˆ Investing æ ¼å¼çš„è¡¨æ ¼ (æ£€æŸ¥åˆ—å: æ—¥æœŸ/æ”¶ç›˜/äº¤æ˜“é‡)")

                df = target_df.copy()
                
                # ä½¿ç”¨ä¸“ç”¨æ—¥æœŸæ¸…æ´—å‡½æ•°
                df['_std_date'] = df['æ—¥æœŸ'].apply(self._clean_investing_date)
                
                # æ£€æŸ¥æ˜¯å¦è§£æå¤±è´¥
                if df['_std_date'].isna().all() and not df.empty:
                    print(f"âŒ [{name}] æ—¥æœŸè§£æå…¨éƒ¨å¤±è´¥! åŸå§‹æ•°æ®ç¤ºä¾‹: {df['æ—¥æœŸ'].iloc[0]}")
                
                df = df.dropna(subset=['_std_date'])
                df['_std_date'] = pd.to_datetime(df['_std_date'])
                
                # è¿‡æ»¤æ—¥æœŸ (ä¿ç•™180å¤©ï¼Œè™½ç„¶å—å‘èµ„é‡‘æ˜¯30å¤©ï¼Œä½†æ’ç”ŸæŒ‡æ•°å»ºè®®ä¿ç•™å¤šä¸€ç‚¹ä»¥è®¡ç®—å‡çº¿)
                cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=180)
                df = df[df['_std_date'] >= cutoff_date]
                
                df['_std_date'] = df['_std_date'].dt.strftime('%Y-%m-%d')
                
                # é‡å‘½åæ˜ å°„
                rename_map = {
                    'æ—¥æœŸ': 'æ—¥æœŸ', 
                    'æ”¶ç›˜': 'close', 
                    'å¼€ç›˜': 'open',
                    'é«˜': 'high', 
                    'ä½': 'low', 
                    'äº¤æ˜“é‡': 'volume', 
                    'æ¶¨è·Œå¹…': 'change_pct'
                }
                
                # æ•°æ®æ¸…æ´—
                if 'volume' in df.columns:
                     # Investing çš„ volume åˆ—åœ¨æ˜ å°„å‰æ˜¯ 'äº¤æ˜“é‡'
                     pass 
                
                # å…ˆé‡å‘½å
                available_map = {k: v for k, v in rename_map.items() if k in df.columns}
                df = df.rename(columns=available_map)
                
                # æ•°å€¼å¤„ç†
                if 'volume' in df.columns:
                    df['volume'] = df['volume'].apply(self.parse_volume)
                
                for col in ['close', 'open', 'high', 'low']:
                    if col in df.columns:
                        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        
                if 'change_pct' in df.columns:
                    df['change_pct'] = df['change_pct'].apply(self.parse_percentage)

                # æ„é€ æœ€ç»ˆåˆ—
                keep_cols = ['_std_date'] + list(available_map.values())
                keep_cols = list(dict.fromkeys(keep_cols)) # å»é‡
                final_cols = [c for c in keep_cols if c in df.columns]
                
                df = df[final_cols]
                df.rename(columns={'_std_date': 'æ—¥æœŸ'}, inplace=True)
                
                records = df.to_dict('records')
                print(f"âœ… [{name}] æŠ“å–æˆåŠŸ! è·å¾— {len(records)} æ¡è®°å½•")
                return name, records, None 

            except Exception as e:
                last_error = str(e)
                print(f"âŒ [{name}] å¤±è´¥: {str(e)[:100]}")
                if attempt < max_retries:
                    time.sleep(2)
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        return name, [], last_error

    def fetch_single_source(self, name, url):
        """
        é€šç”¨æŠ“å–å…¥å£
        """
        # [åˆ†æµé€»è¾‘] å¦‚æœæ˜¯æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°ï¼Œè½¬äº¤ä¸“ç”¨å‡½æ•°å¤„ç†
        if name == "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°":
            return self.fetch_investing_source(name, url)

        max_retries = 5
        # å—å‘èµ„é‡‘ä»…éœ€30å¤©ï¼Œå…¶ä»–æ•°æ®ä¿æŒ180å¤©
        days_to_keep = 30 if "å—å‘èµ„é‡‘" in name else 180
        last_error = None

        for attempt in range(1, max_retries + 1):
            print(f"ğŸŒ [{name}] ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯• (Selenium)...")
            driver = None
            try:
                driver = webdriver.Chrome(options=self.chrome_options)
                
                # [å…³é”®ä¿®å¤] CDP å‘½ä»¤ï¼šåœ¨é¡µé¢åŠ è½½å‰ç§»é™¤ navigator.webdriver æ ‡å¿—
                driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                    "source": """
                        Object.defineProperty(navigator, 'webdriver', {
                            get: () => undefined
                        })
                    """
                })

                # å¼ºåˆ¶è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢é¡µé¢åŠ è½½å¡æ­»
                driver.set_page_load_timeout(30) # ç¼©çŸ­è¶…æ—¶ï¼Œå› ä¸ºç”¨äº† eager æ¨¡å¼ï¼Œåº”è¯¥å¾ˆå¿«
                driver.set_script_timeout(30)
                
                driver.get(url)
                
                # ç­‰å¾…è¡¨æ ¼åŠ è½½ (å…³é”®)
                try:
                    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "table")))
                except Exception:
                    # å¦‚æœæ‰¾ä¸åˆ°è¡¨æ ¼ï¼Œå¯èƒ½æ˜¯åçˆ¬éªŒè¯ï¼Œæˆ–è€…æ˜¯é¡µé¢ç»“æ„å˜äº†
                    print(f"âš ï¸ [{name}] ç­‰å¾…è¡¨æ ¼è¶…æ—¶ï¼Œå°è¯•ç»§ç»­è§£ææºç ...")
                
                # è·å–é¡µé¢æºç è§£æ
                html = driver.page_source
                dfs = pd.read_html(StringIO(html))
                
                if not dfs:
                    raise ValueError("é¡µé¢è§£æä¸ºç©ºï¼Œæœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®")

                # [é€»è¾‘ä¼˜åŒ–] æ ¹æ®åˆ—åç‰¹å¾é€‰æ‹©æ­£ç¡®çš„è¡¨æ ¼
                target_df = None
                
                for df in dfs:
                    # æ¸…æ´—åˆ—å
                    df.columns = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                    
                    # é»˜è®¤é€»è¾‘ï¼šæ‰¾æœ€å¤§çš„ï¼Œæˆ–è€…åŒ¹é…æ—¥æœŸçš„
                    possible_date_cols = ['æœˆä»½', 'æ—¶é—´', 'æ—¥æœŸ', 'å‘å¸ƒæ—¥æœŸ', 'å…¬å¸ƒæ—¥æœŸ']
                    if any(x in str(col) for x in df.columns for col in possible_date_cols):
                        if target_df is None or len(df) > len(target_df):
                            target_df = df
                
                if target_df is None:
                    # å›é€€åˆ°æ—§é€»è¾‘ï¼šé€‰è¡Œæ•°æœ€å¤šçš„
                    target_df = max(dfs, key=lambda x: len(x))

                df = target_df
                
                # å¤„ç† MultiIndex (ä¸œæ–¹è´¢å¯Œå¸¸è§)
                if isinstance(df.columns, pd.MultiIndex):
                    new_cols = []
                    for col in df.columns:
                        valid_parts = [str(c) for c in col if "Unnamed" not in str(c) and str(c).strip() != ""]
                        seen = set()
                        unique_parts = [x for x in valid_parts if not (x in seen or seen.add(x))]
                        new_cols.append("".join(unique_parts))
                    df.columns = new_cols
                
                df.columns = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                
                possible_date_cols = ['æœˆä»½', 'æ—¶é—´', 'æ—¥æœŸ', 'å‘å¸ƒæ—¥æœŸ', 'å…¬å¸ƒæ—¥æœŸ']
                date_col = next((col for col in df.columns if any(x in str(col) for x in possible_date_cols)), None)
                
                if date_col:
                    # è¿™é‡Œä¾ç„¶ä½¿ç”¨åŸæœ‰çš„ clean_date ä»¥ä¿æŒå¯¹ä¸œè´¢æ•°æ®çš„å…¼å®¹æ€§
                    df['_std_date'] = df[date_col].apply(self.clean_date)
                    df = df.dropna(subset=['_std_date'])
                    
                    # å¼ºåˆ¶è½¬æ¢ä¸º datetime ç±»å‹
                    df['_std_date'] = pd.to_datetime(df['_std_date'])
                    
                    cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days_to_keep)
                    df = df[df['_std_date'] >= cutoff_date]
                    
                    df['_std_date'] = df['_std_date'].dt.strftime('%Y-%m-%d')
                    df = df.replace({'-': None, 'nan': None})
                    
                    if name == "ä¸­å›½_å—å‘èµ„é‡‘":
                        df = df.where(pd.notnull(df), None)
                        keep_cols = ['_std_date']
                        for c in df.columns:
                            if "å‡€ä¹°é¢" in c and "å½“æ—¥" in c:
                                keep_cols.append(c)
                            elif "æˆäº¤ç¬”æ•°" in c:
                                keep_cols.append(c)
                        df = df[keep_cols]
                        df.rename(columns={'_std_date': 'æ—¥æœŸ'}, inplace=True)
                    else:
                        # é€šç”¨å¤„ç†
                        df = df.where(pd.notnull(df), None)
                        if 'æ—¥æœŸ' not in df.columns and '_std_date' in df.columns:
                            df['æ—¥æœŸ'] = df['_std_date']

                    records = df.to_dict('records')
                    print(f"âœ… [{name}] æŠ“å–æˆåŠŸ! è·å¾— {len(records)} æ¡è®°å½•")
                    return name, records, None # Success
                else:
                    raise ValueError(f"æœªæ‰¾åˆ°æ—¥æœŸåˆ—: {df.columns.tolist()}")

            except Exception as e:
                last_error = str(e)
                print(f"âŒ [{name}] å¤±è´¥: {last_error[:200]}") 
                if attempt < max_retries:
                    time.sleep(2)
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        # æœ€ç»ˆå¤±è´¥
        return name, [], last_error

    def run_concurrent(self):
        print("ğŸš€ [Scraper] æ­£åœ¨å¹¶å‘æŠ“å–å®è§‚æ•°æ® (Workers=2)...")
        # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—
        self.status_logs = []
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_name = {
                executor.submit(self.fetch_single_source, name, url): name 
                for name, url in self.targets.items()
            }
            for future in as_completed(future_to_name):
                name, data, error_msg = future.result()
                if not error_msg:
                    self.results[name] = data
                    self.status_logs.append({'name': name, 'status': True, 'error': None})
                else:
                    self.results[name] = []
                    self.status_logs.append({'name': name, 'status': False, 'error': error_msg})
                    
        return self.results, self.status_logs

    def organize_data(self):
        """
        å°†æ‰å¹³çš„ results è½¬æ¢ä¸ºåµŒå¥—çš„å­—å…¸ç»“æ„
        """
        nested_data = {
            "china": {},
            "usa": {},
            "japan": {},
            "hk": {} # æ–°å¢é¦™æ¸¯åŒºåŸŸ
        }
        
        for old_key, data_list in self.results.items():
            if not data_list:
                continue
            if old_key in self.key_mapping:
                country_key, metric_key = self.key_mapping[old_key]
                if country_key not in nested_data:
                    nested_data[country_key] = {}
                nested_data[country_key][metric_key] = data_list
        
        return nested_data

    def get_data_dict(self):
        """
        å¯¹å¤–ä¸»æ¥å£ï¼šè¿è¡ŒæŠ“å–å¹¶è¿”å› (data_dict, status_logs)
        """
        self.run_concurrent()
        return self.organize_data(), self.status_logs

    def save_custom_json(self):
        data, _ = self.get_data_dict() # ç‹¬ç«‹è¿è¡Œæ—¶å¿½ç•¥æ—¥å¿—è¿”å›
        try:
            with open(self.output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ ç‹¬ç«‹è¿è¡Œæ•°æ®å·²å†™å…¥: {self.output_path}")
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")

# å¯¹å¤–æš´éœ²çš„ä¾¿æ·å‡½æ•°
def get_macro_data():
    scraper = MacroDataScraper()
    return scraper.get_data_dict()

if __name__ == "__main__":
    scraper = MacroDataScraper()
    scraper.save_custom_json()