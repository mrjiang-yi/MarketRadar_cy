import os
import pandas as pd
import akshare as ak
import yfinance as yf
import requests
import json
from datetime import datetime, timedelta
import logging
import warnings
import socket
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

# === é‚®ä»¶ç›¸å…³åº“ ===
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

# ================= ç¨³å®šæ€§å¢å¼ºè®¾ç½® (æ ¸å¼¹çº§é˜²å¡æ­») =================
# 1. å¼ºåˆ¶ Monkey Patch: ä¿®æ”¹ requests åº•å±‚ï¼Œå¼ºåˆ¶æ‰€æœ‰è¯·æ±‚å¸¦ä¸Šè¶…æ—¶
_original_request = requests.Session.request

def _patched_request(self, method, url, *args, **kwargs):
    # å¼ºåˆ¶è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 10 ç§’ï¼Œé˜²æ­¢ SSL æ¡æ‰‹æˆ– DNS è§£æåƒµæ­»
    if 'timeout' not in kwargs or kwargs['timeout'] is None:
        kwargs['timeout'] = 10
    return _original_request(self, method, url, *args, **kwargs)

requests.Session.request = _patched_request

# 2. è®¾ç½®å…¨å±€ç½‘ç»œåº•å±‚è¶…æ—¶
socket.setdefaulttimeout(10)

# å¿½ç•¥ YFinance å’Œ Pandas çš„éƒ¨åˆ†è­¦å‘Š
warnings.filterwarnings("ignore")
logging.getLogger('yfinance').setLevel(logging.CRITICAL)

# ================= é…ç½®åŒºåŸŸ =================

# --- é‚®ä»¶é…ç½® (å·²ä¿®å¤ä¸º QQ é‚®ç®±æ ‡å‡† SSL é…ç½®) ---
ENABLE_EMAIL = True               
SMTP_SERVER = "smtp.qq.com"       
SMTP_PORT = 465                   # QQé‚®ç®±æ¨èä½¿ç”¨ 465 (SSL)

# === å…³é”®ï¼šä»ç¯å¢ƒå˜é‡è·å–ï¼Œé€‚é… GitHub Actions ===
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")       
SENDER_PASSWORD = os.environ.get("SENDER_PASSWORD") 
RECEIVER_EMAIL = os.environ.get("RECEIVER_EMAIL")   

# --- æœ¬åœ°æµ‹è¯•é˜²å‘†åˆ¤æ–­ ---
if not SENDER_EMAIL:
    # è¿™é‡Œçš„ pass æ˜¯ä¸ºäº†é˜²æ­¢æœ¬åœ°è¿è¡Œæ—¶å¦‚æœæ²¡æœ‰é…ç¯å¢ƒå˜é‡æŠ¥é”™
    # å¦‚æœä½ åœ¨æœ¬åœ°è·‘ï¼Œè¯·ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œæˆ–åœ¨æ­¤å¤„ä¸´æ—¶ç¡¬ç¼–ç (ä¸æ¨è)
    pass

# --- æ•°æ®æŸ¥è¯¢æ—¶é—´æ®µ ---
START_DATE = "2025-12-01"
END_DATE = datetime.now().strftime("%Y-%m-%d")

# ------------------------------------------------
# ä»»åŠ¡ç»„ 1: å…¨çƒå¸‚åœº (åŸæœ‰é…ç½®) -> å­˜ä¸º æŒ‡æ•°.json
# ------------------------------------------------
TARGETS_GLOBAL = {
    "çº³æ–¯è¾¾å…‹":     {"ak": ".IXIC",   "yf": "^IXIC",    "type": "index_us"},
    "æ ‡æ™®500":      {"ak": ".INX",    "yf": "^GSPC",    "type": "index_us"},
    "æ’ç”Ÿç§‘æŠ€":     {"ak": "HSTECH",  "yf": "^HSTECH",  "type": "index_hk"},
    "æ’ç”ŸæŒ‡æ•°":     {"ak": "HSI",     "yf": "^HSI",     "type": "index_hk"},
    "é»„é‡‘(COMEX)":  {"ak": "GC",      "yf": "GC=F",     "type": "future_foreign"},  
    "ç™½é“¶(COMEX)":  {"ak": "SI",      "yf": "SI=F",     "type": "future_foreign"},  # æ–°å¢ï¼šç™½é“¶
    "é“œ(COMEX)":    {"ak": "HG",      "yf": "HG=F",     "type": "future_foreign"},  # æ–°å¢ï¼šé“œ
    "ä¸Šæµ·é‡‘":       {"ak": "Au99.99", "yf": None,       "type": "gold_cn"}, 
    "VNM(ETF)":    {"ak": None,      "yf": "VNM",      "type": "etf"},     
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 2: æ’ç”Ÿç§‘æŠ€ä¸»è¦æˆä»½è‚¡ (Top 20) -> å­˜ä¸º æ’ç”Ÿç§‘æŠ€.json
# ------------------------------------------------
TARGETS_HSTECH_TOP20 = {
    "ç¾å›¢-W":       {"ak": "03690", "yf": "3690.HK", "type": "stock_hk"},
    "è…¾è®¯æ§è‚¡":     {"ak": "00700", "yf": "0700.HK", "type": "stock_hk"},
    "å°ç±³é›†å›¢-W":   {"ak": "01810", "yf": "1810.HK", "type": "stock_hk"},
    "é˜¿é‡Œå·´å·´-SW":  {"ak": "09988", "yf": "9988.HK", "type": "stock_hk"},
    "ç†æƒ³æ±½è½¦-W":   {"ak": "02015", "yf": "2015.HK", "type": "stock_hk"},
    "å¿«æ‰‹-W":       {"ak": "01024", "yf": "1024.HK", "type": "stock_hk"},
    "äº¬ä¸œé›†å›¢-SW":  {"ak": "09618", "yf": "9618.HK", "type": "stock_hk"},
    "ç½‘æ˜“-S":       {"ak": "09999", "yf": "9999.HK", "type": "stock_hk"},
    "ç™¾åº¦é›†å›¢-SW":  {"ak": "09888", "yf": "9888.HK", "type": "stock_hk"},
    "æºç¨‹é›†å›¢-S":   {"ak": "09961", "yf": "9961.HK", "type": "stock_hk"},
    "ä¸­èŠ¯å›½é™…":     {"ak": "00981", "yf": "0981.HK", "type": "stock_hk"},
    "æµ·å°”æ™ºå®¶":     {"ak": "06690", "yf": "6690.HK", "type": "stock_hk"},
    "æ¯”äºšè¿ªç”µå­":   {"ak": "00285", "yf": "0285.HK", "type": "stock_hk"},
    "èˆœå®‡å…‰å­¦ç§‘æŠ€": {"ak": "02382", "yf": "2382.HK", "type": "stock_hk"},
    "é˜…æ–‡é›†å›¢":     {"ak": "00772", "yf": "0772.HK", "type": "stock_hk"},
    "å•†æ±¤-W":       {"ak": "00020", "yf": "0020.HK", "type": "stock_hk"},
    "é‡‘å±±è½¯ä»¶":     {"ak": "03888", "yf": "3888.HK", "type": "stock_hk"},
    "åè™¹åŠå¯¼ä½“":   {"ak": "01347", "yf": "1347.HK", "type": "stock_hk"},
    "é‡‘è¶å›½é™…":     {"ak": "00268", "yf": "0268.HK", "type": "stock_hk"},
    "åŒç¨‹æ—…è¡Œ":     {"ak": "00780", "yf": "0780.HK", "type": "stock_hk"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 3: è¶Šå—åå¤§è‚¡ç¥¨ -> å­˜ä¸º æ–°å…´å¸‚åœº.json
# ------------------------------------------------
TARGETS_VIETNAM_TOP10 = {
    "è¶Šå—ç¹è£é“¶è¡Œ(VPB)":    {"ak": None, "yf": "VPB.VN", "type": "stock_vn"},
    "å†›é˜Ÿå•†ä¸šé“¶è¡Œ(MBB)":    {"ak": None, "yf": "MBB.VN", "type": "stock_vn"},
    "å’Œå‘é›†å›¢(HPG)":        {"ak": None, "yf": "HPG.VN", "type": "stock_vn"},
    "ç§»åŠ¨ä¸–ç•Œ(MWG)":        {"ak": None, "yf": "MWG.VN", "type": "stock_vn"},
    "FPTå…¬å¸(FPT)":         {"ak": None, "yf": "FPT.VN", "type": "stock_vn"},
    "è¥¿è´¡å•†ä¿¡(STB)":        {"ak": None, "yf": "STB.VN", "type": "stock_vn"},
    "èƒ¡å¿—æ˜å‘å±•é“¶è¡Œ(HDB)":  {"ak": None, "yf": "HDB.VN", "type": "stock_vn"},
    "ç§‘æŠ€å•†ä¸šé“¶è¡Œ(TCB)":    {"ak": None, "yf": "TCB.VN", "type": "stock_vn"},
    "Vingroup(VIC)":       {"ak": None, "yf": "VIC.VN", "type": "stock_vn"},
    "Vinhomes(VHM)":       {"ak": None, "yf": "VHM.VN", "type": "stock_vn"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 4: ç¾è‚¡ä¸ƒå·¨å¤´ -> å­˜ä¸º ç¾è‚¡ä¸ƒå·¨å¤´.json
# ------------------------------------------------
TARGETS_US_MAG7 = {
    "è‹¹æœ(AAPL)":    {"ak": None, "yf": "AAPL",  "type": "stock_us"},
    "å¾®è½¯(MSFT)":    {"ak": None, "yf": "MSFT",  "type": "stock_us"},
    "è°·æ­Œ(GOOGL)":   {"ak": None, "yf": "GOOGL", "type": "stock_us"},
    "äºšé©¬é€Š(AMZN)":  {"ak": None, "yf": "AMZN",  "type": "stock_us"},
    "è‹±ä¼Ÿè¾¾(NVDA)":  {"ak": None, "yf": "NVDA",  "type": "stock_us"},
    "Meta(META)":    {"ak": None, "yf": "META",  "type": "stock_us"},
    "ç‰¹æ–¯æ‹‰(TSLA)":  {"ak": None, "yf": "TSLA",  "type": "stock_us"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 5: æ¸¯è‚¡åˆ›æ–°è¯ -> å­˜ä¸º æ¸¯è‚¡åˆ›æ–°è¯.json
# ------------------------------------------------
TARGETS_HK_PHARMA = {
    "ä¿¡è¾¾ç”Ÿç‰©":       {"ak": "01801", "yf": "1801.HK", "type": "stock_hk"},
    "ç™¾æµç¥å·":       {"ak": "06160", "yf": "6160.HK", "type": "stock_hk"},
    "è¯æ˜ç”Ÿç‰©":       {"ak": "02269", "yf": "2269.HK", "type": "stock_hk"},
    "åº·æ–¹ç”Ÿç‰©":       {"ak": "09926", "yf": "9926.HK", "type": "stock_hk"},
    "ä¸­å›½ç”Ÿç‰©åˆ¶è¯":   {"ak": "01177", "yf": "1177.HK", "type": "stock_hk"},
    "çŸ³è¯é›†å›¢":       {"ak": "01093", "yf": "1093.HK", "type": "stock_hk"},
    "ä¸‰ç”Ÿåˆ¶è¯":       {"ak": "01530", "yf": "1530.HK", "type": "stock_hk"},
    "è¯æ˜åº·å¾·":       {"ak": "02359", "yf": "2359.HK", "type": "stock_hk"},
    "ç¿°æ£®åˆ¶è¯":       {"ak": "03692", "yf": "3692.HK", "type": "stock_hk"},
    "ç§‘ä¼¦åšæ³°ç”Ÿç‰©-B": {"ak": "06990", "yf": "6990.HK", "type": "stock_hk"},
}

# ç¯å¢ƒå˜é‡ (å‚è€ƒ data_provider.py çš„å‘½å)
ENV_KEYS = {
    "FMP": os.environ.get("FMP_API_Key"),
}

# ===========================================

class MarketFetcher:
    def __init__(self):
        self.session = requests.Session()
    
    def normalize_df(self, df, name):
        """ç»Ÿä¸€æ¸…æ´—Kçº¿æ•°æ®æ ¼å¼"""
        if df.empty: return df
        
        # ç»Ÿä¸€åˆ—å
        df.columns = [c.lower() for c in df.columns]
        
        # å¤„ç†æ—¥æœŸåˆ—
        if 'date' not in df.columns and 'æ—¥æœŸ' in df.columns:
            df.rename(columns={'æ—¥æœŸ': 'date'}, inplace=True)
        
        # å¤„ç† AkShare ä¸­æ–‡åˆ—åæ˜ å°„
        rename_map = {
            'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume', 'äº¤æ˜“é‡': 'volume', 'æŒä»“é‡': 'open_interest',
            'å¼€ç›˜ä»·': 'open', 'æ”¶ç›˜ä»·': 'close', 'æœ€é«˜ä»·': 'high', 'æœ€ä½ä»·': 'low', 
            'date': 'date' 
        }
        df.rename(columns=rename_map, inplace=True)
        
        # ç¡®ä¿åŒ…å«å¿…è¦åˆ—
        required_cols = ['date', 'open', 'close', 'high', 'low', 'volume']
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸º datetime
        df['date'] = pd.to_datetime(df['date'])
        
        # å¡«å……ç¼ºå¤±åˆ—
        for col in required_cols:
            if col not in df.columns:
                df[col] = 0.0
                
        df['name'] = name
        
        # å¤„ç†å¯èƒ½çš„æ— æ•ˆæ•°å€¼ (å­—ç¬¦ä¸²è½¬æ•°å­—)
        cols_to_numeric = ['open', 'close', 'high', 'low', 'volume']
        for col in cols_to_numeric:
            if col in df.columns and df[col].dtype == object:
                 df[col] = df[col].astype(str).str.replace(',', '').apply(pd.to_numeric, errors='coerce')

        return df[['date', 'name', 'open', 'close', 'high', 'low', 'volume']]

    def fetch_akshare(self, symbol, asset_type):
        """å°è¯•ä» AkShare è·å– Kçº¿ (å¸¦3æ¬¡é‡è¯•)"""
        if not symbol: return pd.DataFrame()
        
        max_retries = 3
        
        for i in range(max_retries):
            # æ‰“å°è¯·æ±‚çŠ¶æ€ (å¦‚æœæ˜¯é‡è¯•ï¼Œæ‰“å°æ¬¡æ•°)
            retry_msg = f" [é‡è¯•{i}]" if i > 0 else ""
            print(f"   âš¡ [AkShare] è¯·æ±‚: {symbol} ({asset_type}){retry_msg} ...", end="", flush=True)

            try:
                df = pd.DataFrame()
                if asset_type == "index_us":
                    df = ak.index_us_stock_sina(symbol=symbol)
                elif asset_type == "index_hk":
                    df = ak.stock_hk_index_daily_sina(symbol=symbol)
                elif asset_type == "gold_cn":
                    df = ak.spot_hist_sge(symbol=symbol)
                elif asset_type == "future_foreign":
                    df = ak.futures_foreign_hist(symbol=symbol)
                elif asset_type == "stock_hk":
                    df = ak.stock_hk_daily(symbol=symbol, adjust="qfq")
                elif asset_type == "stock_vn":
                    try:
                        df = ak.stock_vn_hist(symbol=symbol)
                    except:
                        df = pd.DataFrame()
                
                if not df.empty:
                    print(" âœ…")
                    return df
                else:
                    # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œä¸ä¸€å®šæ˜¯æŠ¥é”™ï¼Œå¯èƒ½æ˜¯çœŸæ²¡æ•°æ®ï¼Œä¸éœ€è¦é‡è¯•ï¼Œç›´æ¥é€€å‡º
                    print(" âŒ (ç©ºæ•°æ®)")
                    return pd.DataFrame()

            except Exception as e:
                print(f" âŒ (Err: {str(e)[:15]})")
                if i < max_retries - 1:
                    time.sleep(2) # å¤±è´¥åä¼‘æ¯2ç§’å†é‡è¯•
                continue # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
        
        print(" âŒ (AkShareå¤šæ¬¡é‡è¯•å¤±è´¥, æ”¾å¼ƒ)")
        return pd.DataFrame()

    def fetch_yfinance(self, symbol):
        """å°è¯•ä» Yahoo Finance è·å– Kçº¿ (å¸¦3æ¬¡é‡è¯•)"""
        if not symbol: return pd.DataFrame()
        
        max_retries = 3
        
        for i in range(max_retries):
            retry_msg = f" [é‡è¯•{i}]" if i > 0 else ""
            print(f"   âš¡ [YFinance] è¯·æ±‚: {symbol}{retry_msg} ...", end="", flush=True)
            
            try:
                df = yf.download(symbol, start=START_DATE, end=END_DATE, progress=False, auto_adjust=False)
                if not df.empty:
                    df = df.reset_index()
                    if isinstance(df.columns, pd.MultiIndex):
                        df.columns = df.columns.droplevel(1)
                    print(" âœ…")
                    return df
                else:
                    print(" âŒ (ç©ºæ•°æ®)")
                    return pd.DataFrame()
            except Exception as e:
                print(f" âŒ (Err: {str(e)[:15]})")
                if i < max_retries - 1:
                    time.sleep(2)
                continue

        print(" âŒ (YFinanceå¤šæ¬¡é‡è¯•å¤±è´¥, æ”¾å¼ƒ)")
        return pd.DataFrame()

    def fetch_fmp(self, name):
        """å°è¯•ä» FMP è·å– (å•†ä¸šAPIå¤‡é€‰)"""
        key = ENV_KEYS.get("FMP")
        if not key: return pd.DataFrame()
        
        symbol_map = {
            "çº³æ–¯è¾¾å…‹": "^IXIC", "æ ‡æ™®500": "^GSPC", 
            "é»„é‡‘(COMEX)": "GCUSD", "VNM(ETF)": "VNM"
        }
        symbol = symbol_map.get(name)
        
        if not symbol: return pd.DataFrame()

        print(f"   âš¡ [FMP] è¯·æ±‚: {symbol} ...", end="", flush=True)
        try:
            url = f"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}?from={START_DATE}&to={END_DATE}&apikey={key}"
            res = requests.get(url, timeout=10) 
            data = res.json()
            if "historical" in data:
                df = pd.DataFrame(data["historical"])
                print(" âœ…")
                return df
        except:
            print(" âŒ")
        return pd.DataFrame()

    def get_kline_data(self, name, config):
        """æ ¸å¿ƒé€»è¾‘ï¼šè´£ä»»é“¾æ¨¡å¼è·å– Kçº¿æ•°æ®"""
        print(f"æ­£åœ¨è·å– Kçº¿ [{name}] ...")
        
        # === ç¨³å®šæ€§å¢å¼ºï¼šè¯·æ±‚å‰éšæœºä¼‘çœ ï¼Œé˜²æ­¢è§¦å‘åçˆ¬è™«æœºåˆ¶ ===
        time.sleep(random.uniform(1.0, 3.0))
        
        # 1. å°è¯• AkShare (å‡½æ•°å†…è‡ªå¸¦é‡è¯•æœºåˆ¶)
        df = self.fetch_akshare(config.get("ak"), config.get("type"))
        df = self.normalize_df(df, name)
        
        # 2. å¤±è´¥åˆ™å°è¯• YFinance (å‡½æ•°å†…è‡ªå¸¦é‡è¯•æœºåˆ¶)
        if df.empty:
            df = self.fetch_yfinance(config.get("yf"))
            df = self.normalize_df(df, name)

        # 3. å¤±è´¥åˆ™å°è¯• FMP
        if df.empty:
            df = self.fetch_fmp(name)
            df = self.normalize_df(df, name)
            
        return df

def fetch_group_data(fetcher, targets, group_name):
    """
    ä¿®æ”¹åçš„é€šç”¨å‡½æ•°ï¼šä¸ç›´æ¥å†™æ–‡ä»¶ï¼Œè€Œæ˜¯è¿”å›æ•°æ®å­—å…¸
    """
    print(f"\nğŸš€ å¼€å§‹å¤„ç†ä»»åŠ¡ç»„: {group_name} (å¹¶å‘æ¨¡å¼)")
    
    kline_list = []
    
    # å®šä¹‰å•ä¸ªä»»åŠ¡å‡½æ•° (ç”¨äºçº¿ç¨‹æ± )
    def fetch_task(name, config):
        try:
            df = fetcher.get_kline_data(name, config)
            if df.empty:
                return None
            
            # è¿‡æ»¤æ—¶é—´æ®µ
            df = df[(df['date'] >= pd.to_datetime(START_DATE)) & (df['date'] <= pd.to_datetime(END_DATE))]
            if df.empty:
                return None
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df['date'] = df['date'].dt.strftime('%Y-%m-%d')
            return df.to_dict(orient='records')
        except Exception as e:
            print(f"âŒ ä»»åŠ¡ {name} å¼‚å¸¸: {e}")
            return None

    # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘
    with ThreadPoolExecutor(max_workers=4) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_name = {executor.submit(fetch_task, name, config): name for name, config in targets.items()}
        
        # è·å–ç»“æœ (åŠ å…¥è¶…æ—¶ä¿æŠ¤ï¼Œé˜²æ­¢å•çº¿ç¨‹æ— é™æŒ‚èµ·)
        for future in as_completed(future_to_name):
            name = future_to_name[future]
            try:
                # === æ ¸å¿ƒä¿®æ”¹ï¼šè®¾ç½® 15 ç§’è¶…æ—¶ ===
                result = future.result(timeout=15)
                
                if result:
                    kline_list.extend(result)
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•è·å– {name} çš„æœ‰æ•ˆæ•°æ®ï¼Œå·²èˆå¼ƒ")
            except TimeoutError:
                print(f" ğŸ’€ ä¸¥é‡è¶…æ—¶: è·å– {name} è¶…è¿‡15ç§’æ— å“åº”ï¼Œå¼ºåˆ¶è·³è¿‡ï¼")
            except Exception as e:
                print(f"âŒ å¤„ç† {name} ç»“æœæ—¶å‡ºé”™: {e}")

    # æ’åº
    if kline_list:
        temp_df = pd.DataFrame(kline_list)
        temp_df.sort_values(by=['date', 'name'], ascending=[False, True], inplace=True)
        final_kline_data = temp_df.to_dict(orient='records')
    else:
        final_kline_data = []

    return final_kline_data


def send_email(subject, body, attachment_files):
    """
    å‘é€å¸¦æœ‰å¤šä¸ªé™„ä»¶çš„é‚®ä»¶ (QQé‚®ç®±ä½¿ç”¨ SMTP_SSL:465)
    """
    # æ£€æŸ¥å¿…è¦é…ç½®æ˜¯å¦é½å…¨
    if not ENABLE_EMAIL:
        print("\nğŸ”• é‚®ä»¶åŠŸèƒ½å·²å…³é—­ï¼Œè·³è¿‡å‘é€ã€‚")
        return
    
    if not SENDER_EMAIL or not SENDER_PASSWORD:
        print("\nâŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° SENDER_EMAIL æˆ– SENDER_PASSWORD ç¯å¢ƒå˜é‡ï¼Œæ— æ³•å‘é€é‚®ä»¶ï¼")
        return

    print("\nğŸ“§ æ­£åœ¨å‡†å¤‡å‘é€é‚®ä»¶...")
    
    msg = MIMEMultipart()
    msg['From'] = SENDER_EMAIL
    msg['To'] = RECEIVER_EMAIL
    msg['Subject'] = subject

    msg.attach(MIMEText(body, 'plain'))

    attachment_count = 0
    for file_path in attachment_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, "rb") as attachment:
                    part = MIMEBase("application", "octet-stream")
                    part.set_payload(attachment.read())
                
                encoders.encode_base64(part)
                
                # æ­£ç¡®è®¾ç½®ä¸­æ–‡æ–‡ä»¶å
                filename = os.path.basename(file_path)
                part.add_header('Content-Disposition', 'attachment', filename=filename)
                
                msg.attach(part)
                print(f"   ğŸ“ å·²æ·»åŠ é™„ä»¶: {filename}")
                attachment_count += 1
            except Exception as e:
                print(f"   âŒ æ·»åŠ é™„ä»¶ {file_path} å¤±è´¥: {e}")
        else:
            print(f"   âš ï¸ é™„ä»¶æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    if attachment_count == 0:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆé™„ä»¶è¢«æ·»åŠ ï¼Œä»å°è¯•å‘é€é‚®ä»¶...")

    try:
        # === æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ SMTP_SSL è¿æ¥ 465 ç«¯å£ ===
        print(f"ğŸš€ è¿æ¥ SMTP æœåŠ¡å™¨ {SMTP_SERVER}:{SMTP_PORT} (SSL) å¹¶å‘é€...")
        
        server = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT)
        server.login(SENDER_EMAIL, SENDER_PASSWORD)
        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())
        server.quit()
        print("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")


def generate_report():
    # === æ–°å¢ï¼šé…·ç‚«çš„å¯åŠ¨ Banner ===
    print(r"""
  __  __            _        _   ____          _            
 |  \/  | __ _ _ __| | _____| |_|  _ \ __ _ __| | __ _ _ __ 
 | |\/| |/ _` | '__| |/ / _ \ __| |_) / _` / _` |/ _` | '__|
 | |  | | (_| | |  |   <  __/ |_|  _ < (_| (_| | (_| | |   
 |_|  |_|\__,_|_|  |_|\_\___|\__|_| \_\__,_\__,_|\__,_|_|   
                                                            
    """)
    print("=========================================")
    print(f"ğŸ“… å¤šå¸‚åœºæ•°æ®é‡‡é›†å™¨ (MarketRadar - GitHub Actions Ready)")
    print(f"ğŸ•’ æ—¶é—´æ®µ: {START_DATE} è‡³ {END_DATE}")
    print("=========================================\n")

    fetcher = MarketFetcher()
    
    # æ±‡æ€»æ‰€æœ‰æ•°æ®åˆ°ä¸€ä¸ªå¤§å­—å…¸
    all_data_collection = {
        "meta": {
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "date_range": f"{START_DATE} to {END_DATE}",
            "description": "Global Market Data Consolidated Report"
        },
        "data": {}
    }

    # 1. æŠ“å–æŒ‡æ•°æ•°æ®
    all_data_collection["data"]["æŒ‡æ•°"] = fetch_group_data(fetcher, TARGETS_GLOBAL, "æŒ‡æ•°")

    # 2. æŠ“å–æ’ç”Ÿç§‘æŠ€
    all_data_collection["data"]["æ’ç”Ÿç§‘æŠ€"] = fetch_group_data(fetcher, TARGETS_HSTECH_TOP20, "æ’ç”Ÿç§‘æŠ€")
    
    # 3. æŠ“å–æ–°å…´å¸‚åœº
    all_data_collection["data"]["æ–°å…´å¸‚åœº"] = fetch_group_data(fetcher, TARGETS_VIETNAM_TOP10, "æ–°å…´å¸‚åœº")
    
    # 4. æŠ“å–ç¾è‚¡ä¸ƒå·¨å¤´
    all_data_collection["data"]["ç¾è‚¡ä¸ƒå·¨å¤´"] = fetch_group_data(fetcher, TARGETS_US_MAG7, "ç¾è‚¡ä¸ƒå·¨å¤´")
    
    # 5. æŠ“å–æ¸¯è‚¡åˆ›æ–°è¯
    all_data_collection["data"]["æ¸¯è‚¡åˆ›æ–°è¯"] = fetch_group_data(fetcher, TARGETS_HK_PHARMA, "æ¸¯è‚¡åˆ›æ–°è¯")
    
    print("\nğŸ‰ æ‰€æœ‰æ•°æ®æŠ“å–ä»»åŠ¡å¤„ç†å®Œæˆï¼æ­£åœ¨åˆå¹¶å†™å…¥æ–‡ä»¶...")

    # === åˆå¹¶å†™å…¥åˆ°ä¸€ä¸ª JSON æ–‡ä»¶ ===
    output_filename = "é‡‘èæ•°æ®.json"
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(all_data_collection, f, ensure_ascii=False, indent=4)
        print(f"âœ… æˆåŠŸ! æ‰€æœ‰æ•°æ®å·²åˆå¹¶å†™å…¥ {output_filename}ã€‚")
    except Exception as e:
        print(f"âŒ å†™å…¥åˆå¹¶ JSON å¤±è´¥: {e}")
    
    # === å‘é€é‚®ä»¶é€»è¾‘ ===
    generated_files = [output_filename]
    
    email_subject = f"å…¨çƒå¸‚åœºKçº¿æ•°æ®æŠ¥å‘Š_{datetime.now().strftime('%Y-%m-%d')}"
    email_body = f"""
    æ‚¨å¥½ï¼Œ
    
    è¿™æ˜¯ä»Šå¤©çš„å…¨é‡å¸‚åœº K çº¿æ•°æ®ï¼ˆå·²åˆå¹¶ï¼‰ã€‚
    ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    æ•°æ®èŒƒå›´: {START_DATE} è‡³ {END_DATE}
    
    é™„ä»¶åˆ—è¡¨:
    {', '.join(generated_files)}
    
    è¯·æŸ¥æ”¶ã€‚
    """
    
    send_email(email_subject, email_body, generated_files)

if __name__ == "__main__":
    generate_report()