import os
import pandas as pd
import akshare as ak
import yfinance as yf
import requests
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo  # æ–°å¢: æ—¶åŒºå¤„ç†
import logging
import warnings
import socket
import time
import random
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

# === å¼•å…¥å·¥å…·åº“ ===
import utils

# === é‚®ä»¶ç›¸å…³åº“ ===
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

# ================= ç¨³å®šæ€§å¢å¼ºè®¾ç½® (æ ¸å¼¹çº§é˜²å¡æ­») =================
# 1. å¼ºåˆ¶ Monkey Patch: ä¿®æ”¹ requests åº•å±‚ï¼Œå¼ºåˆ¶æ‰€æœ‰è¯·æ±‚å¸¦ä¸Šè¶…æ—¶
_original_request = requests.Session.request

def _patched_request(self, method, url, *args, **kwargs):
    # å¼ºåˆ¶è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 10 ç§’ï¼Œé˜²æ­¢ SSL æ¡æ‰‹æˆ– DNS è§£æåƒµæ­»
    if 'timeout' not in kwargs or kwargs['timeout'] is None:
        kwargs['timeout'] = 10
    return _original_request(self, method, url, *args, **kwargs)

requests.Session.request = _patched_request

# 2. è®¾ç½®å…¨å±€ç½‘ç»œåº•å±‚è¶…æ—¶
socket.setdefaulttimeout(10)

# å¿½ç•¥ YFinance å’Œ Pandas çš„éƒ¨åˆ†è­¦å‘Š
warnings.filterwarnings("ignore")
logging.getLogger('yfinance').setLevel(logging.CRITICAL)

# ================= é…ç½®åŒºåŸŸ =================

# --- é‚®ä»¶é…ç½® (å·²ä¿®å¤ä¸º QQ é‚®ç®±æ ‡å‡† SSL é…ç½®) ---
ENABLE_EMAIL = True               
SMTP_SERVER = "smtp.qq.com"       
SMTP_PORT = 465                   # QQé‚®ç®±æ¨èä½¿ç”¨ 465 (SSL)

# === å…³é”®ï¼šä»ç¯å¢ƒå˜é‡è·å–ï¼Œé€‚é… GitHub Actions ===
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")       
SENDER_PASSWORD = os.environ.get("SENDER_PASSWORD") 
RECEIVER_EMAIL = os.environ.get("RECEIVER_EMAIL")   

# --- æœ¬åœ°æµ‹è¯•é˜²å‘†åˆ¤æ–­ ---
if not SENDER_EMAIL:
    print("âš ï¸ è­¦å‘Š: æœªè®¾ç½® SENDER_EMAIL ç¯å¢ƒå˜é‡ï¼Œé‚®ä»¶å‘é€åŠŸèƒ½å¯èƒ½å—é™ã€‚")

# --- æ•°æ®æŸ¥è¯¢æ—¶é—´æ®µ (å¼ºåˆ¶åŒ—äº¬æ—¶é—´) ---
TZ_CN = ZoneInfo("Asia/Shanghai")
NOW_CN = datetime.now(TZ_CN)

# REPORT_START_DATE: æŠ¥å‘Šä¸­Kçº¿å±•ç¤ºçš„èµ·å§‹æ—¶é—´ (åŠ¨æ€è®¾ç½®ï¼šè¿‡å»20å¤©)
REPORT_START_DATE = (NOW_CN - timedelta(days=20)).strftime("%Y-%m-%d")

# FETCH_START_DATE: API å®é™…æ‹‰å–çš„èµ·å§‹æ—¶é—´ (å›æº¯500å¤©ï¼Œç¡®ä¿èƒ½è®¡ç®— MA250 å¹´çº¿)
FETCH_START_DATE = (NOW_CN - timedelta(days=500)).strftime("%Y-%m-%d")
END_DATE = NOW_CN.strftime("%Y-%m-%d")

# ------------------------------------------------
# ä»»åŠ¡ç»„ 1: å…¨çƒå¸‚åœº (æŒ‡æ•°.json)
# ------------------------------------------------
TARGETS_GLOBAL = {
    # [ä¿®æ”¹] çº³æ–¯è¾¾å…‹ä»£ç å˜æ›´ä¸º NDX (çº³æŒ‡100)
    "çº³æ–¯è¾¾å…‹":     {"ak": ".NDX",    "yf": "^NDX",     "type": "index_us"},
    "æ ‡æ™®500":      {"ak": ".INX",    "yf": "^GSPC",    "type": "index_us"},
    "æ’ç”Ÿç§‘æŠ€":     {"ak": "HSTECH",  "yf": "^HSTECH",  "type": "index_hk"},
    "æ’ç”ŸæŒ‡æ•°":     {"ak": "HSI",     "yf": "^HSI",     "type": "index_hk"},
    # [ä¿®æ”¹] ç§»é™¤è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°ï¼Œäº¤ç”± Step 4 çš„ fetch_data çˆ¬è™«ä¸“é—¨å¤„ç†ï¼Œé¿å… YFinance æŠ¥é”™
    "é»„é‡‘(COMEX)":  {"ak": "GC",      "yf": "GC=F",     "type": "future_foreign"},  
    "ç™½é“¶(COMEX)":  {"ak": "SI",      "yf": "SI=F",     "type": "future_foreign"},  
    "é“œ(COMEX)":    {"ak": "HG",      "yf": "HG=F",     "type": "future_foreign"}, 
    # [é…ç½®] ä¸Šæµ·é‡‘æ”¹ç”¨æœŸè´§ä¸»åŠ›åˆçº¦ (au0)
    "ä¸Šæµ·é‡‘":       {"ak": "au0",     "yf": None,       "type": "future_zh_sina"}, 
    "VNM(ETF)":     {"ak": "VNM",     "yf": "VNM",      "type": "stock_us"},
    # [æ–°å¢] åŸæ²¹å’Œé“€
    "åŸæ²¹(WTI)":    {"ak": "CL",      "yf": "CL=F",     "type": "future_foreign"},
    "é“€(URA)":      {"ak": "URA",     "yf": "URA",      "type": "stock_us"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 2: æ’ç”Ÿç§‘æŠ€ä¸»è¦æˆä»½è‚¡ (Top 20)
# ------------------------------------------------
TARGETS_HSTECH_TOP20 = {
    "ç¾å›¢-W":       {"ak": "03690", "yf": "3690.HK", "type": "stock_hk"},
    "è…¾è®¯æ§è‚¡":     {"ak": "00700", "yf": "0700.HK", "type": "stock_hk"},
    "å°ç±³é›†å›¢-W":   {"ak": "01810", "yf": "1810.HK", "type": "stock_hk"},
    "é˜¿é‡Œå·´å·´-SW":  {"ak": "09988", "yf": "9988.HK", "type": "stock_hk"},
    "ç†æƒ³æ±½è½¦-W":   {"ak": "02015", "yf": "2015.HK", "type": "stock_hk"},
    "å¿«æ‰‹-W":       {"ak": "01024", "yf": "1024.HK", "type": "stock_hk"},
    "äº¬ä¸œé›†å›¢-SW":  {"ak": "09618", "yf": "9618.HK", "type": "stock_hk"},
    "ç½‘æ˜“-S":       {"ak": "09999", "yf": "9999.HK", "type": "stock_hk"},
    "ç™¾åº¦é›†å›¢-SW":  {"ak": "09888", "yf": "9888.HK", "type": "stock_hk"},
    "æºç¨‹é›†å›¢-S":   {"ak": "09961", "yf": "9961.HK", "type": "stock_hk"},
    "ä¸­èŠ¯å›½é™…":     {"ak": "00981", "yf": "0981.HK", "type": "stock_hk"},
    "æµ·å°”æ™ºå®¶":     {"ak": "06690", "yf": "6690.HK", "type": "stock_hk"},
    "æ¯”äºšè¿ªç”µå­":   {"ak": "00285", "yf": "0285.HK", "type": "stock_hk"},
    "èˆœå®‡å…‰å­¦ç§‘æŠ€": {"ak": "02382", "yf": "2382.HK", "type": "stock_hk"},
    "é˜…æ–‡é›†å›¢":     {"ak": "00772", "yf": "0772.HK", "type": "stock_hk"},
    "å•†æ±¤-W":       {"ak": "00020", "yf": "0020.HK", "type": "stock_hk"},
    "é‡‘å±±è½¯ä»¶":     {"ak": "03888", "yf": "3888.HK", "type": "stock_hk"},
    "åè™¹åŠå¯¼ä½“":   {"ak": "01347", "yf": "1347.HK", "type": "stock_hk"},
    "é‡‘è¶å›½é™…":     {"ak": "00268", "yf": "0268.HK", "type": "stock_hk"},
    "åŒç¨‹æ—…è¡Œ":     {"ak": "00780", "yf": "0780.HK", "type": "stock_hk"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 3: è¶Šå—åå¤§è‚¡ç¥¨
# ------------------------------------------------
TARGETS_VIETNAM_TOP10 = {
    "è¶Šå—ç¹è£é“¶è¡Œ(VPB)":    {"ak": None, "yf": "VPB.VN", "type": "stock_vn"},
    "å†›é˜Ÿå•†ä¸šé“¶è¡Œ(MBB)":    {"ak": None, "yf": "MBB.VN", "type": "stock_vn"},
    "å’Œå‘é›†å›¢(HPG)":        {"ak": None, "yf": "HPG.VN", "type": "stock_vn"},
    "ç§»åŠ¨ä¸–ç•Œ(MWG)":        {"ak": None, "yf": "MWG.VN", "type": "stock_vn"},
    "FPTå…¬å¸(FPT)":         {"ak": None, "yf": "FPT.VN", "type": "stock_vn"},
    "è¥¿è´¡å•†ä¿¡(STB)":        {"ak": None, "yf": "STB.VN", "type": "stock_vn"},
    "èƒ¡å¿—æ˜å‘å±•é“¶è¡Œ(HDB)":  {"ak": None, "yf": "HDB.VN", "type": "stock_vn"},
    "ç§‘æŠ€å•†ä¸šé“¶è¡Œ(TCB)":    {"ak": None, "yf": "TCB.VN", "type": "stock_vn"},
    "Vingroup(VIC)":       {"ak": None, "yf": "VIC.VN", "type": "stock_vn"},
    "Vinhomes(VHM)":       {"ak": None, "yf": "VHM.VN", "type": "stock_vn"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 4: ç¾è‚¡ä¸ƒå·¨å¤´
# ------------------------------------------------
TARGETS_US_MAG7 = {
    "è‹¹æœ(AAPL)":    {"ak": None, "yf": "AAPL",  "type": "stock_us"},
    "å¾®è½¯(MSFT)":    {"ak": None, "yf": "MSFT",  "type": "stock_us"},
    "è°·æ­Œ(GOOGL)":   {"ak": None, "yf": "GOOGL", "type": "stock_us"},
    "äºšé©¬é€Š(AMZN)":  {"ak": None, "yf": "AMZN",  "type": "stock_us"},
    "è‹±ä¼Ÿè¾¾(NVDA)":  {"ak": None, "yf": "NVDA",  "type": "stock_us"},
    "Meta(META)":    {"ak": None, "yf": "META",  "type": "stock_us"},
    "ç‰¹æ–¯æ‹‰(TSLA)":  {"ak": None, "yf": "TSLA",  "type": "stock_us"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 5: æ¸¯è‚¡åˆ›æ–°è¯
# ------------------------------------------------
TARGETS_HK_PHARMA = {
    "ä¿¡è¾¾ç”Ÿç‰©":       {"ak": "01801", "yf": "1801.HK", "type": "stock_hk"},
    "ç™¾æµç¥å·":       {"ak": "06160", "yf": "6160.HK", "type": "stock_hk"},
    "è¯æ˜ç”Ÿç‰©":       {"ak": "02269", "yf": "2269.HK", "type": "stock_hk"},
    "åº·æ–¹ç”Ÿç‰©":       {"ak": "09926", "yf": "9926.HK", "type": "stock_hk"},
    "ä¸­å›½ç”Ÿç‰©åˆ¶è¯":   {"ak": "01177", "yf": "1177.HK", "type": "stock_hk"},
    "çŸ³è¯é›†å›¢":       {"ak": "01093", "yf": "1093.HK", "type": "stock_hk"},
    "ä¸‰ç”Ÿåˆ¶è¯":       {"ak": "01530", "yf": "1530.HK", "type": "stock_hk"},
    "è¯æ˜åº·å¾·":       {"ak": "02359", "yf": "2359.HK", "type": "stock_hk"},
    "ç¿°æ£®åˆ¶è¯":       {"ak": "03692", "yf": "3692.HK", "type": "stock_hk"},
    "ç§‘ä¼¦åšæ³°ç”Ÿç‰©-B": {"ak": "06990", "yf": "6990.HK", "type": "stock_hk"},
}

# ------------------------------------------------
# ä»»åŠ¡ç»„ 6: æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•° (å·²ç¡®ä¿çŠ¶æ€è¿½è¸ª)
# ------------------------------------------------
TARGETS_HK_HEALTHCARE = {
    "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°": {"ak": "HSHCI", "yf": "^HSHCI", "type": "index_hk"},
}

# ------------------------------------------------
# [æ–°å¢] ä»»åŠ¡ç»„ 7: ç§‘åˆ›50 ETF (ä¿®æ­£ä»£ç ä¸º588000)
# ------------------------------------------------
TARGETS_STAR50_ETF = {
    "ç§‘åˆ›50ETF": {"ak": "588000", "yf": "588000.SS", "type": "etf_zh"},
}

# ------------------------------------------------
# [æ–°å¢] ä»»åŠ¡ç»„ 8: ç§‘åˆ›50 æŒä»“è‚¡
# ------------------------------------------------
TARGETS_STAR50_HOLDINGS = {
    "ä¸­èŠ¯å›½é™…": {"ak": "688981", "yf": "688981.SS", "type": "stock_zh_a"},
    "æµ·å…‰ä¿¡æ¯": {"ak": "688041", "yf": "688041.SS", "type": "stock_zh_a"},
    "å¯’æ­¦çºª":   {"ak": "688256", "yf": "688256.SS", "type": "stock_zh_a"},
    "æ¾œèµ·ç§‘æŠ€": {"ak": "688008", "yf": "688008.SS", "type": "stock_zh_a"},
    "ä¸­å¾®å…¬å¸": {"ak": "688012", "yf": "688012.SS", "type": "stock_zh_a"},
    "è”å½±åŒ»ç–—": {"ak": "688271", "yf": "688271.SS", "type": "stock_zh_a"},
    "é‡‘å±±åŠå…¬": {"ak": "688111", "yf": "688111.SS", "type": "stock_zh_a"},
    "èŠ¯åŸè‚¡ä»½": {"ak": "688521", "yf": "688521.SS", "type": "stock_zh_a"},
    "çŸ³å¤´ç§‘æŠ€": {"ak": "688169", "yf": "688169.SS", "type": "stock_zh_a"},
    "ä¼ éŸ³æ§è‚¡": {"ak": "688036", "yf": "688036.SS", "type": "stock_zh_a"},
    "æ²ªç¡…äº§ä¸š": {"ak": "688126", "yf": "688126.SS", "type": "stock_zh_a"},
    "åæµ·æ¸…ç§‘": {"ak": "688120", "yf": "688120.SS", "type": "stock_zh_a"},
    "æ™¶æ™¨è‚¡ä»½": {"ak": "688099", "yf": "688099.SS", "type": "stock_zh_a"},
    "æ‹“è†ç§‘æŠ€": {"ak": "688072", "yf": "688072.SS", "type": "stock_zh_a"},
    "æ’ç„ç§‘æŠ€": {"ak": "688608", "yf": "688608.SS", "type": "stock_zh_a"},
    "ä¸­æ§æŠ€æœ¯": {"ak": "688777", "yf": "688777.SS", "type": "stock_zh_a"},
    "ä½°ç»´å­˜å‚¨": {"ak": "688525", "yf": "688525.SS", "type": "stock_zh_a"},
    "æ€ç‰¹å¨":   {"ak": "688213", "yf": "688213.SS", "type": "stock_zh_a"},
    "èŠ¯è”é›†æˆ": {"ak": "688469", "yf": "688469.SS", "type": "stock_zh_a"},
    "ç™¾åˆ©å¤©æ’": {"ak": "688506", "yf": "688506.SS", "type": "stock_zh_a"},
}

# ç¯å¢ƒå˜é‡ (å‚è€ƒ data_provider.py çš„å‘½å)
ENV_KEYS = {
    "FMP": os.environ.get("FMP_API_Key"),
}

# ===========================================

class MarketFetcher:
    def __init__(self):
        self.session = requests.Session()
    
    def normalize_df(self, df, name):
        """ç»Ÿä¸€æ¸…æ´—Kçº¿æ•°æ®æ ¼å¼å¹¶è‡ªåŠ¨è¡¥å…¨æŒ‡æ ‡"""
        if df.empty: return df
        
        # 1. ç»Ÿä¸€åˆ—å (Lower case)
        df.columns = [c.lower() for c in df.columns]
        
        # 2. å¤„ç†æ—¥æœŸåˆ—å
        if 'date' not in df.columns and 'æ—¥æœŸ' in df.columns:
            df.rename(columns={'æ—¥æœŸ': 'date'}, inplace=True)
        
        # 3. å¤„ç† AkShare ä¸­æ–‡åˆ—åæ˜ å°„
        rename_map = {
            'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high', 'æœ€ä½': 'low', 
            'æˆäº¤é‡': 'volume', 'äº¤æ˜“é‡': 'volume', 'æŒä»“é‡': 'open_interest',
            'æˆäº¤é¢': 'amount', 'é‡æ¯”': 'volume_ratio',
            'å¼€ç›˜ä»·': 'open', 'æ”¶ç›˜ä»·': 'close', 'æœ€é«˜ä»·': 'high', 'æœ€ä½ä»·': 'low', 
            'date': 'date' 
        }
        df.rename(columns=rename_map, inplace=True)
        
        # 4. ç¡®ä¿æ—¥æœŸæ ¼å¼å¹¶æ’åº (è®¡ç®—æŒ‡æ ‡å¿…é¡»æŒ‰æ—¶é—´é¡ºåº)
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values(by='date', ascending=True)
        
        # 5. ç¡®ä¿åŒ…å«åŸºç¡€åˆ— (ä¸å­˜åœ¨åˆ™å…ˆç½®ä¸ºç©º)
        for col in ['open', 'close', 'high', 'low', 'volume']:
            if col not in df.columns:
                df[col] = 0.0
        
        if 'name' not in df.columns:
            df['name'] = name

        # 6. æ•°å€¼è½¬æ¢ (å¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸², é€—å·ç­‰)
        cols_to_numeric = ['open', 'close', 'high', 'low', 'volume', 'amount', 'volume_ratio']
        for col in cols_to_numeric:
            if col in df.columns:
                if df[col].dtype == object:
                     df[col] = df[col].astype(str).str.replace(',', '').apply(pd.to_numeric, errors='coerce')
                else:
                     df[col] = pd.to_numeric(df[col], errors='coerce')

        # 7. è¡¥å…¨/è®¡ç®— æˆäº¤é¢ (Amount)
        # å¦‚æœæ•°æ®æºæ²¡ç»™ amountï¼Œå°±ç”¨ close * volume è¿‘ä¼¼
        if 'amount' not in df.columns or df['amount'].isna().all():
            df['amount'] = df['close'] * df['volume']
        else:
            df['amount'] = df['amount'].fillna(df['close'] * df['volume'])
            
        # 8. è¡¥å…¨/è®¡ç®— é‡æ¯” (Volume Ratio)
        # é€»è¾‘: å½“æ—¥æˆäº¤é‡ / è¿‡å»5æ—¥æˆäº¤é‡å‡å€¼(ä¸å«å½“æ—¥)
        # å…¬å¼: Volume / Shift(Rolling(5).Mean())
        need_calc_vr = False
        if 'volume_ratio' not in df.columns:
            need_calc_vr = True
        elif df['volume_ratio'].isna().all():
            need_calc_vr = True
        
        if need_calc_vr:
            # è®¡ç®— 5æ—¥å‡é‡ (shift(1)è¡¨ç¤ºå–å‰5å¤©ï¼Œä¸å«ä»Šå¤©)
            ma5_vol = df['volume'].rolling(window=5, min_periods=1).mean().shift(1)
            
            # è®¡ç®—æ¯”ç‡ (å¤„ç†é™¤ä»¥0çš„æƒ…å†µ)
            # ä½¿ç”¨ np.divide å®‰å…¨é™¤æ³•ï¼Œåˆ†æ¯ä¸º0æ—¶å¡« NaN
            # ç„¶å fillna(1.0) æˆ– 0.0ï¼Œè¿™é‡Œé€šå¸¸é‡æ¯”ä¸º0æˆ–1æ¯”è¾ƒåˆé€‚ï¼Œæˆ–è€…ä¿ç•™ NaN
            # è¿™é‡Œç®€å•å¤„ç†ï¼šå¦‚æœåˆ†æ¯ä¸º0æˆ–NaNï¼Œé‡æ¯”è®¾ä¸º0
            df['volume_ratio'] = df['volume'] / ma5_vol
            df['volume_ratio'] = df['volume_ratio'].replace([float('inf'), -float('inf')], 0.0).fillna(0.0)

        # 9. æœ€ç»ˆåˆ—ç­›é€‰ä¸å¡«å……
        final_cols = ['date', 'name', 'open', 'close', 'high', 'low', 'volume', 'amount', 'volume_ratio']
        
        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½åœ¨ (ä¾‹å¦‚è®¡ç®—åå¯èƒ½äº§ç”Ÿçš„ NaN)
        for col in final_cols:
            if col not in df.columns:
                df[col] = 0.0
        
        # å¡«å……æ®‹ç•™ NaN (ä¾‹å¦‚ç¬¬ä¸€å¤©æ²¡æœ‰å‰5æ—¥å‡å€¼)
        df.fillna(0, inplace=True)

        return df[final_cols]

    def fetch_akshare(self, symbol, asset_type):
        """å°è¯•ä» AkShare è·å– Kçº¿ (å¸¦5æ¬¡é‡è¯•)"""
        if not symbol: return pd.DataFrame()
        
        max_retries = 5
        
        for i in range(max_retries):
            retry_msg = f" [é‡è¯•{i}]" if i > 0 else ""
            print(f"   âš¡ [AkShare] è¯·æ±‚: {symbol} ({asset_type}){retry_msg} ...", end="", flush=True)

            try:
                df = pd.DataFrame()
                
                # é¢„å¤„ç†æ—¥æœŸï¼šéƒ¨åˆ†AkShareæ¥å£éœ€è¦ YYYYMMDD æ ¼å¼
                start_date_clean = FETCH_START_DATE.replace("-", "")
                end_date_clean = END_DATE.replace("-", "")

                if asset_type == "index_us":
                    df = ak.index_us_stock_sina(symbol=symbol)
                elif asset_type == "index_hk":
                    df = ak.stock_hk_index_daily_sina(symbol=symbol)
                elif asset_type == "gold_cn":
                    df = ak.spot_hist_sge(symbol=symbol)
                elif asset_type == "future_foreign":
                    df = ak.futures_foreign_hist(symbol=symbol)
                elif asset_type == "stock_hk":
                    df = ak.stock_hk_daily(symbol=symbol, adjust="qfq")
                elif asset_type == "stock_vn":
                    try:
                        df = ak.stock_vn_hist(symbol=symbol)
                    except:
                        df = pd.DataFrame()
                elif asset_type == "stock_us":
                    df = ak.stock_us_daily(symbol=symbol, adjust="qfq")
                elif asset_type == "future_zh_sina":
                    df = ak.futures_main_sina(symbol=symbol)
                # [æ–°å¢] Aè‚¡ ETF åŸºé‡‘ (å¦‚ç§‘åˆ›50ETF 588000)
                elif asset_type == "etf_zh":
                    df = ak.fund_etf_hist_em(symbol=symbol, period="daily", start_date=start_date_clean, end_date=end_date_clean, adjust="qfq")
                # [æ–°å¢] Aè‚¡ è‚¡ç¥¨ (å¦‚ 688981)
                elif asset_type == "stock_zh_a":
                    df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date_clean, end_date=end_date_clean, adjust="qfq")
                
                if not df.empty:
                    print(" âœ…")
                    return df
                else:
                    print(" âŒ (ç©ºæ•°æ®)")
                    return pd.DataFrame()

            except Exception as e:
                print(f" âŒ (Err: {str(e)[:15]})")
                if i < max_retries - 1:
                    time.sleep(2)
                continue
        
        print(" âŒ (AkShareå¤šæ¬¡é‡è¯•å¤±è´¥, æ”¾å¼ƒ)")
        return pd.DataFrame()

    def fetch_yfinance(self, symbol):
        """å°è¯•ä» Yahoo Finance è·å– Kçº¿ (å¸¦5æ¬¡é‡è¯•)"""
        if not symbol: return pd.DataFrame()
        
        max_retries = 5
        
        for i in range(max_retries):
            retry_msg = f" [é‡è¯•{i}]" if i > 0 else ""
            print(f"   âš¡ [YFinance] è¯·æ±‚: {symbol}{retry_msg} ...", end="", flush=True)
            
            try:
                # ä½¿ç”¨ FETCH_START_DATE (å›æº¯500å¤©) ä»¥ç¡®ä¿å‡çº¿è®¡ç®—æ­£ç¡®
                df = yf.download(symbol, start=FETCH_START_DATE, end=END_DATE, progress=False, auto_adjust=False)
                if not df.empty:
                    df = df.reset_index()
                    if isinstance(df.columns, pd.MultiIndex):
                        df.columns = df.columns.droplevel(1)
                    print(" âœ…")
                    return df
                else:
                    print(" âŒ (ç©ºæ•°æ®)")
                    return pd.DataFrame()
            except Exception as e:
                print(f" âŒ (Err: {str(e)[:15]})")
                if i < max_retries - 1:
                    time.sleep(2)
                continue

        print(" âŒ (YFinanceå¤šæ¬¡é‡è¯•å¤±è´¥, æ”¾å¼ƒ)")
        return pd.DataFrame()

    def fetch_fmp(self, name):
        """å°è¯•ä» FMP è·å– (å•†ä¸šAPIå¤‡é€‰)"""
        key = ENV_KEYS.get("FMP")
        if not key: return pd.DataFrame()
        
        symbol_map = {
            "çº³æ–¯è¾¾å…‹": "^IXIC", "æ ‡æ™®500": "^GSPC", 
            "é»„é‡‘(COMEX)": "GCUSD", "VNM(ETF)": "VNM",
            "è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°": "^VNINDEX"
        }
        symbol = symbol_map.get(name)
        
        if not symbol: return pd.DataFrame()

        print(f"   âš¡ [FMP] è¯·æ±‚: {symbol} ...", end="", flush=True)
        try:
            # ä½¿ç”¨ FETCH_START_DATE
            url = f"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}?from={FETCH_START_DATE}&to={END_DATE}&apikey={key}"
            res = requests.get(url, timeout=10) 
            data = res.json()
            if "historical" in data:
                df = pd.DataFrame(data["historical"])
                print(" âœ…")
                return df
        except:
            print(" âŒ")
        return pd.DataFrame()

    def get_kline_data(self, name, config):
        """æ ¸å¿ƒé€»è¾‘ï¼šè´£ä»»é“¾æ¨¡å¼è·å– Kçº¿æ•°æ®"""
        print(f"æ­£åœ¨è·å– Kçº¿ [{name}] ...")
        
        # === ç¨³å®šæ€§å¢å¼ºï¼šè¯·æ±‚å‰éšæœºä¼‘çœ ï¼Œé˜²æ­¢è§¦å‘åçˆ¬è™«æœºåˆ¶ ===
        time.sleep(random.uniform(1.0, 3.0))
        
        # 1. å°è¯• AkShare (å‡½æ•°å†…è‡ªå¸¦é‡è¯•æœºåˆ¶)
        # åªæœ‰å½“é…ç½®äº† ak ç¬¦å·æ—¶æ‰è°ƒç”¨
        df = pd.DataFrame()
        if config.get("ak"):
            df = self.fetch_akshare(config.get("ak"), config.get("type"))
            df = self.normalize_df(df, name)
        
        # 2. å¤±è´¥æˆ–æœªé…ç½® AkShare åˆ™å°è¯• YFinance
        if df.empty:
            df = self.fetch_yfinance(config.get("yf"))
            df = self.normalize_df(df, name)

        # 3. å¤±è´¥åˆ™å°è¯• FMP
        if df.empty:
            df = self.fetch_fmp(name)
            df = self.normalize_df(df, name)
            
        return df

def fetch_group_data(fetcher, targets, group_name):
    """
    é€šç”¨å‡½æ•°ï¼šè¿”å› (Kçº¿æ•°æ®åˆ—è¡¨, å‡çº¿æ•°æ®åˆ—è¡¨, çŠ¶æ€æ—¥å¿—åˆ—è¡¨)
    """
    print(f"\nğŸš€ å¼€å§‹å¤„ç†ä»»åŠ¡ç»„: {group_name} (å¹¶å‘æ¨¡å¼)")
    
    kline_list = []
    ma_list = []
    status_logs = []
    
    # å®šä¹‰å•ä¸ªä»»åŠ¡å‡½æ•° (ç”¨äºçº¿ç¨‹æ± )
    def fetch_task(name, config):
        try:
            # 1. è·å–é•¿å‘¨æœŸæ•°æ® (500å¤©+)
            df = fetcher.get_kline_data(name, config)
            if df.empty:
                return None, None, {'name': name, 'status': False, 'error': "Data source returned empty after retries"}
            
            # 2. ç¡®ä¿æŒ‰ç…§æ—¥æœŸæ’åº
            df = df.sort_values(by='date', ascending=True)

            # 3. è®¡ç®—å‡çº¿ (åŸºäºé•¿å‘¨æœŸæ•°æ®)
            ma_info_list = utils.calculate_ma(df) 
            ma_info = ma_info_list[0] if ma_info_list else None

            # 4. åˆ‡ç‰‡ä¸ºç”¨æˆ·é…ç½®çš„çŸ­å‘¨æœŸ (ç”¨äºå±•ç¤º Kçº¿)
            # ä½¿ç”¨ REPORT_START_DATE è¿›è¡Œè¿‡æ»¤ï¼Œé˜²æ­¢JSONè¿‡å¤§
            df_slice = df[(df['date'] >= pd.to_datetime(REPORT_START_DATE)) & (df['date'] <= pd.to_datetime(END_DATE))].copy()
            
            if df_slice.empty:
                return None, ma_info, {'name': name, 'status': True, 'error': None} # å‡çº¿è®¡ç®—æˆåŠŸï¼Œåªæ˜¯å±•ç¤ºåŒºé—´æ— æ•°æ®ï¼Œç®—æˆåŠŸ
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df_slice['date'] = df_slice['date'].dt.strftime('%Y-%m-%d')
            kline_records = df_slice.to_dict(orient='records')
            
            return kline_records, ma_info, {'name': name, 'status': True, 'error': None}

        except Exception as e:
            print(f"âŒ ä»»åŠ¡ {name} å¼‚å¸¸: {e}")
            return None, None, {'name': name, 'status': False, 'error': str(e)}

    # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘
    with ThreadPoolExecutor(max_workers=4) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_name = {executor.submit(fetch_task, name, config): name for name, config in targets.items()}
        
        # è·å–ç»“æœ
        for future in as_completed(future_to_name):
            name = future_to_name[future]
            try:
                # è®¾ç½® 15 ç§’è¶…æ—¶
                result = future.result(timeout=15)
                klines, ma, status = result # unpack result
                
                status_logs.append(status)
                
                if klines:
                    kline_list.extend(klines)
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•è·å– {name} çš„Kçº¿æ•°æ®")
                
                if ma:
                    ma_list.append(ma)
                    
            except TimeoutError:
                print(f" ğŸ’€ ä¸¥é‡è¶…æ—¶: è·å– {name} è¶…è¿‡15ç§’æ— å“åº”ï¼Œå¼ºåˆ¶è·³è¿‡ï¼")
                status_logs.append({'name': name, 'status': False, 'error': "Thread timed out (15s)"})
            except Exception as e:
                print(f"âŒ å¤„ç† {name} ç»“æœæ—¶å‡ºé”™: {e}")
                status_logs.append({'name': name, 'status': False, 'error': f"Processing error: {str(e)}"})

    # æ’åº
    if kline_list:
        temp_df = pd.DataFrame(kline_list)
        temp_df.sort_values(by=['date', 'name'], ascending=[False, True], inplace=True)
        final_kline_data = temp_df.to_dict(orient='records')
    else:
        final_kline_data = []

    return final_kline_data, ma_list, status_logs


def send_email(subject, body, attachment_files):
    """
    å‘é€å¸¦æœ‰å¤šä¸ªé™„ä»¶çš„é‚®ä»¶ (QQé‚®ç®±ä½¿ç”¨ SMTP_SSL:465)
    """
    # æ£€æŸ¥å¿…è¦é…ç½®æ˜¯å¦é½å…¨
    if not ENABLE_EMAIL:
        print("\nğŸ”• é‚®ä»¶åŠŸèƒ½å·²å…³é—­ï¼Œè·³è¿‡å‘é€ã€‚")
        return
    
    if not SENDER_EMAIL or not SENDER_PASSWORD:
        print("\nâŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° SENDER_EMAIL æˆ– SENDER_PASSWORD ç¯å¢ƒå˜é‡ï¼Œæ— æ³•å‘é€é‚®ä»¶ï¼")
        return

    print("\nğŸ“§ æ­£åœ¨å‡†å¤‡å‘é€é‚®ä»¶...")
    
    msg = MIMEMultipart()
    msg['From'] = SENDER_EMAIL
    msg['To'] = RECEIVER_EMAIL
    msg['Subject'] = subject

    msg.attach(MIMEText(body, 'plain'))

    attachment_count = 0
    for file_path in attachment_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, "rb") as attachment:
                    part = MIMEBase("application", "octet-stream")
                    part.set_payload(attachment.read())
                
                encoders.encode_base64(part)
                
                # æ­£ç¡®è®¾ç½®ä¸­æ–‡æ–‡ä»¶å
                filename = os.path.basename(file_path)
                part.add_header('Content-Disposition', 'attachment', filename=filename)
                
                msg.attach(part)
                print(f"   ğŸ“ å·²æ·»åŠ é™„ä»¶: {filename}")
                attachment_count += 1
            except Exception as e:
                print(f"   âŒ æ·»åŠ é™„ä»¶ {file_path} å¤±è´¥: {e}")
        else:
            print(f"   âš ï¸ é™„ä»¶æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    if attachment_count == 0:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆé™„ä»¶è¢«æ·»åŠ ï¼Œä»å°è¯•å‘é€é‚®ä»¶...")

    try:
        # === æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ SMTP_SSL è¿æ¥ 465 ç«¯å£ ===
        print(f"ğŸš€ è¿æ¥ SMTP æœåŠ¡å™¨ {SMTP_SERVER}:{SMTP_PORT} (SSL) å¹¶å‘é€...")
        
        server = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT)
        server.login(SENDER_EMAIL, SENDER_PASSWORD)
        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())
        server.quit()
        print("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")


def get_all_kline_data():
    """
    å¯¹å¤–æ¥å£å‡½æ•°ï¼šæ‰§è¡Œæ‰€æœ‰Kçº¿æŠ“å–ä»»åŠ¡å¹¶è¿”å› (data_collection, status_logs)
    """
    print(f"ğŸ“… å¤šå¸‚åœºæ•°æ®é‡‡é›†å™¨ (MarketRadar - Module)")
    print(f"ğŸ•’ æŠ¥å‘Šå‘¨æœŸ: {REPORT_START_DATE} è‡³ {END_DATE}")
    print(f"ğŸ•’ è®¡ç®—å‘¨æœŸ: {FETCH_START_DATE} è‡³ {END_DATE}")
    
    fetcher = MarketFetcher()
    
    # æ±‡æ€»æ‰€æœ‰æ•°æ®åˆ°ä¸€ä¸ªå¤§å­—å…¸ (ä½¿ç”¨åŒ—äº¬æ—¶é—´)
    all_data_collection = {
        "meta": {
            "generated_at": datetime.now(TZ_CN).strftime("%Y-%m-%d %H:%M:%S"),
            "date_range": f"{REPORT_START_DATE} to {END_DATE}",
            "description": "Global Market Data Consolidated Report"
        },
        "data": {},
        "ma_data": [] # ä¸“é—¨å­˜æ”¾å‡çº¿æ•°æ®
    }

    all_ma_data = []
    all_status_logs = []

    # 1. æŠ“å–æŒ‡æ•°æ•°æ®
    data_idx, ma_idx, logs_idx = fetch_group_data(fetcher, TARGETS_GLOBAL, "æŒ‡æ•°")
    all_data_collection["data"]["æŒ‡æ•°"] = data_idx
    all_ma_data.extend(ma_idx)
    all_status_logs.extend(logs_idx)

    # 2. æŠ“å–æ’ç”Ÿç§‘æŠ€
    data_hstech, ma_hstech, logs_hstech = fetch_group_data(fetcher, TARGETS_HSTECH_TOP20, "æ’ç”Ÿç§‘æŠ€")
    all_data_collection["data"]["æ’ç”Ÿç§‘æŠ€"] = data_hstech
    all_ma_data.extend(ma_hstech)
    all_status_logs.extend(logs_hstech)
    
    # 3. æŠ“å–æ–°å…´å¸‚åœº
    data_vn, ma_vn, logs_vn = fetch_group_data(fetcher, TARGETS_VIETNAM_TOP10, "æ–°å…´å¸‚åœº")
    all_data_collection["data"]["æ–°å…´å¸‚åœº"] = data_vn
    all_ma_data.extend(ma_vn)
    all_status_logs.extend(logs_vn)
    
    # 4. æŠ“å–ç¾è‚¡ä¸ƒå·¨å¤´
    data_us, ma_us, logs_us = fetch_group_data(fetcher, TARGETS_US_MAG7, "ç¾è‚¡ä¸ƒå·¨å¤´")
    all_data_collection["data"]["ç¾è‚¡ä¸ƒå·¨å¤´"] = data_us
    all_ma_data.extend(ma_us)
    all_status_logs.extend(logs_us)
    
    # 5. æŠ“å–æ¸¯è‚¡åˆ›æ–°è¯
    data_hk, ma_hk, logs_hk = fetch_group_data(fetcher, TARGETS_HK_PHARMA, "æ¸¯è‚¡åˆ›æ–°è¯")
    all_data_collection["data"]["æ¸¯è‚¡åˆ›æ–°è¯"] = data_hk
    all_ma_data.extend(ma_hk)
    all_status_logs.extend(logs_hk)
    
    # 6. æŠ“å–æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°
    data_hc, ma_hc, logs_hc = fetch_group_data(fetcher, TARGETS_HK_HEALTHCARE, "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°")
    all_data_collection["data"]["æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°"] = data_hc
    all_ma_data.extend(ma_hc)
    all_status_logs.extend(logs_hc)

    # [æ–°å¢] 7. æŠ“å–ç§‘åˆ›50 ETF
    data_star_etf, ma_star_etf, logs_star_etf = fetch_group_data(fetcher, TARGETS_STAR50_ETF, "ç§‘åˆ›50ETF")
    all_data_collection["data"]["ç§‘åˆ›50ETF"] = data_star_etf
    all_ma_data.extend(ma_star_etf)
    all_status_logs.extend(logs_star_etf)

    # [æ–°å¢] 8. æŠ“å–ç§‘åˆ›50æŒä»“
    data_star_holdings, ma_star_holdings, logs_star_holdings = fetch_group_data(fetcher, TARGETS_STAR50_HOLDINGS, "ç§‘åˆ›50æŒä»“")
    all_data_collection["data"]["ç§‘åˆ›50æŒä»“"] = data_star_holdings
    all_ma_data.extend(ma_star_holdings)
    all_status_logs.extend(logs_star_holdings)
    
    # å°†æ±‡æ€»çš„å‡çº¿æ•°æ®å­˜å…¥
    all_data_collection["ma_data"] = all_ma_data
    
    print("\nğŸ‰ Kçº¿æ•°æ®æŠ“å– & å‡çº¿è®¡ç®— ä»»åŠ¡å¤„ç†å®Œæˆï¼")
    return all_data_collection, all_status_logs

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œæ‰§è¡Œé»˜è®¤é€»è¾‘å¹¶ä¿å­˜æ–‡ä»¶
    data, _ = get_all_kline_data()
    
    output_filename = "é‡‘èæ•°æ®.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³ {output_filename}")