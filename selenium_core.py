# selenium_core.py
# -----------------------------------------------------------------------------
# DeepSeek Finance Project - Selenium Scraper Core Logic
# -----------------------------------------------------------------------------

import time
import json
import pandas as pd
import re
from io import StringIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class MacroDataScraper:
    def __init__(self):
        # ç›®æ ‡æ•°æ®æºé…ç½®
        # [ä¿®æ”¹] å·²ç§»é™¤ "ä¸­å›½_å—å‘èµ„é‡‘" ä»¥é¿å…å†—ä½™å’Œ 0 è®°å½•é—®é¢˜
        self.targets = {
            "ä¸­å›½_CPI": "https://data.eastmoney.com/cjsj/cpi.html",
            "ä¸­å›½_PMI": "https://data.eastmoney.com/cjsj/pmi.html",
            "ä¸­å›½_PPI": "https://data.eastmoney.com/cjsj/ppi.html",
            "ä¸­å›½_è´§å¸ä¾›åº”é‡": "https://data.eastmoney.com/cjsj/hbgyl.html",
            "ä¸­å›½_LPR": "https://data.eastmoney.com/cjsj/globalRateLPR.html",
            "ç¾å›½_ISMåˆ¶é€ ä¸šPMI": "https://data.eastmoney.com/cjsj/foreign_0_0.html",
            "ç¾å›½_ISMéåˆ¶é€ ä¸šæŒ‡æ•°": "https://data.eastmoney.com/cjsj/foreign_0_1.html",
            "ç¾å›½_éå†œå°±ä¸š": "https://data.eastmoney.com/cjsj/foreign_0_2.html",
            "ç¾å›½_æ ¸å¿ƒé›¶å”®é”€å”®æœˆç‡": "https://data.eastmoney.com/cjsj/foreign_0_9.html",
            "ç¾å›½_åˆ©ç‡å†³è®®": "https://data.eastmoney.com/cjsj/foreign_8_0.html",
            "æ—¥æœ¬_å¤®è¡Œåˆ©ç‡å†³è®®": "https://data.eastmoney.com/cjsj/foreign_3_0.html",
            "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°": "https://cn.investing.com/indices/hang-seng-healthcare-historical-data",
            "CNN_FearGreed": "https://edition.cnn.com/markets/fear-and-greed"
        }

        self.key_mapping = {
            "ä¸­å›½_CPI": ("china", "CPI"),
            "ä¸­å›½_PMI": ("china", "PMI_åˆ¶é€ ä¸š"),
            "ä¸­å›½_PPI": ("china", "PPI"),
            "ä¸­å›½_è´§å¸ä¾›åº”é‡": ("china", "è´§å¸ä¾›åº”é‡"),
            "ä¸­å›½_LPR": ("china", "LPR"),
            "ç¾å›½_ISMåˆ¶é€ ä¸šPMI": ("usa", "ISM_åˆ¶é€ ä¸šPMI"),
            "ç¾å›½_ISMéåˆ¶é€ ä¸šæŒ‡æ•°": ("usa", "ISM_éåˆ¶é€ ä¸šPMI"),
            "ç¾å›½_éå†œå°±ä¸š": ("usa", "éå†œå°±ä¸šäººæ•°"),
            "ç¾å›½_æ ¸å¿ƒé›¶å”®é”€å”®æœˆç‡": ("usa", "é›¶å”®é”€å”®æœˆç‡"),
            "ç¾å›½_åˆ©ç‡å†³è®®": ("usa", "åˆ©ç‡å†³è®®"),
            "æ—¥æœ¬_å¤®è¡Œåˆ©ç‡å†³è®®": ("japan", "å¤®è¡Œåˆ©ç‡"),
            "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°": ("hk", "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°"),
            "CNN_FearGreed": ("market_fx", "CNN_FearGreed")
        }
        
        self.results = {}
        self.status_logs = []
        
        self.chrome_options = Options()
        self.chrome_options.add_argument("--headless")
        self.chrome_options.add_argument("--disable-gpu")
        self.chrome_options.add_argument("--log-level=3")
        self.chrome_options.add_argument("--no-sandbox")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        self.chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36')
        
        self.chrome_options.page_load_strategy = 'eager'

        prefs = {"profile.managed_default_content_settings.images": 2}
        self.chrome_options.add_experimental_option("prefs", prefs)
        
        self.output_path = "OnlineReport.json"

    def clean_date(self, date_str):
        try:
            date_str = str(date_str).strip()
            if "å¹´" in date_str:
                clean_str = date_str.replace("æœˆä»½", "").replace("æœˆ", "").replace("æ—¥", "").replace("å¹´", "-")
                if clean_str.count("-") == 1:
                    clean_str += "-01"
                return pd.to_datetime(clean_str)
            return pd.to_datetime(date_str)
        except Exception:
            return pd.NaT

    def _clean_investing_date(self, date_str):
        try:
            date_str = str(date_str).strip()
            if "å¹´" in date_str:
                clean_str = date_str.replace("å¹´", "-").replace("æœˆä»½", "").replace("æœˆ", "-").replace("æ—¥", "")
                clean_str = re.sub(r'-+', '-', clean_str)
                if clean_str.count("-") == 1:
                    clean_str += "-01"
                return pd.to_datetime(clean_str)
            return pd.to_datetime(date_str)
        except Exception:
            return pd.NaT

    def parse_volume(self, vol_str):
        if not isinstance(vol_str, str):
            return vol_str
        
        vol_str = vol_str.upper().strip()
        if vol_str in ['-', '', 'NAN', 'NONE']:
            return 0.0
            
        multiplier = 1.0
        if 'K' in vol_str:
            multiplier = 1000.0
            vol_str = vol_str.replace('K', '')
        elif 'M' in vol_str:
            multiplier = 1000000.0
            vol_str = vol_str.replace('M', '')
        elif 'B' in vol_str:
            multiplier = 1000000000.0
            vol_str = vol_str.replace('B', '')
            
        try:
            return float(vol_str) * multiplier
        except:
            return 0.0

    def parse_percentage(self, pct_str):
        if not isinstance(pct_str, str):
            return pct_str
        try:
            return float(pct_str.replace('%', '').replace(',', ''))
        except:
            return 0.0

    def fetch_cnn_fear_greed(self, name, url):
        """
        ä¸“é—¨æŠ“å– CNN Fear & Greed Index
        ç»“æ„å˜åŠ¨é¢‘ç¹ï¼Œæ”¹ä¸ºéé¡ºåºçš„ç‹¬ç«‹æ­£åˆ™åŒ¹é…ã€‚
        ä¼˜å…ˆåŒ¹é… "Fear & Greed Index [Num]" (Header) æˆ– "Timeline [Num]"
        """
        max_retries = 5
        last_error = None
        
        for attempt in range(1, max_retries + 1):
            print(f"ğŸŒ [{name}] ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯• (Selenium - CNN)...")
            driver = None
            try:
                driver = webdriver.Chrome(options=self.chrome_options)
                
                driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                    "source": """Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"""
                })

                # è®¾ç½®å¤§çª—å£ä»¥ç¡®ä¿æ¡Œé¢å¸ƒå±€
                driver.set_window_size(1920, 1080)
                driver.set_page_load_timeout(45)
                driver.get(url)

                try:
                    print(f"   [Debug] Page Title: {driver.title}")
                except:
                    pass
                
                # æ»šåŠ¨é¡µé¢
                try:
                    driver.execute_script("window.scrollBy(0, 500);")
                    time.sleep(2)
                except:
                    pass
                
                # å°è¯•ç­‰å¾… Timelineï¼Œä½†ä¸å¼ºæ±‚ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¸åŒ…å«æ•°å­—
                try:
                    WebDriverWait(driver, 15).until(
                        EC.text_to_be_present_in_element((By.TAG_NAME, "body"), "Timeline")
                    )
                except:
                    print(f"âš ï¸ [{name}] ç­‰å¾… 'Timeline' å…³é”®å­—è¶…æ—¶ï¼Œç»§ç»­è§£æ...")
                
                body_text = driver.find_element(By.TAG_NAME, "body").text
                print(f"   [Debug] Body text length: {len(body_text)}")
                
                # æ–‡æœ¬è§„èŒƒåŒ–
                normalized_text = re.sub(r'\s+', ' ', body_text).strip()
                
                # --- ç‹¬ç«‹è§£æå„ä¸ªæŒ‡æ ‡ ---
                
                # 1. å½“å‰å€¼ (Current Value)
                # ç­–ç•¥ A: "Fear & Greed Index 51" (é€šå¸¸åœ¨ Header)
                # ç­–ç•¥ B: "Timeline 51" (æœ‰æ—¶åœ¨ Body)
                # ç­–ç•¥ C: "Now: 51"
                current_val = None
                
                match_header = re.search(r"Fear & Greed Index\s+(\d+)", normalized_text, re.IGNORECASE)
                if match_header:
                    current_val = int(match_header.group(1))
                else:
                    match_timeline = re.search(r"Timeline\s+(\d+)", normalized_text, re.IGNORECASE)
                    if match_timeline:
                        current_val = int(match_timeline.group(1))

                # 2. å†å²å€¼
                prev_close = 0
                week_ago = 0
                month_ago = 0
                year_ago = 0
                
                m_prev = re.search(r"Previous close\s+(\d+)", normalized_text, re.IGNORECASE)
                if m_prev: prev_close = int(m_prev.group(1))
                
                m_week = re.search(r"1 week ago\s+(\d+)", normalized_text, re.IGNORECASE)
                if m_week: week_ago = int(m_week.group(1))
                
                m_month = re.search(r"1 month ago\s+(\d+)", normalized_text, re.IGNORECASE)
                if m_month: month_ago = int(m_month.group(1))
                
                # "1 year ago" ç»å¸¸å› ä¸ºé¡µé¢æˆªæ–­æˆ–æ‡’åŠ è½½ç¼ºå¤±ï¼Œè®¾ä¸ºå¯é€‰
                m_year = re.search(r"1 year ago\s+(\d+)", normalized_text, re.IGNORECASE)
                if m_year: year_ago = int(m_year.group(1))
                
                # --- éªŒè¯ç»“æœ ---
                if current_val is not None:
                    record = {
                        "æ—¥æœŸ": pd.Timestamp.now().strftime('%Y-%m-%d'),
                        "æœ€æ–°å€¼": current_val,
                        "å‰å€¼": prev_close,
                        "ä¸€å‘¨å‰": week_ago,
                        "ä¸€æœˆå‰": month_ago,
                        "ä¸€å¹´å‰": year_ago, # å¯èƒ½ä¸º 0
                        "description": "CNN Fear & Greed Index"
                    }
                    print(f"âœ… [{name}] æŠ“å–æˆåŠŸ! å½“å‰å€¼: {current_val} (å‰å€¼:{prev_close}, 1å‘¨:{week_ago})")
                    return name, [record], None
                else:
                    # Debug è¾…åŠ©
                    print(f"âš ï¸ æœªæ‰¾åˆ°å½“å‰å€¼ (Current Value)ã€‚")
                    key_index = normalized_text.lower().find("timeline")
                    if key_index != -1:
                        preview = normalized_text[max(0, key_index-50):min(len(normalized_text), key_index+100)]
                        print(f"   [Debug Context] ...{preview}...")
                    raise ValueError("æ— æ³•è§£æå½“å‰ææƒ§è´ªå©ªæŒ‡æ•°æ•°å€¼")

            except Exception as e:
                last_error = str(e)
                print(f"âŒ [{name}] å¤±è´¥: {str(e)[:100]}")
                if attempt < max_retries:
                    time.sleep(3)
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
                        
        return name, [], last_error

    def fetch_investing_source(self, name, url):
        max_retries = 5
        last_error = None
        
        for attempt in range(1, max_retries + 1):
            print(f"ğŸŒ [{name}] ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯• (Selenium - Investingä¸“çº¿)...")
            driver = None
            try:
                driver = webdriver.Chrome(options=self.chrome_options)
                
                driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                    "source": """Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"""
                })

                driver.set_page_load_timeout(60)
                driver.set_script_timeout(60)
                
                driver.get(url)
                
                try:
                    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "table")))
                except:
                    print(f"âš ï¸ [{name}] ç­‰å¾…è¡¨æ ¼è¶…æ—¶ï¼Œå°è¯•ç»§ç»­è§£ææºç ...")
                
                html = driver.page_source
                dfs = pd.read_html(StringIO(html))
                
                if not dfs:
                    raise ValueError("é¡µé¢è§£æä¸ºç©ºï¼Œæœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®")

                target_df = None
                for df in dfs:
                    cols = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                    if all(k in cols for k in ['æ—¥æœŸ', 'æ”¶ç›˜', 'äº¤æ˜“é‡']):
                        df.columns = cols 
                        target_df = df
                        break
                
                if target_df is None:
                    for df in dfs:
                        cols = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                        if 'æ—¥æœŸ' in cols and 'æ”¶ç›˜' in cols:
                            df.columns = cols
                            target_df = df
                            break

                if target_df is None:
                     raise ValueError(f"æœªæ‰¾åˆ°ç¬¦åˆ Investing æ ¼å¼çš„è¡¨æ ¼ (æ£€æŸ¥åˆ—å: æ—¥æœŸ/æ”¶ç›˜/äº¤æ˜“é‡)")

                df = target_df.copy()
                
                df['_std_date'] = df['æ—¥æœŸ'].apply(self._clean_investing_date)
                
                if df['_std_date'].isna().all() and not df.empty:
                    print(f"âŒ [{name}] æ—¥æœŸè§£æå…¨éƒ¨å¤±è´¥! åŸå§‹æ•°æ®ç¤ºä¾‹: {df['æ—¥æœŸ'].iloc[0]}")
                
                df = df.dropna(subset=['_std_date'])
                df['_std_date'] = pd.to_datetime(df['_std_date'])
                
                cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=180)
                df = df[df['_std_date'] >= cutoff_date]
                
                df['_std_date'] = df['_std_date'].dt.strftime('%Y-%m-%d')
                
                rename_map = {
                    'æ—¥æœŸ': 'æ—¥æœŸ', 
                    'æ”¶ç›˜': 'close', 
                    'å¼€ç›˜': 'open',
                    'é«˜': 'high', 
                    'ä½': 'low', 
                    'äº¤æ˜“é‡': 'volume', 
                    'æ¶¨è·Œå¹…': 'change_pct'
                }
                
                available_map = {k: v for k, v in rename_map.items() if k in df.columns}
                df = df.rename(columns=available_map)
                
                if 'volume' in df.columns:
                    df['volume'] = df['volume'].apply(self.parse_volume)
                
                for col in ['close', 'open', 'high', 'low']:
                    if col in df.columns:
                        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        
                if 'change_pct' in df.columns:
                    df['change_pct'] = df['change_pct'].apply(self.parse_percentage)

                keep_cols = ['_std_date'] + list(available_map.values())
                keep_cols = list(dict.fromkeys(keep_cols))
                final_cols = [c for c in keep_cols if c in df.columns]
                
                df = df[final_cols]
                df.rename(columns={'_std_date': 'æ—¥æœŸ'}, inplace=True)
                
                records = df.to_dict('records')
                print(f"âœ… [{name}] æŠ“å–æˆåŠŸ! è·å¾— {len(records)} æ¡è®°å½•")
                return name, records, None 

            except Exception as e:
                last_error = str(e)
                print(f"âŒ [{name}] å¤±è´¥: {str(e)[:100]}")
                if attempt < max_retries:
                    time.sleep(2)
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        return name, [], last_error

    def fetch_single_source(self, name, url):
        if name == "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°":
            return self.fetch_investing_source(name, url)
        
        if name == "CNN_FearGreed":
            return self.fetch_cnn_fear_greed(name, url)

        max_retries = 5
        days_to_keep = 30 if "å—å‘èµ„é‡‘" in name else 180
        last_error = None

        for attempt in range(1, max_retries + 1):
            print(f"ğŸŒ [{name}] ç¬¬ {attempt}/{max_retries} æ¬¡å°è¯• (Selenium)...")
            driver = None
            try:
                driver = webdriver.Chrome(options=self.chrome_options)
                
                driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                    "source": """
                        Object.defineProperty(navigator, 'webdriver', {
                            get: () => undefined
                        })
                    """
                })

                driver.set_page_load_timeout(30)
                driver.set_script_timeout(30)
                
                driver.get(url)
                
                try:
                    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "table")))
                except Exception:
                    print(f"âš ï¸ [{name}] ç­‰å¾…è¡¨æ ¼è¶…æ—¶ï¼Œå°è¯•ç»§ç»­è§£ææºç ...")
                
                html = driver.page_source
                dfs = pd.read_html(StringIO(html))
                
                if not dfs:
                    raise ValueError("é¡µé¢è§£æä¸ºç©ºï¼Œæœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®")

                target_df = None
                
                for df in dfs:
                    df.columns = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                    
                    possible_date_cols = ['æœˆä»½', 'æ—¶é—´', 'æ—¥æœŸ', 'å‘å¸ƒæ—¥æœŸ', 'å…¬å¸ƒæ—¥æœŸ']
                    if any(x in str(col) for x in df.columns for col in possible_date_cols):
                        if target_df is None or len(df) > len(target_df):
                            target_df = df
                
                if target_df is None:
                    target_df = max(dfs, key=lambda x: len(x))

                df = target_df
                
                if isinstance(df.columns, pd.MultiIndex):
                    new_cols = []
                    for col in df.columns:
                        valid_parts = [str(c) for c in col if "Unnamed" not in str(c) and str(c).strip() != ""]
                        seen = set()
                        unique_parts = [x for x in valid_parts if not (x in seen or seen.add(x))]
                        new_cols.append("".join(unique_parts))
                    df.columns = new_cols
                
                df.columns = [str(c).replace(" ", "").replace("\n", "").strip() for c in df.columns]
                
                possible_date_cols = ['æœˆä»½', 'æ—¶é—´', 'æ—¥æœŸ', 'å‘å¸ƒæ—¥æœŸ', 'å…¬å¸ƒæ—¥æœŸ']
                date_col = next((col for col in df.columns if any(x in str(col) for x in possible_date_cols)), None)
                
                if date_col:
                    df['_std_date'] = df[date_col].apply(self.clean_date)
                    df = df.dropna(subset=['_std_date'])
                    
                    df['_std_date'] = pd.to_datetime(df['_std_date'])
                    
                    cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=days_to_keep)
                    df = df[df['_std_date'] >= cutoff_date]
                    
                    df['_std_date'] = df['_std_date'].dt.strftime('%Y-%m-%d')
                    df = df.replace({'-': None, 'nan': None})
                    
                    # [ä¿®æ”¹] å—å‘èµ„é‡‘é€»è¾‘å·²ä» targets ç§»é™¤ï¼Œè¿™é‡Œçš„æ¸…ç†é€»è¾‘ä»…ä¸ºé˜²å¾¡æ€§ä¿ç•™
                    if name == "ä¸­å›½_å—å‘èµ„é‡‘":
                        df = df.where(pd.notnull(df), None)
                        keep_cols = ['_std_date']
                        for c in df.columns:
                            if "å‡€ä¹°é¢" in c and "å½“æ—¥" in c:
                                keep_cols.append(c)
                            elif "æˆäº¤ç¬”æ•°" in c:
                                keep_cols.append(c)
                        df = df[keep_cols]
                        df.rename(columns={'_std_date': 'æ—¥æœŸ'}, inplace=True)
                    else:
                        df = df.where(pd.notnull(df), None)
                        if 'æ—¥æœŸ' not in df.columns and '_std_date' in df.columns:
                            df['æ—¥æœŸ'] = df['_std_date']

                    records = df.to_dict('records')
                    print(f"âœ… [{name}] æŠ“å–æˆåŠŸ! è·å¾— {len(records)} æ¡è®°å½•")
                    return name, records, None
                else:
                    raise ValueError(f"æœªæ‰¾åˆ°æ—¥æœŸåˆ—: {df.columns.tolist()}")

            except Exception as e:
                last_error = str(e)
                print(f"âŒ [{name}] å¤±è´¥: {last_error[:200]}") 
                if attempt < max_retries:
                    time.sleep(2)
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        return name, [], last_error

    def run_concurrent(self):
        print("ğŸš€ [Scraper] æ­£åœ¨å¹¶å‘æŠ“å–å®è§‚æ•°æ® (Workers=2)...")
        self.status_logs = []
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_name = {
                executor.submit(self.fetch_single_source, name, url): name 
                for name, url in self.targets.items()
            }
            for future in as_completed(future_to_name):
                name, data, error_msg = future.result()
                if not error_msg:
                    self.results[name] = data
                    self.status_logs.append({'name': name, 'status': True, 'error': None})
                else:
                    self.results[name] = []
                    self.status_logs.append({'name': name, 'status': False, 'error': error_msg})
                    
        return self.results, self.status_logs

    def organize_data(self):
        nested_data = {
            "china": {},
            "usa": {},
            "japan": {},
            "hk": {},
            "market_fx": {}
        }
        
        for old_key, data_list in self.results.items():
            if not data_list:
                continue
            if old_key in self.key_mapping:
                country_key, metric_key = self.key_mapping[old_key]
                if country_key not in nested_data:
                    nested_data[country_key] = {}
                nested_data[country_key][metric_key] = data_list
        
        return nested_data

    def get_data_dict(self):
        self.run_concurrent()
        return self.organize_data(), self.status_logs