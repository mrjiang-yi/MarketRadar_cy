import json
import os
import sys
import time
import math
import pandas as pd  # æ–°å¢: ç”¨äºæ•°æ®å¤„ç†
from datetime import datetime
from zoneinfo import ZoneInfo  # æ–°å¢: æ—¶åŒºå¤„ç†

# -----------------------------------------------------------------------------
# è·¯å¾„å…¼å®¹å¤„ç†ï¼šç¡®ä¿èƒ½å¯¼å…¥ä¸Šå±‚æˆ–åŒçº§æ¨¡å—
# -----------------------------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

# å¼•å…¥æ¨¡å—
import fetch_data  # è´Ÿè´£ FX å’Œ å›½å€º
import MarketRadar # è´Ÿè´£ Kçº¿ å’Œ å‡çº¿
import utils       # æ–°å¢: è´Ÿè´£è®¡ç®—å‡çº¿ (calculate_ma)

try:
    import scrape_economy_selenium # è´Ÿè´£ CPI/PPI ç­‰å®è§‚æŒ‡æ ‡ (Root Dir)
except ImportError:
    # å°è¯•ä»åŒçº§ç›®å½•å¯¼å…¥ï¼ˆå¦‚æœéƒ¨ç½²ç»“æ„æ‰å¹³åŒ–ï¼‰
    import scrape_economy_selenium

# è¾“å‡ºæ–‡ä»¶åç§°
OUTPUT_FILENAME = "MarketRadar_Report.json"
LOG_FILENAME = "market_data_status.txt"

# å®šä¹‰åŒ—äº¬æ—¶åŒº
TZ_CN = ZoneInfo("Asia/Shanghai")

def print_banner():
    print(r"""
  __  __            _        _   ____          _            
 |  \/  | __ _ _ __| | _____| |_|  _ \ __ _ __| | __ _ _ __ 
 | |\/| |/ _` | '__| |/ / _ \ __| |_) / _` / _` |/ _` | '__|
 | |  | | (_| | |  |   <  __/ |_|  _ < (_| (_| | (_| | |   
 |_|  |_|\__,_|_|  |_|\_\___|\__|_| \_\__,_\__,_|\__,_|_|   
                                                            
    """)

def clean_and_round(data):
    """
    æ•°æ®æ¸…æ´—æ ¸å¿ƒé€»è¾‘ï¼š
    1. é€’å½’éå†å­—å…¸å’Œåˆ—è¡¨
    2. æµ®ç‚¹æ•°å¼ºåˆ¶ä¿ç•™2ä½å°æ•°
    3. å¤„ç†ç‰¹æ®Šæ•°å€¼ (NaN/Inf -> None)
    """
    if isinstance(data, dict):
        return {k: clean_and_round(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [clean_and_round(x) for x in data]
    elif isinstance(data, float):
        if math.isnan(data) or math.isinf(data):
            return None
        return round(data, 2)
    else:
        return data

def deep_merge(dict1, dict2):
    """
    æ·±åº¦åˆå¹¶ä¸¤ä¸ªå­—å…¸ (dict2 è¦†ç›–/è¡¥å…… dict1)
    """
    result = dict1.copy()
    for key, value in dict2.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
    return result

def merge_final_report(macro_data_combined, kline_data_dict, ma_data_list):
    """
    æ•´åˆæ‰€æœ‰æ¨¡å—çš„æ•°æ®
    """
    merged = {
        "meta": kline_data_dict.get("meta", {}),
        "æŠ€æœ¯åˆ†æ": {
            "å‘¨Kçº¿ç§»åŠ¨å¹³å‡çº¿": ma_data_list
        },
        # å®è§‚éƒ¨åˆ†
        "market_fx": macro_data_combined.get("market_fx", {}),
        "china": macro_data_combined.get("china", {}),
        "usa": macro_data_combined.get("usa", {}),
        "japan": macro_data_combined.get("japan", {}),
        "hk": macro_data_combined.get("hk", {}), # [ä¿®å¤] æ˜¾å¼åŒ…å«é¦™æ¸¯æ•°æ®(SeleniumæŠ“å–çš„æŒ‡æ•°)
        # Kçº¿æ•°æ®éƒ¨åˆ†
        "market_klines": kline_data_dict.get("data", {})
    }
    
    # å¼ºåˆ¶æ›´æ–° meta æ—¶é—´ä¸ºå½“å‰åŒ—äº¬æ—¶é—´
    merged["meta"]["generated_at"] = datetime.now(TZ_CN).strftime("%Y-%m-%d %H:%M:%S")
    merged["meta"]["description"] = "MarketRadar Consolidated Report (Selenium Macro + Online FX + Klines)"
    
    return merged

def save_compact_json(data, filename):
    """
    è‡ªå®šä¹‰ JSON ä¿å­˜ï¼šåˆ—è¡¨å¼ºåˆ¶å•è¡Œæ˜¾ç¤º
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('{\n')
            keys = list(data.keys())
            for i, key in enumerate(keys):
                val = data[key]
                f.write(f'    "{key}": ')
                if isinstance(val, dict):
                    f.write('{\n')
                    sub_keys = list(val.keys())
                    for j, sub_key in enumerate(sub_keys):
                        sub_val = val[sub_key]
                        f.write(f'        "{sub_key}": ')
                        if isinstance(sub_val, list):
                            f.write('[\n')
                            for k, item in enumerate(sub_val):
                                item_str = json.dumps(item, ensure_ascii=False)
                                comma = "," if k < len(sub_val) - 1 else ""
                                f.write(f'            {item_str}{comma}\n')
                            f.write('        ]')
                        else:
                            f.write(json.dumps(sub_val, ensure_ascii=False))
                        if j < len(sub_keys) - 1: f.write(',\n')
                        else: f.write('\n')
                    f.write('    }')
                else:
                    f.write(json.dumps(val, ensure_ascii=False))
                if i < len(keys) - 1: f.write(',\n')
                else: f.write('\n')
            f.write('}')
        print(f"\nâœ… æˆåŠŸ! æŠ¥å‘Šå·²å†™å…¥ {filename}")
        return True
    except Exception as e:
        print(f"\nâŒ å†™å…¥å¤±è´¥: {e}")
        return False

def write_status_log(logs, filename):
    """
    å†™å…¥çŠ¶æ€æ—¥å¿—æ–‡ä»¶
    æ ¼å¼: [æ—¶é—´] [çŠ¶æ€] åç§° | é”™è¯¯ä¿¡æ¯(å¦‚æœ‰)
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"MarketRadar Data Fetch Log - {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n")
            
            for log in logs:
                status_str = "[PASS]" if log['status'] else "[FAIL]"
                # æ—¥å¿—æ¡ç›®æ—¶é—´ä¹Ÿä½¿ç”¨åŒ—äº¬æ—¶é—´
                timestamp = datetime.now(TZ_CN).strftime('%H:%M:%S')
                line = f"[{timestamp}] {status_str} {log['name']}"
                if not log['status'] and log['error']:
                    line += f" | Error: {log['error']}"
                f.write(line + "\n")
        print(f"ğŸ“ çŠ¶æ€æ—¥å¿—å·²å†™å…¥: {filename}")
        return True
    except Exception as e:
        print(f"âŒ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
        return False

def generate_email_body_summary(logs):
    """
    ç”Ÿæˆé‚®ä»¶æ­£æ–‡çš„çŠ¶æ€æ±‡æ€» (ä»…å±•ç¤ºæˆåŠŸ/å¤±è´¥çŠ¶æ€ï¼Œä¸å±•ç¤ºå…·ä½“æŠ¥é”™)
    """
    lines = ["æ•°æ®è·å–çŠ¶æ€æ±‡æ€»:"]
    lines.append("-" * 30)
    
    # ç®€å•çš„åˆ†ç±»ç»Ÿè®¡
    success_count = sum(1 for l in logs if l['status'])
    fail_count = sum(1 for l in logs if not l['status'])
    
    lines.append(f"æ€»è®¡: {len(logs)} | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}")
    lines.append("")
    
    for log in logs:
        status_icon = "âœ…" if log['status'] else "âŒ"
        lines.append(f"{status_icon} {log['name']}")
    
    return "\n".join(lines)

def main():
    start_time = time.time()
    print_banner()
    print("ğŸš€ MarketRadar å¯åŠ¨ä¸»ç¨‹åº (Integrated Version)...")
    
    all_status_logs = []

    # 1. è·å–åŸºç¡€ FX å’Œ å›½å€ºæ•°æ® (fetch_data - Online Only)
    print("\n[Step 1/4] è·å–æ±‡ç‡ä¸å›½å€ºæ•°æ® (fetch_data)...")
    try:
        base_macro, logs_fx = fetch_data.get_market_fx_and_bonds()
        all_status_logs.extend(logs_fx)
    except Exception as e:
        print(f"âŒ fetch_data å¤±è´¥: {e}")
        base_macro = {"market_fx": {}, "china": {}, "usa": {}, "japan": {}}
        all_status_logs.append({'name': 'fetch_data_module', 'status': False, 'error': str(e)})

    # 2. è·å–å¤æ‚å®è§‚æ•°æ® (Selenium Scraper)
    print("\n[Step 2/4] æŠ“å–å®è§‚ç»æµæŒ‡æ ‡ (Selenium)...")
    try:
        # ç›´æ¥è°ƒç”¨ scrape_economy_selenium æš´éœ²çš„æ¥å£
        selenium_macro, logs_selenium = scrape_economy_selenium.get_macro_data()
        all_status_logs.extend(logs_selenium)
    except Exception as e:
        print(f"âŒ Selenium æŠ“å–å¤±è´¥ (å¯èƒ½æ˜¯ç¯å¢ƒé—®é¢˜): {e}")
        selenium_macro = {}
        all_status_logs.append({'name': 'selenium_module', 'status': False, 'error': str(e)})

    # åˆå¹¶å®è§‚æ•°æ® (Base + Selenium)
    combined_macro = deep_merge(base_macro, selenium_macro)

    # 3. è·å–Kçº¿ä¸å‡çº¿ (MarketRadar)
    print("\n[Step 3/4] è·å– Kçº¿æ•°æ® & è®¡ç®—å‡çº¿...")
    try:
        kline_result, logs_klines = MarketRadar.get_all_kline_data()
        all_status_logs.extend(logs_klines)
        
        kline_data_dict = {"meta": kline_result.get("meta"), "data": kline_result.get("data")}
        ma_data_list = kline_result.get("ma_data", [])
        print(f"âœ… è·å–åˆ° {len(ma_data_list)} æ¡å‡çº¿æ•°æ®")
    except Exception as e:
        print(f"âŒ è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
        kline_data_dict = {"meta": {}, "data": {}}
        ma_data_list = []
        all_status_logs.append({'name': 'kline_module', 'status': False, 'error': str(e)})

    # [Step 3.5 - ä¼˜åŒ–é€»è¾‘] ä»…è®¡ç®—æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°çš„å‡çº¿ (æ•°æ®ä¿ç•™åœ¨ hk å­—æ®µï¼Œä¸å¤åˆ¶åˆ° market_klines ä»¥å…é‡å¤)
    hshci_key = "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°"
    hk_data = combined_macro.get("hk", {})
    
    # 1. æ¸…ç† MarketRadar å¯èƒ½äº§ç”Ÿçš„ç©ºæ•°æ®/å¤±è´¥æ•°æ®
    if "data" in kline_data_dict and kline_data_dict["data"]:
        if hshci_key in kline_data_dict["data"]:
            del kline_data_dict["data"][hshci_key]
            print(f"ğŸ§¹ å·²ç§»é™¤ market_klines ä¸­çš„ {hshci_key} (é¿å…é‡å¤ï¼Œä»…ä¿ç•™ hk å­—æ®µæ•°æ®)")

    # 2. ä» hk æ•°æ®è®¡ç®—å‡çº¿
    if hshci_key in hk_data and hk_data[hshci_key]:
        print(f"\n[Step 3.5] âš¡ æ­£åœ¨åŸºäº Selenium æ•°æ®è®¡ç®— {hshci_key} å‡çº¿...")
        try:
            raw_data = hk_data[hshci_key]
            df_hshci = pd.DataFrame(raw_data)
            
            # é€‚é… utils.calculate_ma: éœ€è¦ 'date', 'close', 'name'
            if 'æ—¥æœŸ' in df_hshci.columns:
                df_hshci.rename(columns={'æ—¥æœŸ': 'date'}, inplace=True)
            
            df_hshci['name'] = hshci_key
            
            # ç±»å‹è½¬æ¢
            for col in ['close', 'open', 'high', 'low', 'volume']:
                if col in df_hshci.columns:
                    df_hshci[col] = pd.to_numeric(df_hshci[col], errors='coerce')

            if 'date' in df_hshci.columns:
                 df_hshci['date'] = pd.to_datetime(df_hshci['date'])
                 hshci_ma_list = utils.calculate_ma(df_hshci)
                 if hshci_ma_list:
                     ma_data_list.extend(hshci_ma_list)
                     print(f"âœ… {hshci_key} å‡çº¿è®¡ç®—å®Œæˆ")
        except Exception as e_ma:
             print(f"âš ï¸ {hshci_key} å‡çº¿è®¡ç®—å¤±è´¥: {e_ma}")

    # æ–°å¢: 4. æŠ“å–è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•° (VNI) Kçº¿ å¹¶è®¡ç®—å‡çº¿
    print("\n[Step 4/4] è·å–è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•° (Investing.com)...")
    try:
        vni_data, vni_err = fetch_data.fetch_vietnam_index_klines()
        if vni_data:
            # å­˜å…¥ kline_data_dict çš„ 'data' å­—æ®µï¼Œé”®åä¸º 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°'
            # ç¡®ä¿ data å­—æ®µå·²åˆå§‹åŒ–
            if "data" not in kline_data_dict or kline_data_dict["data"] is None:
                kline_data_dict["data"] = {}
                
            # [é‡è¦] ç¡®ä¿æ•°æ®ç»“æ„ä¸ä¸»ç¨‹åºä¸€è‡´
            kline_data_dict["data"]["è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"] = vni_data
            
            # --- è®¡ç®—è¶Šå—æŒ‡æ•°å‡çº¿ ---
            try:
                # è½¬æ¢ä¸º DataFrame æ ¼å¼é€‚é… utils.calculate_ma
                df_vni = pd.DataFrame(vni_data)
                df_vni['name'] = "è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°" # å¿…é¡»æ·»åŠ  name åˆ—
                
                # è®¡ç®—å‡çº¿
                vni_ma_list = utils.calculate_ma(df_vni)
                if vni_ma_list:
                    ma_data_list.extend(vni_ma_list)
                    print(f"âœ… è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–æˆåŠŸ ({len(vni_data)} æ¡è®°å½•) & å‡çº¿å·²è®¡ç®—")
                else:
                    print(f"âœ… è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–æˆåŠŸ ({len(vni_data)} æ¡è®°å½•) (å‡çº¿è®¡ç®—æ— ç»“æœ)")
                
                all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': None})
                
            except Exception as e_ma:
                print(f"âš ï¸ è¶Šå—æ•°æ®è·å–æˆåŠŸä½†å‡çº¿è®¡ç®—å¤±è´¥: {e_ma}")
                # ä»ç„¶æ ‡è®°ä¸ºæˆåŠŸï¼Œå› ä¸ºæ ¸å¿ƒæ•°æ®å·²è·å–
                all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': f"MA Error: {e_ma}"})
            # -----------------------
            
        else:
            all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': False, 'error': vni_err})
            print(f"âŒ è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–å¤±è´¥: {vni_err}")
    except Exception as e:
        print(f"âŒ è¶Šå—æŒ‡æ•°æ¨¡å—å¼‚å¸¸: {e}")
        all_status_logs.append({'name': 'vni_module', 'status': False, 'error': str(e)})

    # 5. æœ€ç»ˆæ•´åˆä¸æ¸…æ´—
    print("\n[Step 5] æ•´åˆæ•°æ®å¹¶æ¸…æ´—...")
    final_data = merge_final_report(combined_macro, kline_data_dict, ma_data_list)
    final_data = clean_and_round(final_data)

    # 6. ç”Ÿæˆæ—¥å¿—æ–‡ä»¶
    write_status_log(all_status_logs, LOG_FILENAME)

    # 7. ä¿å­˜ä¸å‘é€
    if save_compact_json(final_data, OUTPUT_FILENAME):
        # é‚®ä»¶é€»è¾‘
        try:
            # é‚®ä»¶ä¸»é¢˜ä½¿ç”¨åŒ—äº¬æ—¶é—´
            email_subject = f"MarketRadarå…¨é‡æ—¥æŠ¥_{datetime.now(TZ_CN).strftime('%Y-%m-%d')}"
            
            # æ„å»ºæ­£æ–‡ (ä½¿ç”¨åŒ—äº¬æ—¶é—´)
            # [æ›´æ–°] æè¿°ä¸­å¢åŠ  "ç§‘åˆ›50"
            base_body = f"ç”Ÿæˆæ—¶é—´: {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\nåŒ…å«: å®è§‚(Selenium), æ±‡ç‡/å›½å€º(Online), Kçº¿(Stock/VNI/ç§‘åˆ›50)\n\n"
            status_body = generate_email_body_summary(all_status_logs)
            email_body = base_body + status_body
            
            # é™„ä»¶åˆ—è¡¨ï¼šæ•°æ®æŠ¥å‘Š + çŠ¶æ€æ—¥å¿—
            attachments = [OUTPUT_FILENAME, LOG_FILENAME]
            
            MarketRadar.send_email(email_subject, email_body, attachments)
        except Exception as e:
            print(f"âš ï¸ é‚®ä»¶å‘é€è·³è¿‡æˆ–å¤±è´¥: {e}")

    print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")

if __name__ == "__main__":
    main()