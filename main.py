# import json
# import os
# import sys
# import time
# import math
# import numpy as np
# from datetime import datetime
# from zoneinfo import ZoneInfo

# # å¼•å…¥æ¨¡å—
# import fetch_data
# import MarketRadar
# import utils
# import scrape_economy_selenium

# # é…ç½®
# OUTPUT_FILENAME = "MarketRadar_Report.json"
# LOG_FILENAME = "market_data_status.txt"
# TZ_CN = ZoneInfo("Asia/Shanghai")

# class NpEncoder(json.JSONEncoder):
#     def default(self, obj):
#         if isinstance(obj, np.integer): return int(obj)
#         elif isinstance(obj, np.floating): return float(obj)
#         elif isinstance(obj, np.ndarray): return obj.tolist()
#         return super(NpEncoder, self).default(obj)

# def deep_merge(dict1, dict2):
#     result = dict1.copy()
#     for key, value in dict2.items():
#         if key in result and isinstance(result[key], dict) and isinstance(value, dict):
#             result[key] = deep_merge(result[key], value)
#         else:
#             result[key] = value
#     return result

# def main():
#     start_time = time.time()
#     print("ğŸš€ MarketRadar å¯åŠ¨...")
    
#     all_status_logs = []

#     # 1. åŸºç¡€ FX å’Œ å›½å€º
#     print("\n[Step 1] è·å–æ±‡ç‡ä¸å›½å€º...")
#     try:
#         base_macro, logs_fx = fetch_data.get_market_fx_and_bonds()
#         all_status_logs.extend(logs_fx)
#     except Exception as e:
#         print(f"âŒ fetch_data å¤±è´¥: {e}")
#         base_macro = {}

#     # 2. å®è§‚ (Selenium)
#     print("\n[Step 2] æŠ“å–å®è§‚ç»æµ (Selenium)...")
#     try:
#         selenium_macro, logs_selenium = scrape_economy_selenium.get_macro_data()
#         all_status_logs.extend(logs_selenium)
#     except Exception as e:
#         print(f"âŒ Selenium å¤±è´¥: {e}")
#         selenium_macro = {}

#     combined_macro = deep_merge(base_macro, selenium_macro)

#     # 3. Kçº¿ä¸è‡ªå®šä¹‰æ ‡çš„
#     print("\n[Step 3] è·å– Kçº¿ & è‡ªå®šä¹‰æ ‡çš„...")
#     try:
#         kline_result, logs_klines = MarketRadar.get_all_kline_data()
#         all_status_logs.extend(logs_klines)
#     except Exception as e:
#         print(f"âŒ MarketRadar å¤±è´¥: {e}")
#         kline_result = {}

#     # 4. æ•´åˆ
#     final_report = {
#         "meta": kline_result.get("meta", {}),
#         "æŠ€æœ¯åˆ†æ": {
#             "æŒ‡æ•°+ä¸ªè‚¡æ—¥å‡çº¿": kline_result.get("ma_data", {}).get("general", []),
#             "å¤§å®—å•†å“": kline_result.get("ma_data", {}).get("commodities", [])
#         },
#         "å®è§‚æ•°æ®": combined_macro,
#         "market_klines": kline_result.get("data", {})
#     }

#     # 5. ä¿å­˜
#     with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
#         json.dump(final_report, f, ensure_ascii=False, indent=4, cls=NpEncoder)
#     print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {OUTPUT_FILENAME}")

#     # 6. ç”Ÿæˆæ—¥å¿—
#     with open(LOG_FILENAME, "w", encoding="utf-8") as f:
#         for log in all_status_logs:
#             status = "OK" if log['status'] else "FAIL"
#             f.write(f"{status} | {log['name']} | {log.get('error','')}\n")

#     # 7. å‘é€é‚®ä»¶
#     try:
#         MarketRadar.send_email(
#             f"MarketRadaræ—¥æŠ¥_{datetime.now(TZ_CN).strftime('%Y-%m-%d')}",
#             "åŒ…å«: å®è§‚, æ±‡ç‡, Kçº¿, è‡ªå®šä¹‰åŸºé‡‘\næ•°æ®è§é™„ä»¶ã€‚",
#             [OUTPUT_FILENAME, LOG_FILENAME]
#         )
#     except Exception as e:
#         print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")

#     # 8. ã€æ–°å¢ã€‘é£ä¹¦æ¨é€
#     print("\n[Step 8] æ¨é€é£ä¹¦...")
#     feishu_url = os.environ.get("FEISHU_WEBHOOK_URL")
#     if feishu_url:
#         utils.send_to_feishu(feishu_url, final_report)
#     else:
#         print("âš ï¸ æœªè®¾ç½® FEISHU_WEBHOOK_URLï¼Œè·³è¿‡ã€‚")

#     print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")

# if __name__ == "__main__":
#     main()




import json
import os
import sys
import time
import math
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from itertools import groupby

# ç¡®ä¿èƒ½å¯¼å…¥åŒçº§æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

import fetch_data
import MarketRadar
import utils
import scrape_economy_selenium
import fetch_data_core

OUTPUT_FILENAME = "MarketRadar_Report.json"
LOG_FILENAME = "market_data_status.txt"
TZ_CN = ZoneInfo("Asia/Shanghai")
REPORT_DAYS = 20

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer): return int(obj)
        elif isinstance(obj, np.floating): return float(obj)
        elif isinstance(obj, np.ndarray): return obj.tolist()
        return super(NpEncoder, self).default(obj)

def print_banner():
    print(r"""
  __  __            _        _   ____          _            
 |  \/  | __ _ _ __| | _____| |_|  _ \ __ _ __| | __ _ _ __ 
 | |\/| |/ _` | '__| |/ / _ \ __| |_) / _` / _` |/ _` | '__|
 | |  | | (_| | |  |   <  __/ |_|  _ < (_| (_| | (_| | |   
 |_|  |_|\__,_|_|  |_|\_\___|\__|_| \_\__,_\__,_|\__,_|_|   
    """)

def clean_and_round(data):
    if isinstance(data, dict):
        return {k: clean_and_round(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [clean_and_round(x) for x in data]
    elif isinstance(data, float):
        if math.isnan(data) or math.isinf(data): return None
        return round(data, 2)
    elif isinstance(data, (np.int64, np.int32)):
        return int(data)
    else:
        return data

def deep_merge(dict1, dict2):
    result = dict1.copy()
    for key, value in dict2.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
    return result

def merge_final_report(macro_data_combined, kline_data_dict, ma_data_dict, kcb50_data=None):
    merged = {
        "meta": kline_data_dict.get("meta", {}),
        "æŠ€æœ¯åˆ†æ": {
            "æŒ‡æ•°+ä¸ªè‚¡æ—¥å‡çº¿": ma_data_dict.get("general", []),
            "å¤§å®—å•†å“": ma_data_dict.get("commodities", [])
        },
        "market_fx": macro_data_combined.get("market_fx", {}),
        "ç§‘åˆ›50": kcb50_data if kcb50_data else {},
        "china": macro_data_combined.get("china", {}),
        "usa": macro_data_combined.get("usa", {}),
        "japan": macro_data_combined.get("japan", {}),
        "hk": macro_data_combined.get("hk", {}),
        # è¿™é‡Œæ±‡èšäº†æ‰€æœ‰Kçº¿æ•°æ®ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰æ ‡çš„ã€æŒ‡æ•°ç­‰
        "market_klines": kline_data_dict.get("data", {})
    }
    
    merged["meta"]["generated_at"] = datetime.now(TZ_CN).strftime("%Y-%m-%d %H:%M:%S")
    merged["meta"]["description"] = "MarketRadar Consolidated Report"
    return merged

def save_compact_json(data, filename):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('{\n')
            keys = list(data.keys())
            for i, key in enumerate(keys):
                val = data[key]
                f.write(f'    "{key}": ')
                if isinstance(val, dict):
                    f.write('{\n')
                    sub_keys = list(val.keys())
                    for j, sub_key in enumerate(sub_keys):
                        sub_val = val[sub_key]
                        f.write(f'        "{sub_key}": ')
                        if isinstance(sub_val, list):
                            f.write('[\n')
                            for k, item in enumerate(sub_val):
                                item_str = json.dumps(item, ensure_ascii=False, cls=NpEncoder)
                                comma = "," if k < len(sub_val) - 1 else ""
                                f.write(f'            {item_str}{comma}\n')
                            f.write('        ]')
                        else:
                            f.write(json.dumps(sub_val, ensure_ascii=False, cls=NpEncoder))
                        if j < len(sub_keys) - 1: f.write(',\n')
                        else: f.write('\n')
                    f.write('    }')
                else:
                    f.write(json.dumps(val, ensure_ascii=False, cls=NpEncoder))
                if i < len(keys) - 1: f.write(',\n')
                else: f.write('\n')
            f.write('}')
        print(f"\nâœ… æˆåŠŸ! æŠ¥å‘Šå·²å†™å…¥ {filename}")
        return True
    except Exception as e:
        print(f"\nâŒ å†™å…¥å¤±è´¥: {e}")
        return False

def write_status_log(logs, filename):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"MarketRadar Log - {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n")
            for log in logs:
                status_str = "[PASS]" if log['status'] else "[FAIL]"
                line = f"{status_str} {log['name']}"
                if not log['status'] and log['error']:
                    line += f" | Error: {log['error']}"
                f.write(line + "\n")
        print(f"ğŸ“ çŠ¶æ€æ—¥å¿—å·²å†™å…¥: {filename}")
        return True
    except Exception as e:
        return False

def generate_signals_summary(ma_data_dict):
    lines = []
    all_ma = ma_data_dict.get("general", []) + ma_data_dict.get("commodities", [])
    signals_found = False
    lines.append("\nğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡ä¿¡å·æ‰«æ:")
    lines.append("-" * 30)
    for item in all_ma:
        name = item.get('åç§°', item.get('name', 'Unknown'))
        signals = item.get('Signals', [])
        active_signals = [s for s in signals if s != "æ— ç‰¹æ®ŠæŠ€æœ¯å½¢æ€"]
        if active_signals:
            signals_found = True
            lines.append(f"ğŸ”´ [{name}]: {', '.join(active_signals)}")
    if not signals_found:
        lines.append("ä»Šæ—¥æ— ç‰¹æ®ŠæŠ€æœ¯ä¿¡å·ã€‚")
    return "\n".join(lines)

def generate_email_body_summary(logs, signal_summary):
    lines = ["ğŸ“Š æ•°æ®çŠ¶æ€æ±‡æ€»:"]
    success_count = sum(1 for l in logs if l['status'])
    fail_count = sum(1 for l in logs if not l['status'])
    lines.append(f"æ€»è®¡: {len(logs)} | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}")
    lines.append("")
    for log in logs:
        status_icon = "âœ…" if log['status'] else "âŒ"
        lines.append(f"{status_icon} {log['name']}")
    lines.append("\n" + signal_summary)
    return "\n".join(lines)

def parse_chinese_date(date_str):
    try:
        if 'å¹´' in str(date_str):
            return datetime.strptime(str(date_str).strip(), '%Yå¹´%mæœˆ%dæ—¥')
        return pd.to_datetime(date_str)
    except:
        return pd.to_datetime(date_str, errors='coerce')

# --- æ–°å¢æ ¸å¿ƒå‡½æ•°ï¼šè¡¥å…¨æ¶¨è·Œå¹… ---
def enrich_data_with_changes(final_data):
    """
    éå† market_klines ä¸‹æ‰€æœ‰æ ‡çš„ï¼Œå¦‚æœç¼ºå¤± chg_pct åˆ™æ‰‹åŠ¨è®¡ç®—
    """
    if "market_klines" not in final_data:
        return final_data

    print("\nâš¡ æ­£åœ¨è®¡ç®—ç¼ºå¤±çš„æ¶¨è·Œå¹…æ•°æ®...")
    for category, items in final_data["market_klines"].items():
        for item in items:
            # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„ change_pct, chg_pct, pct_chg
            existing_chg = item.get("change_pct") or item.get("chg_pct") or item.get("pct_chg")
            
            if existing_chg is not None and existing_chg != 0:
                # ç»Ÿä¸€å­—æ®µåä¸º chg_pct ä»¥ä¾¿ utils.py è¯»å–
                item['chg_pct'] = existing_chg
            else:
                # æ‰‹åŠ¨è®¡ç®—ï¼š(close - open) / open
                try:
                    close_p = float(item.get("close", 0))
                    open_p = float(item.get("open", 0))
                    if open_p != 0:
                        calculated_chg = round(((close_p - open_p) / open_p) * 100, 2)
                        item['chg_pct'] = calculated_chg
                        item['change_pct'] = calculated_chg # åŒå¤‡ä»½
                    else:
                        item['chg_pct'] = 0.0
                except:
                    item['chg_pct'] = 0.0
    return final_data

def main():
    start_time = time.time()
    print_banner()
    print("ğŸš€ MarketRadar å¯åŠ¨ä¸»ç¨‹åº (ä¿®å¤å®Œæ•´ç‰ˆ)...")
    
    all_status_logs = []

    # 1. åŸºç¡€ FX å’Œ å›½å€º
    print("\n[Step 1] è·å–æ±‡ç‡ä¸å›½å€º...")
    try:
        base_macro, logs_fx = fetch_data.get_market_fx_and_bonds()
        all_status_logs.extend(logs_fx)
    except Exception as e:
        print(f"âŒ fetch_data å¤±è´¥: {e}")
        base_macro = {"market_fx": {}, "china": {}, "usa": {}, "japan": {}}
        all_status_logs.append({'name': 'fetch_data_module', 'status': False, 'error': str(e)})

    # 2. å®è§‚ (Selenium)
    print("\n[Step 2] æŠ“å–å®è§‚ç»æµ (Selenium)...")
    try:
        selenium_macro, logs_selenium = scrape_economy_selenium.get_macro_data()
        all_status_logs.extend(logs_selenium)
    except Exception as e:
        print(f"âŒ Selenium æŠ“å–å¤±è´¥: {e}")
        selenium_macro = {}
        all_status_logs.append({'name': 'selenium_module', 'status': False, 'error': str(e)})

    combined_macro = deep_merge(base_macro, selenium_macro)

    # 3. Kçº¿ä¸è‡ªå®šä¹‰æ ‡çš„ (MarketRadar)
    print("\n[Step 3] è·å– Kçº¿ & è‡ªå®šä¹‰æ ‡çš„ (åˆ¸å•†/æœ‰è‰²ç­‰)...")
    try:
        kline_result, logs_klines = MarketRadar.get_all_kline_data()
        all_status_logs.extend(logs_klines)
        
        kline_data_dict = {"meta": kline_result.get("meta"), "data": kline_result.get("data")}
        ma_data_dict = kline_result.get("ma_data", {"general": [], "commodities": []})
        
        print(f"âœ… æŠ“å–å®Œæˆ: é€šç”¨ {len(ma_data_dict['general'])} æ¡, å•†å“ {len(ma_data_dict['commodities'])} æ¡")
    except Exception as e:
        print(f"âŒ MarketRadar å¤±è´¥: {e}")
        kline_data_dict = {"meta": {}, "data": {}}
        ma_data_dict = {"general": [], "commodities": []}
        all_status_logs.append({'name': 'kline_module', 'status': False, 'error': str(e)})

    # [Step 3.5] å¤„ç†æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°
    hshci_key = "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°"
    hk_data = combined_macro.get("hk", {})
    if "data" in kline_data_dict and kline_data_dict["data"]:
        if hshci_key in kline_data_dict["data"]:
            del kline_data_dict["data"][hshci_key]

    if hshci_key in hk_data and hk_data[hshci_key]:
        try:
            raw_data = hk_data[hshci_key]
            df_hshci = pd.DataFrame(raw_data)
            if 'æ—¥æœŸ' in df_hshci.columns: df_hshci['date'] = df_hshci['æ—¥æœŸ'].apply(parse_chinese_date)
            elif 'date' in df_hshci.columns: df_hshci['date'] = pd.to_datetime(df_hshci['date'])
            df_hshci['name'] = hshci_key
            for col in ['close', 'open', 'high', 'low', 'volume']:
                if col in df_hshci.columns: df_hshci[col] = pd.to_numeric(df_hshci[col], errors='coerce')

            if 'date' in df_hshci.columns:
                 hshci_ma = utils.calculate_ma(df_hshci)
                 if hshci_ma: ma_data_dict["general"].extend(hshci_ma)
                 
                 cutoff = pd.Timestamp.now() - pd.Timedelta(days=REPORT_DAYS)
                 df_slice = df_hshci[df_hshci['date'] >= cutoff].copy()
                 df_slice['date'] = df_slice['date'].dt.strftime('%Y-%m-%d')
                 combined_macro['hk'][hshci_key] = df_slice.to_dict(orient='records')
        except Exception as e:
             print(f"âš ï¸ {hshci_key} å¤„ç†å¤±è´¥: {e}")

    # [Step 4] è¶Šå—æŒ‡æ•°
    print("\n[Step 4] è·å–è¶Šå—æŒ‡æ•°...")
    try:
        vni_data, vni_err = fetch_data.fetch_vietnam_index_klines()
        if vni_data:
            if "data" not in kline_data_dict: kline_data_dict["data"] = {}
            kline_data_dict["data"]["è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"] = vni_data
            try:
                df_vni = pd.DataFrame(vni_data)
                df_vni['name'] = "è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"
                vni_ma = utils.calculate_ma(df_vni)
                if vni_ma: ma_data_dict["general"].extend(vni_ma)
                all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': None})
            except Exception as e:
                all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': f"MA Error: {e}"})
        else:
            all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': False, 'error': vni_err})
    except Exception as e:
        all_status_logs.append({'name': 'vni_module', 'status': False, 'error': str(e)})

    # [Step 4.5] Aè‚¡æŒ‡æ•°å‡çº¿è®¡ç®—
    ashare_list = combined_macro.get("market_klines", {}).pop("Aè‚¡æŒ‡æ•°", None)
    if ashare_list:
        print("\n[Step 4.5] è®¡ç®— Aè‚¡æŒ‡æ•° å‡çº¿...")
        try:
            ashare_list.sort(key=lambda x: x['name'])
            for name, group in groupby(ashare_list, key=lambda x: x['name']):
                records = list(group)
                records.sort(key=lambda x: x['date'])
                df_ashare = pd.DataFrame(records)
                df_ashare['date'] = pd.to_datetime(df_ashare['date'])
                for c in ['close', 'open', 'high', 'low', 'volume']:
                    if c in df_ashare.columns: df_ashare[c] = pd.to_numeric(df_ashare[c], errors='coerce')
                
                ma_res = utils.calculate_ma(df_ashare)
                if ma_res: ma_data_dict["general"].extend(ma_res)
                
                df_ashare['date'] = df_ashare['date'].dt.strftime('%Y-%m-%d')
                if "data" not in kline_data_dict: kline_data_dict["data"] = {}
                kline_data_dict["data"][name] = df_ashare.to_dict(orient='records')
        except Exception as e:
            print(f"âš ï¸ Aè‚¡æŒ‡æ•°å¤„ç†å¤±è´¥: {e}")

    # [Step 4.6] 60åˆ†é’ŸKçº¿
    kcb50_dict = {}
    try:
        kcb50_60m, err = fetch_data_core.fetch_kcb50_60m()
        kcb50_dict["ç§‘åˆ›50_60åˆ†é’ŸKçº¿"] = kcb50_60m if kcb50_60m else []
    except:
        kcb50_dict["ç§‘åˆ›50_60åˆ†é’ŸKçº¿"] = []
        
    china_data = combined_macro.get("china", {})
    for k in ["ç§‘åˆ›50å®æ—¶å¿«ç…§", "ç§‘åˆ›50èèµ„èåˆ¸", "ç§‘åˆ›50ä¼°å€¼"]:
        if k in china_data: kcb50_dict[k] = china_data.pop(k)

    try:
        hstech_60m, err = fetch_data_core.fetch_hstech_60m()
        if "hk" not in combined_macro: combined_macro["hk"] = {}
        combined_macro["hk"]["æ’ç”Ÿç§‘æŠ€æŒ‡æ•°_60m"] = hstech_60m if hstech_60m else []
    except:
        if "hk" not in combined_macro: combined_macro["hk"] = {}
        combined_macro["hk"]["æ’ç”Ÿç§‘æŠ€æŒ‡æ•°_60m"] = []

    # [Step 4.7] å…­å¤§é“¶è¡Œ
    try:
        bank_dfs = fetch_data_core.fetch_us_banks_daily()
        for df in bank_dfs:
            name = df['name'].iloc[0]
            ma_res = utils.calculate_ma(df)
            if ma_res: ma_data_dict["general"].extend(ma_res)
            
            cutoff = pd.Timestamp.now() - pd.Timedelta(days=REPORT_DAYS)
            df_slice = df[df['date'] >= cutoff].copy()
            df_slice['date'] = df_slice['date'].dt.strftime('%Y-%m-%d')
            
            if "data" not in kline_data_dict: kline_data_dict["data"] = {}
            kline_data_dict["data"][name] = df_slice.to_dict(orient='records')
            all_status_logs.append({'name': f"Bank_{name}", 'status': True, 'error': None})
    except Exception as e:
        print(f"âš ï¸ å…­å¤§é“¶è¡Œå¼‚å¸¸: {e}")

    # [Step 5] æ•´åˆä¸æ¸…æ´—
    print("\n[Step 5] æ•´åˆæ•°æ®å¹¶æ¸…æ´—...")
    final_data = merge_final_report(combined_macro, kline_data_dict, ma_data_dict, kcb50_data=kcb50_dict)
    
    # ğŸŒŸ å…³é”®ä¿®å¤ï¼šè¡¥å…¨æ¶¨è·Œå¹…æ•°æ® (Fix 0% issue)
    final_data = enrich_data_with_changes(final_data)
    
    final_data = clean_and_round(final_data)

    # æ—¥å¿—å¤„ç†
    success_names = set(log['name'] for log in all_status_logs if log.get('status'))
    cleaned_logs = [log for log in all_status_logs if log['status'] or log['name'] not in success_names]
    write_status_log(cleaned_logs, LOG_FILENAME)
    
    signal_summary = generate_signals_summary(ma_data_dict)
    print(signal_summary)

    # [Step 6] ä¿å­˜ & é‚®ä»¶
    if save_compact_json(final_data, OUTPUT_FILENAME):
        try:
            email_subject = f"MarketRadarå…¨é‡æ—¥æŠ¥_{datetime.now(TZ_CN).strftime('%Y-%m-%d')}"
            base_body = f"ç”Ÿæˆæ—¶é—´: {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            email_body = base_body + generate_email_body_summary(cleaned_logs, signal_summary)
            MarketRadar.send_email(email_subject, email_body, [OUTPUT_FILENAME, LOG_FILENAME])
        except Exception as e:
            print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")

    print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
    
    # [Step 7] é£ä¹¦æ¨é€
    print("\n[Step 7] æ¨é€è‡³é£ä¹¦...")
    feishu_url = os.environ.get("FEISHU_WEBHOOK_URL")
    if feishu_url:
        # ä½¿ç”¨è¡¥å…¨äº†æ¶¨è·Œå¹…çš„ final_data
        utils.send_to_feishu(feishu_url, final_data) 
    else:
        print("âš ï¸ æœªè®¾ç½® FEISHU_WEBHOOK_URL")

if __name__ == "__main__":
    main()











# import json
# import os
# import sys
# import time
# import math
# import pandas as pd
# import numpy as np
# from datetime import datetime, timedelta
# from zoneinfo import ZoneInfo
# from itertools import groupby

# current_dir = os.path.dirname(os.path.abspath(__file__))
# parent_dir = os.path.dirname(current_dir)
# if parent_dir not in sys.path:
#     sys.path.append(parent_dir)

# import fetch_data
# import MarketRadar
# import utils
# import scrape_economy_selenium
# # å¼•å…¥ fetch_data_core ä»¥ç›´æ¥è°ƒç”¨æ–°åŠŸèƒ½
# import fetch_data_core

# OUTPUT_FILENAME = "MarketRadar_Report.json"
# LOG_FILENAME = "market_data_status.txt"
# TZ_CN = ZoneInfo("Asia/Shanghai")

# # å®šä¹‰æŠ¥å‘Šçš„æ—¶é—´èŒƒå›´ï¼ˆç”¨äºæˆªå–æœ€ç»ˆå±•ç¤ºçš„æ•°æ®ï¼‰
# # è®¡ç®—å‡çº¿éœ€è¦æ›´é•¿çš„æ•°æ®ï¼Œä½†æŠ¥å‘Šåªå±•ç¤ºè¿‘æœŸ
# REPORT_DAYS = 20

# class NpEncoder(json.JSONEncoder):
#     """
#     ä¸“é—¨è§£å†³ 'Object of type int64 is not JSON serializable' é”™è¯¯çš„ç¼–ç å™¨
#     """
#     def default(self, obj):
#         if isinstance(obj, np.integer):
#             return int(obj)
#         elif isinstance(obj, np.floating):
#             return float(obj)
#         elif isinstance(obj, np.ndarray):
#             return obj.tolist()
#         return super(NpEncoder, self).default(obj)

# def print_banner():
#     print(r"""
#   __  __            _        _   ____          _            
#  |  \/  | __ _ _ __| | _____| |_|  _ \ __ _ __| | __ _ _ __ 
#  | |\/| |/ _` | '__| |/ / _ \ __| |_) / _` / _` |/ _` | '__|
#  | |  | | (_| | |  |   <  __/ |_|  _ < (_| (_| | (_| | |   
#  |_|  |_|\__,_|_|  |_|\_\___|\__|_| \_\__,_\__,_|\__,_|_|   
                                                            
#     """)

# def clean_and_round(data):
#     if isinstance(data, dict):
#         return {k: clean_and_round(v) for k, v in data.items()}
#     elif isinstance(data, list):
#         return [clean_and_round(x) for x in data]
#     elif isinstance(data, float):
#         if math.isnan(data) or math.isinf(data):
#             return None
#         return round(data, 2)
#     elif isinstance(data, (np.int64, np.int32)):
#         return int(data)
#     else:
#         return data

# def deep_merge(dict1, dict2):
#     result = dict1.copy()
#     for key, value in dict2.items():
#         if key in result and isinstance(result[key], dict) and isinstance(value, dict):
#             result[key] = deep_merge(result[key], value)
#         else:
#             result[key] = value
#     return result

# def merge_final_report(macro_data_combined, kline_data_dict, ma_data_dict, kcb50_data=None):
#     """
#     æ•´åˆæ‰€æœ‰æ¨¡å—çš„æ•°æ®
#     ma_data_dict: {"general": [...], "commodities": [...]}
#     kcb50_data: æ–°å¢çš„ç§‘åˆ›50ç‹¬ç«‹æ¿å—
#     """
#     merged = {
#         "meta": kline_data_dict.get("meta", {}),
#         "æŠ€æœ¯åˆ†æ": {
#             "æŒ‡æ•°+ä¸ªè‚¡æ—¥å‡çº¿": ma_data_dict.get("general", []),
#             "å¤§å®—å•†å“": ma_data_dict.get("commodities", [])
#         },
#         "market_fx": macro_data_combined.get("market_fx", {}),
#         "ç§‘åˆ›50": kcb50_data if kcb50_data else {},  # æ–°å¢é¡¶å±‚é¡¹
#         "china": macro_data_combined.get("china", {}),
#         "usa": macro_data_combined.get("usa", {}),
#         "japan": macro_data_combined.get("japan", {}),
#         "hk": macro_data_combined.get("hk", {}), 
#         "market_klines": kline_data_dict.get("data", {})
#     }
    
#     merged["meta"]["generated_at"] = datetime.now(TZ_CN).strftime("%Y-%m-%d %H:%M:%S")
#     merged["meta"]["description"] = "MarketRadar Consolidated Report (Selenium Macro + Online FX + Klines)"
    
#     return merged

# def save_compact_json(data, filename):
#     try:
#         with open(filename, 'w', encoding='utf-8') as f:
#             f.write('{\n')
#             keys = list(data.keys())
#             for i, key in enumerate(keys):
#                 val = data[key]
#                 f.write(f'    "{key}": ')
#                 if isinstance(val, dict):
#                     f.write('{\n')
#                     sub_keys = list(val.keys())
#                     for j, sub_key in enumerate(sub_keys):
#                         sub_val = val[sub_key]
#                         f.write(f'        "{sub_key}": ')
#                         if isinstance(sub_val, list):
#                             f.write('[\n')
#                             for k, item in enumerate(sub_val):
#                                 # ä½¿ç”¨ NpEncoder è§£å†³ int64 åºåˆ—åŒ–é”™è¯¯
#                                 item_str = json.dumps(item, ensure_ascii=False, cls=NpEncoder)
#                                 comma = "," if k < len(sub_val) - 1 else ""
#                                 f.write(f'            {item_str}{comma}\n')
#                             f.write('        ]')
#                         else:
#                             f.write(json.dumps(sub_val, ensure_ascii=False, cls=NpEncoder))
#                         if j < len(sub_keys) - 1: f.write(',\n')
#                         else: f.write('\n')
#                     f.write('    }')
#                 else:
#                     f.write(json.dumps(val, ensure_ascii=False, cls=NpEncoder))
#                 if i < len(keys) - 1: f.write(',\n')
#                 else: f.write('\n')
#             f.write('}')
#         print(f"\nâœ… æˆåŠŸ! æŠ¥å‘Šå·²å†™å…¥ {filename}")
#         return True
#     except Exception as e:
#         print(f"\nâŒ å†™å…¥å¤±è´¥: {e}")
#         return False

# def write_status_log(logs, filename):
#     try:
#         with open(filename, 'w', encoding='utf-8') as f:
#             f.write(f"MarketRadar Data Fetch Log - {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\n")
#             f.write("="*60 + "\n")
            
#             for log in logs:
#                 status_str = "[PASS]" if log['status'] else "[FAIL]"
#                 timestamp = datetime.now(TZ_CN).strftime('%H:%M:%S')
#                 line = f"[{timestamp}] {status_str} {log['name']}"
#                 if not log['status'] and log['error']:
#                     line += f" | Error: {log['error']}"
#                 f.write(line + "\n")
#         print(f"ğŸ“ çŠ¶æ€æ—¥å¿—å·²å†™å…¥: {filename}")
#         return True
#     except Exception as e:
#         print(f"âŒ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
#         return False

# def generate_signals_summary(ma_data_dict):
#     """
#     ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¿¡å·æ‘˜è¦
#     """
#     lines = []
    
#     # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
#     all_ma = ma_data_dict.get("general", []) + ma_data_dict.get("commodities", [])
    
#     signals_found = False
#     lines.append("\nğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡ä¿¡å·æ‰«æ:")
#     lines.append("-" * 30)
    
#     for item in all_ma:
#         # [ä¿®æ”¹] ä¼˜å…ˆè·å– "åç§°" (ä¸­æ–‡Key)
#         name = item.get('åç§°', item.get('name', 'Unknown'))
#         signals = item.get('Signals', [])
        
#         # [ä¿®æ”¹] è¿‡æ»¤æ‰ "æ— ç‰¹æ®ŠæŠ€æœ¯å½¢æ€"ï¼Œä¸åœ¨æ‘˜è¦ä¸­æ˜¾ç¤º
#         active_signals = [s for s in signals if s != "æ— ç‰¹æ®ŠæŠ€æœ¯å½¢æ€"]
        
#         if active_signals:
#             signals_found = True
#             lines.append(f"ğŸ”´ [{name}]: {', '.join(active_signals)}")
            
#     if not signals_found:
#         lines.append("ä»Šæ—¥æ— ç‰¹æ®ŠæŠ€æœ¯ä¿¡å·ã€‚")
        
#     return "\n".join(lines)

# def generate_email_body_summary(logs, signal_summary):
#     lines = ["ğŸ“Š æ•°æ®è·å–çŠ¶æ€æ±‡æ€»:"]
#     lines.append("-" * 30)
    
#     success_count = sum(1 for l in logs if l['status'])
#     fail_count = sum(1 for l in logs if not l['status'])
    
#     lines.append(f"æ€»è®¡: {len(logs)} | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}")
#     lines.append("")
    
#     for log in logs:
#         status_icon = "âœ…" if log['status'] else "âŒ"
#         lines.append(f"{status_icon} {log['name']}")
    
#     lines.append("\n" + signal_summary)
    
#     return "\n".join(lines)

# def parse_chinese_date(date_str):
#     """
#     è§£æ 'YYYYå¹´MMæœˆDDæ—¥' æˆ– 'YYYY-MM-DD' æ ¼å¼çš„æ—¥æœŸ
#     """
#     try:
#         if 'å¹´' in str(date_str):
#             return datetime.strptime(str(date_str).strip(), '%Yå¹´%mæœˆ%dæ—¥')
#         return pd.to_datetime(date_str)
#     except:
#         return pd.to_datetime(date_str, errors='coerce')

# def main():
#     start_time = time.time()
#     print_banner()
#     print("ğŸš€ MarketRadar å¯åŠ¨ä¸»ç¨‹åº (Integrated Version)...")
    
#     all_status_logs = []

#     print("\n[Step 1/4] è·å–æ±‡ç‡ä¸å›½å€ºæ•°æ® (fetch_data)...")
#     try:
#         base_macro, logs_fx = fetch_data.get_market_fx_and_bonds()
#         all_status_logs.extend(logs_fx)
#     except Exception as e:
#         print(f"âŒ fetch_data å¤±è´¥: {e}")
#         base_macro = {"market_fx": {}, "china": {}, "usa": {}, "japan": {}}
#         all_status_logs.append({'name': 'fetch_data_module', 'status': False, 'error': str(e)})

#     print("\n[Step 2/4] æŠ“å–å®è§‚ç»æµæŒ‡æ ‡ (Selenium)...")
#     try:
#         selenium_macro, logs_selenium = scrape_economy_selenium.get_macro_data()
#         all_status_logs.extend(logs_selenium)
#     except Exception as e:
#         print(f"âŒ Selenium æŠ“å–å¤±è´¥ (å¯èƒ½æ˜¯ç¯å¢ƒé—®é¢˜): {e}")
#         selenium_macro = {}
#         all_status_logs.append({'name': 'selenium_module', 'status': False, 'error': str(e)})

#     combined_macro = deep_merge(base_macro, selenium_macro)

#     print("\n[Step 3/4] è·å– Kçº¿æ•°æ® & è®¡ç®—å‡çº¿ & æŠ€æœ¯æŒ‡æ ‡...")
#     try:
#         kline_result, logs_klines = MarketRadar.get_all_kline_data()
#         all_status_logs.extend(logs_klines)
        
#         kline_data_dict = {"meta": kline_result.get("meta"), "data": kline_result.get("data")}
#         ma_data_dict = kline_result.get("ma_data", {"general": [], "commodities": []})
        
#         count_general = len(ma_data_dict.get("general", []))
#         count_comm = len(ma_data_dict.get("commodities", []))
#         print(f"âœ… è·å–åˆ°å‡çº¿æ•°æ®: é€šç”¨ {count_general} æ¡, å¤§å®—å•†å“ {count_comm} æ¡")
#     except Exception as e:
#         print(f"âŒ è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
#         kline_data_dict = {"meta": {}, "data": {}}
#         ma_data_dict = {"general": [], "commodities": []}
#         all_status_logs.append({'name': 'kline_module', 'status': False, 'error': str(e)})

#     # [Step 3.5] å¤„ç†æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°
#     hshci_key = "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°"
#     hk_data = combined_macro.get("hk", {})
    
#     if "data" in kline_data_dict and kline_data_dict["data"]:
#         if hshci_key in kline_data_dict["data"]:
#             del kline_data_dict["data"][hshci_key]
#             print(f"ğŸ§¹ å·²ä» market_klines å­—æ®µç§»é™¤ {hshci_key} (ä»…ä¿ç•™ hk å­—æ®µæ•°æ®ï¼Œé˜²æ­¢åŒä»½è¾“å‡º)")

#     if hshci_key in hk_data and hk_data[hshci_key]:
#         print(f"\n[Step 3.5] âš¡ æ­£åœ¨åŸºäº Selenium æ•°æ®è®¡ç®— {hshci_key} å‡çº¿...")
#         try:
#             raw_data = hk_data[hshci_key]
#             df_hshci = pd.DataFrame(raw_data)
            
#             if 'æ—¥æœŸ' in df_hshci.columns:
#                 df_hshci['date'] = df_hshci['æ—¥æœŸ'].apply(parse_chinese_date)
#             elif 'date' in df_hshci.columns:
#                 df_hshci['date'] = pd.to_datetime(df_hshci['date'])
            
#             df_hshci['name'] = hshci_key
#             for col in ['close', 'open', 'high', 'low', 'volume']:
#                 if col in df_hshci.columns:
#                     df_hshci[col] = pd.to_numeric(df_hshci[col], errors='coerce')

#             if 'date' in df_hshci.columns:
#                  hshci_ma_list = utils.calculate_ma(df_hshci)
#                  if hshci_ma_list:
#                      ma_data_dict["general"].extend(hshci_ma_list)
#                      print(f"âœ… {hshci_key} å‡çº¿è®¡ç®—å®Œæˆ")
                 
#                  cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=REPORT_DAYS)
#                  df_slice = df_hshci[df_hshci['date'] >= cutoff_date].copy()
#                  df_slice['date'] = df_slice['date'].dt.strftime('%Y-%m-%d')
                 
#                  sliced_records = df_slice.to_dict(orient='records')
#                  combined_macro['hk'][hshci_key] = sliced_records
#                  print(f"âœ‚ï¸ {hshci_key} æ•°æ®å·²åˆ‡ç‰‡ (ä¿ç•™æœ€è¿‘ {len(sliced_records)} æ¡)")

#         except Exception as e_ma:
#              print(f"âš ï¸ {hshci_key} å‡çº¿è®¡ç®—æˆ–åˆ‡ç‰‡å¤±è´¥: {e_ma}")

#     # [Step 4/4] è·å–è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•° (Investing.com)...
#     print("\n[Step 4/4] è·å–è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•° (Investing.com)...")
#     try:
#         vni_data, vni_err = fetch_data.fetch_vietnam_index_klines()
#         if vni_data:
#             if "data" not in kline_data_dict or kline_data_dict["data"] is None:
#                 kline_data_dict["data"] = {}
                
#             kline_data_dict["data"]["è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"] = vni_data
            
#             try:
#                 df_vni = pd.DataFrame(vni_data)
#                 df_vni['name'] = "è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"
                
#                 vni_ma_list = utils.calculate_ma(df_vni)
#                 if vni_ma_list:
#                     ma_data_dict["general"].extend(vni_ma_list)
#                     print(f"âœ… è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–æˆåŠŸ ({len(vni_data)} æ¡è®°å½•) & å‡çº¿å·²è®¡ç®—")
#                 else:
#                     print(f"âœ… è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–æˆåŠŸ ({len(vni_data)} æ¡è®°å½•) (å‡çº¿è®¡ç®—æ— ç»“æœ)")
                
#                 all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': None})
                
#             except Exception as e_ma:
#                 print(f"âš ï¸ è¶Šå—æ•°æ®è·å–æˆåŠŸä½†å‡çº¿è®¡ç®—å¤±è´¥: {e_ma}")
#                 all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': f"MA Error: {e_ma}"})
            
#         else:
#             all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': False, 'error': vni_err})
#             print(f"âŒ è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–å¤±è´¥: {vni_err}")
#     except Exception as e:
#         print(f"âŒ è¶Šå—æŒ‡æ•°æ¨¡å—å¼‚å¸¸: {e}")
#         all_status_logs.append({'name': 'vni_module', 'status': False, 'error': str(e)})

#     # [Step 4.5] å¤„ç† Aè‚¡æŒ‡æ•° (æ–°å¢é€»è¾‘)
#     # ä» combined_macro ä¸­æå–ï¼Œå¹¶è®¡ç®—å‡çº¿
#     ashare_list = combined_macro.get("market_klines", {}).pop("Aè‚¡æŒ‡æ•°", None) # Pop to remove from raw macro data
#     if ashare_list:
#         print(f"\n[Step 4.5] âš¡ æ­£åœ¨è®¡ç®— Aè‚¡æŒ‡æ•° å‡çº¿...")
#         # ashare_list æ˜¯æ‰å¹³åˆ—è¡¨: [{date, name, close...}, ...]
#         # æŒ‰ name åˆ†ç»„å¤„ç†
#         try:
#             # Sort by name first for groupby
#             ashare_list.sort(key=lambda x: x['name'])
#             for name, group in groupby(ashare_list, key=lambda x: x['name']):
#                 records = list(group)
#                 # Sort by date
#                 records.sort(key=lambda x: x['date'])
                
#                 df_ashare = pd.DataFrame(records)
#                 df_ashare['date'] = pd.to_datetime(df_ashare['date'])
                
#                 # Ensure numeric columns
#                 cols = ['close', 'open', 'high', 'low', 'volume']
#                 for c in cols:
#                     if c in df_ashare.columns:
#                         df_ashare[c] = pd.to_numeric(df_ashare[c], errors='coerce')
                    
#                 # Calculate MA
#                 ma_res = utils.calculate_ma(df_ashare)
#                 if ma_res:
#                     ma_data_dict["general"].extend(ma_res)
                
#                 # Prepare for K-line data storage (convert date back to string)
#                 df_ashare['date'] = df_ashare['date'].dt.strftime('%Y-%m-%d')
                
#                 # Update kline_data_dict
#                 if "data" not in kline_data_dict:
#                      kline_data_dict["data"] = {}
#                 kline_data_dict["data"][name] = df_ashare.to_dict(orient='records')
                
#                 print(f"   Processed {name}: {len(records)} records")
#         except Exception as e:
#             print(f"âš ï¸ Aè‚¡æŒ‡æ•°å¤„ç†å¤±è´¥: {e}")

#     # [Step 4.6] è·å– 60åˆ†é’ŸKçº¿ (ç§‘åˆ›50 & æ’ç”Ÿç§‘æŠ€)
#     print("\n[Step 4.6] è·å– 60åˆ†é’ŸKçº¿ (ç§‘åˆ›50 & æ’ç”Ÿç§‘æŠ€)...")
#     kcb50_dict = {}
    
#     # 1. ç§‘åˆ›50 60m
#     try:
#         kcb50_60m, err = fetch_data_core.fetch_kcb50_60m()
#         if kcb50_60m:
#             kcb50_dict["ç§‘åˆ›50_60åˆ†é’ŸKçº¿"] = kcb50_60m
#             all_status_logs.append({'name': 'ç§‘åˆ›50_60m', 'status': True, 'error': None})
#         else:
#             # [ä¿®å¤] å³ä½¿å¤±è´¥ä¹Ÿåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ï¼Œé˜²æ­¢å‰ç«¯ç¼ºå¤±Key
#             kcb50_dict["ç§‘åˆ›50_60åˆ†é’ŸKçº¿"] = []
#             all_status_logs.append({'name': 'ç§‘åˆ›50_60m', 'status': False, 'error': err})
#     except Exception as e:
#         print(f"âš ï¸ ç§‘åˆ›50_60m å¼‚å¸¸: {e}")
#         kcb50_dict["ç§‘åˆ›50_60åˆ†é’ŸKçº¿"] = []
        
#     # 2. è¿ç§»åŸ China ä¸‹çš„ç§‘åˆ›50å­—æ®µ
#     china_data = combined_macro.get("china", {})
#     keys_to_move = ["ç§‘åˆ›50å®æ—¶å¿«ç…§", "ç§‘åˆ›50èèµ„èåˆ¸", "ç§‘åˆ›50ä¼°å€¼"]
#     for k in keys_to_move:
#         if k in china_data:
#             kcb50_dict[k] = china_data.pop(k) # Move data
            
#     # 3. æ’ç”Ÿç§‘æŠ€ 60m
#     try:
#         hstech_60m, err = fetch_data_core.fetch_hstech_60m()
#         if "hk" not in combined_macro: combined_macro["hk"] = {}
        
#         if hstech_60m:
#             combined_macro["hk"]["æ’ç”Ÿç§‘æŠ€æŒ‡æ•°_60m"] = hstech_60m
#             all_status_logs.append({'name': 'æ’ç”Ÿç§‘æŠ€_60m', 'status': True, 'error': None})
#         else:
#             # [ä¿®å¤] å³ä½¿å¤±è´¥ä¹Ÿåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
#             combined_macro["hk"]["æ’ç”Ÿç§‘æŠ€æŒ‡æ•°_60m"] = []
#             all_status_logs.append({'name': 'æ’ç”Ÿç§‘æŠ€_60m', 'status': False, 'error': err})
#     except Exception as e:
#         print(f"âš ï¸ æ’ç”Ÿç§‘æŠ€_60m å¼‚å¸¸: {e}")
#         if "hk" not in combined_macro: combined_macro["hk"] = {}
#         combined_macro["hk"]["æ’ç”Ÿç§‘æŠ€æŒ‡æ•°_60m"] = []

#     # [Step 4.7] è·å–å…­å¤§é“¶è¡Œ Kçº¿ä¸å‡çº¿
#     print("\n[Step 4.7] è·å–å…­å¤§é“¶è¡Œæ—¥çº¿æ•°æ®...")
#     try:
#         bank_dfs = fetch_data_core.fetch_us_banks_daily()
#         for df in bank_dfs:
#             name = df['name'].iloc[0]
#             # è®¡ç®—å‡çº¿
#             ma_res = utils.calculate_ma(df)
#             if ma_res:
#                 ma_data_dict["general"].extend(ma_res)
            
#             # å­˜å‚¨ Kçº¿ (åˆ‡ç‰‡)
#             cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=REPORT_DAYS)
#             df_slice = df[df['date'] >= cutoff_date].copy()
#             df_slice['date'] = df_slice['date'].dt.strftime('%Y-%m-%d')
            
#             if "data" not in kline_data_dict: kline_data_dict["data"] = {}
#             kline_data_dict["data"][name] = df_slice.to_dict(orient='records')
            
#             all_status_logs.append({'name': f"Bank_{name}", 'status': True, 'error': None})
            
#     except Exception as e:
#         print(f"âš ï¸ å…­å¤§é“¶è¡Œæ•°æ®è·å–å¼‚å¸¸: {e}")
#         all_status_logs.append({'name': 'US_Banks', 'status': False, 'error': str(e)})

#     print("\n[Step 5] æ•´åˆæ•°æ®å¹¶æ¸…æ´—...")
#     # ä¼ å…¥ kcb50_dict
#     final_data = merge_final_report(combined_macro, kline_data_dict, ma_data_dict, kcb50_data=kcb50_dict)
#     final_data = clean_and_round(final_data)

#     success_names = set(log['name'] for log in all_status_logs if log.get('status'))
#     cleaned_logs = []
#     for log in all_status_logs:
#         if log['status']:
#             cleaned_logs.append(log)
#         else:
#             if log['name'] not in success_names:
#                 cleaned_logs.append(log)
    
#     write_status_log(cleaned_logs, LOG_FILENAME)
    
#     # ç”ŸæˆæŠ€æœ¯ä¿¡å·æ‘˜è¦
#     signal_summary = generate_signals_summary(ma_data_dict)
#     print(signal_summary)

#     if save_compact_json(final_data, OUTPUT_FILENAME):
#         try:
#             email_subject = f"MarketRadarå…¨é‡æ—¥æŠ¥_{datetime.now(TZ_CN).strftime('%Y-%m-%d')}"
#             base_body = f"ç”Ÿæˆæ—¶é—´: {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\nåŒ…å«: å®è§‚, æ±‡ç‡, Kçº¿(Stock/VNI/ç§‘åˆ›50/Aè‚¡/é“¶è¡Œ), ä¿¡å·æ‰«æ(MyTT)\n\n"
            
#             email_body = generate_email_body_summary(cleaned_logs, signal_summary)
#             email_body = base_body + email_body
            
#             attachments = [OUTPUT_FILENAME, LOG_FILENAME]
            
#             MarketRadar.send_email(email_subject, email_body, attachments)
#         except Exception as e:
#             print(f"âš ï¸ é‚®ä»¶å‘é€è·³è¿‡æˆ–å¤±è´¥: {e}")

#     print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
#     # 2. æ–°å¢ï¼šè°ƒç”¨é£ä¹¦æ¨é€ (ç¡®ä¿ utils.py å·²ç»æ›´æ–°è¿‡)
#     print("\n[Step 6] æ­£åœ¨æ¨é€è‡³é£ä¹¦...")
#     feishu_url = os.environ.get("FEISHU_WEBHOOK_URL")
#     if feishu_url:
#          # æ³¨æ„ï¼šè¿™é‡Œçš„ final_data åŒ…å«äº† market_klines ç­‰æ‰€æœ‰ä¿¡æ¯
#         utils.send_to_feishu(feishu_url, final_data) 
#     else:
#         print("âš ï¸ æœªå‘ç° FEISHU_WEBHOOK_URLï¼Œè·³è¿‡é£ä¹¦æ¨é€")

# if __name__ == "__main__":
#     main()
