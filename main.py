import json
import os
import sys
import time
import math
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

import fetch_data
import MarketRadar
import utils
import scrape_economy_selenium

OUTPUT_FILENAME = "MarketRadar_Report.json"
LOG_FILENAME = "market_data_status.txt"
TZ_CN = ZoneInfo("Asia/Shanghai")

# å®šä¹‰æŠ¥å‘Šçš„æ—¶é—´èŒƒå›´ï¼ˆç”¨äºæˆªå–æœ€ç»ˆå±•ç¤ºçš„æ•°æ®ï¼‰
# è®¡ç®—å‡çº¿éœ€è¦æ›´é•¿çš„æ•°æ®ï¼Œä½†æŠ¥å‘Šåªå±•ç¤ºè¿‘æœŸ
REPORT_DAYS = 20

class NpEncoder(json.JSONEncoder):
    """
    ä¸“é—¨è§£å†³ 'Object of type int64 is not JSON serializable' é”™è¯¯çš„ç¼–ç å™¨
    """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NpEncoder, self).default(obj)

def print_banner():
    print(r"""
  __  __            _        _   ____          _            
 |  \/  | __ _ _ __| | _____| |_|  _ \ __ _ __| | __ _ _ __ 
 | |\/| |/ _` | '__| |/ / _ \ __| |_) / _` / _` |/ _` | '__|
 | |  | | (_| | |  |   <  __/ |_|  _ < (_| (_| | (_| | |   
 |_|  |_|\__,_|_|  |_|\_\___|\__|_| \_\__,_\__,_|\__,_|_|   
                                                            
    """)

def clean_and_round(data):
    if isinstance(data, dict):
        return {k: clean_and_round(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [clean_and_round(x) for x in data]
    elif isinstance(data, float):
        if math.isnan(data) or math.isinf(data):
            return None
        return round(data, 2)
    elif isinstance(data, (np.int64, np.int32)):
        return int(data)
    else:
        return data

def deep_merge(dict1, dict2):
    result = dict1.copy()
    for key, value in dict2.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
    return result

def merge_final_report(macro_data_combined, kline_data_dict, ma_data_dict):
    """
    æ•´åˆæ‰€æœ‰æ¨¡å—çš„æ•°æ®
    ma_data_dict: {"general": [...], "commodities": [...]}
    """
    merged = {
        "meta": kline_data_dict.get("meta", {}),
        "æŠ€æœ¯åˆ†æ": {
            "æŒ‡æ•°+ä¸ªè‚¡æ—¥å‡çº¿": ma_data_dict.get("general", []),
            "å¤§å®—å•†å“": ma_data_dict.get("commodities", [])
        },
        "market_fx": macro_data_combined.get("market_fx", {}),
        "china": macro_data_combined.get("china", {}),
        "usa": macro_data_combined.get("usa", {}),
        "japan": macro_data_combined.get("japan", {}),
        "hk": macro_data_combined.get("hk", {}), 
        "market_klines": kline_data_dict.get("data", {})
    }
    
    merged["meta"]["generated_at"] = datetime.now(TZ_CN).strftime("%Y-%m-%d %H:%M:%S")
    merged["meta"]["description"] = "MarketRadar Consolidated Report (Selenium Macro + Online FX + Klines)"
    
    return merged

def save_compact_json(data, filename):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('{\n')
            keys = list(data.keys())
            for i, key in enumerate(keys):
                val = data[key]
                f.write(f'    "{key}": ')
                if isinstance(val, dict):
                    f.write('{\n')
                    sub_keys = list(val.keys())
                    for j, sub_key in enumerate(sub_keys):
                        sub_val = val[sub_key]
                        f.write(f'        "{sub_key}": ')
                        if isinstance(sub_val, list):
                            f.write('[\n')
                            for k, item in enumerate(sub_val):
                                # ä½¿ç”¨ NpEncoder è§£å†³ int64 åºåˆ—åŒ–é”™è¯¯
                                item_str = json.dumps(item, ensure_ascii=False, cls=NpEncoder)
                                comma = "," if k < len(sub_val) - 1 else ""
                                f.write(f'            {item_str}{comma}\n')
                            f.write('        ]')
                        else:
                            f.write(json.dumps(sub_val, ensure_ascii=False, cls=NpEncoder))
                        if j < len(sub_keys) - 1: f.write(',\n')
                        else: f.write('\n')
                    f.write('    }')
                else:
                    f.write(json.dumps(val, ensure_ascii=False, cls=NpEncoder))
                if i < len(keys) - 1: f.write(',\n')
                else: f.write('\n')
            f.write('}')
        print(f"\nâœ… æˆåŠŸ! æŠ¥å‘Šå·²å†™å…¥ {filename}")
        return True
    except Exception as e:
        print(f"\nâŒ å†™å…¥å¤±è´¥: {e}")
        return False

def write_status_log(logs, filename):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"MarketRadar Data Fetch Log - {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n")
            
            for log in logs:
                status_str = "[PASS]" if log['status'] else "[FAIL]"
                timestamp = datetime.now(TZ_CN).strftime('%H:%M:%S')
                line = f"[{timestamp}] {status_str} {log['name']}"
                if not log['status'] and log['error']:
                    line += f" | Error: {log['error']}"
                f.write(line + "\n")
        print(f"ğŸ“ çŠ¶æ€æ—¥å¿—å·²å†™å…¥: {filename}")
        return True
    except Exception as e:
        print(f"âŒ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
        return False

def generate_signals_summary(ma_data_dict):
    """
    ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¿¡å·æ‘˜è¦
    """
    lines = []
    
    # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
    all_ma = ma_data_dict.get("general", []) + ma_data_dict.get("commodities", [])
    
    signals_found = False
    lines.append("\nğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡ä¿¡å·æ‰«æ:")
    lines.append("-" * 30)
    
    for item in all_ma:
        # [ä¿®æ”¹] ä¼˜å…ˆè·å– "åç§°" (ä¸­æ–‡Key)
        name = item.get('åç§°', item.get('name', 'Unknown'))
        signals = item.get('Signals', [])
        
        # [ä¿®æ”¹] è¿‡æ»¤æ‰ "æ— ç‰¹æ®ŠæŠ€æœ¯å½¢æ€"ï¼Œä¸åœ¨æ‘˜è¦ä¸­æ˜¾ç¤º
        active_signals = [s for s in signals if s != "æ— ç‰¹æ®ŠæŠ€æœ¯å½¢æ€"]
        
        if active_signals:
            signals_found = True
            lines.append(f"ğŸ”´ [{name}]: {', '.join(active_signals)}")
            
    if not signals_found:
        lines.append("ä»Šæ—¥æ— ç‰¹æ®ŠæŠ€æœ¯ä¿¡å·ã€‚")
        
    return "\n".join(lines)

def generate_email_body_summary(logs, signal_summary):
    lines = ["ğŸ“Š æ•°æ®è·å–çŠ¶æ€æ±‡æ€»:"]
    lines.append("-" * 30)
    
    success_count = sum(1 for l in logs if l['status'])
    fail_count = sum(1 for l in logs if not l['status'])
    
    lines.append(f"æ€»è®¡: {len(logs)} | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}")
    lines.append("")
    
    for log in logs:
        status_icon = "âœ…" if log['status'] else "âŒ"
        lines.append(f"{status_icon} {log['name']}")
    
    lines.append("\n" + signal_summary)
    
    return "\n".join(lines)

def parse_chinese_date(date_str):
    """
    è§£æ 'YYYYå¹´MMæœˆDDæ—¥' æˆ– 'YYYY-MM-DD' æ ¼å¼çš„æ—¥æœŸ
    """
    try:
        if 'å¹´' in str(date_str):
            return datetime.strptime(str(date_str).strip(), '%Yå¹´%mæœˆ%dæ—¥')
        return pd.to_datetime(date_str)
    except:
        return pd.to_datetime(date_str, errors='coerce')

def main():
    start_time = time.time()
    print_banner()
    print("ğŸš€ MarketRadar å¯åŠ¨ä¸»ç¨‹åº (Integrated Version)...")
    
    all_status_logs = []

    print("\n[Step 1/4] è·å–æ±‡ç‡ä¸å›½å€ºæ•°æ® (fetch_data)...")
    try:
        base_macro, logs_fx = fetch_data.get_market_fx_and_bonds()
        all_status_logs.extend(logs_fx)
    except Exception as e:
        print(f"âŒ fetch_data å¤±è´¥: {e}")
        base_macro = {"market_fx": {}, "china": {}, "usa": {}, "japan": {}}
        all_status_logs.append({'name': 'fetch_data_module', 'status': False, 'error': str(e)})

    print("\n[Step 2/4] æŠ“å–å®è§‚ç»æµæŒ‡æ ‡ (Selenium)...")
    try:
        selenium_macro, logs_selenium = scrape_economy_selenium.get_macro_data()
        all_status_logs.extend(logs_selenium)
    except Exception as e:
        print(f"âŒ Selenium æŠ“å–å¤±è´¥ (å¯èƒ½æ˜¯ç¯å¢ƒé—®é¢˜): {e}")
        selenium_macro = {}
        all_status_logs.append({'name': 'selenium_module', 'status': False, 'error': str(e)})

    combined_macro = deep_merge(base_macro, selenium_macro)

    print("\n[Step 3/4] è·å– Kçº¿æ•°æ® & è®¡ç®—å‡çº¿ & æŠ€æœ¯æŒ‡æ ‡...")
    try:
        kline_result, logs_klines = MarketRadar.get_all_kline_data()
        all_status_logs.extend(logs_klines)
        
        kline_data_dict = {"meta": kline_result.get("meta"), "data": kline_result.get("data")}
        ma_data_dict = kline_result.get("ma_data", {"general": [], "commodities": []})
        
        count_general = len(ma_data_dict.get("general", []))
        count_comm = len(ma_data_dict.get("commodities", []))
        print(f"âœ… è·å–åˆ°å‡çº¿æ•°æ®: é€šç”¨ {count_general} æ¡, å¤§å®—å•†å“ {count_comm} æ¡")
    except Exception as e:
        print(f"âŒ è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
        kline_data_dict = {"meta": {}, "data": {}}
        ma_data_dict = {"general": [], "commodities": []}
        all_status_logs.append({'name': 'kline_module', 'status': False, 'error': str(e)})

    # [Step 3.5] å¤„ç†æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•° (æ¢å¤å¹¶ä¿®å¤é€»è¾‘)
    # ç­–ç•¥: æŠ“å–æ—¶ä½¿ç”¨é•¿æ•°æ®(å·²ç”±selenium_coreå®Œæˆ) -> è®¡ç®—å‡çº¿ -> åˆ‡ç‰‡ä¿ç•™æœ€è¿‘20å¤©æ”¾å…¥æŠ¥å‘Š
    hshci_key = "æ’ç”ŸåŒ»ç–—ä¿å¥æŒ‡æ•°"
    hk_data = combined_macro.get("hk", {})
    
    # æ¸…ç†: å¦‚æœ market_klines é‡Œæœ‰(æ¥è‡ªAkShareå¤±è´¥çš„ç©ºæ•°æ®), å…ˆåˆ æ‰
    if "data" in kline_data_dict and kline_data_dict["data"]:
        if hshci_key in kline_data_dict["data"]:
            del kline_data_dict["data"][hshci_key]
            print(f"ğŸ§¹ å·²ä» market_klines å­—æ®µç§»é™¤ {hshci_key} (ä»…ä¿ç•™ hk å­—æ®µæ•°æ®ï¼Œé˜²æ­¢åŒä»½è¾“å‡º)")

    if hshci_key in hk_data and hk_data[hshci_key]:
        print(f"\n[Step 3.5] âš¡ æ­£åœ¨åŸºäº Selenium æ•°æ®è®¡ç®— {hshci_key} å‡çº¿...")
        try:
            raw_data = hk_data[hshci_key]
            df_hshci = pd.DataFrame(raw_data)
            
            # 1. æ—¥æœŸæ ‡å‡†åŒ– (å¤„ç†ä¸­æ–‡æ—¥æœŸ)
            if 'æ—¥æœŸ' in df_hshci.columns:
                df_hshci['date'] = df_hshci['æ—¥æœŸ'].apply(parse_chinese_date)
            elif 'date' in df_hshci.columns:
                df_hshci['date'] = pd.to_datetime(df_hshci['date'])
            
            # 2. æ•°å€¼è½¬æ¢
            df_hshci['name'] = hshci_key
            for col in ['close', 'open', 'high', 'low', 'volume']:
                if col in df_hshci.columns:
                    df_hshci[col] = pd.to_numeric(df_hshci[col], errors='coerce')

            # 3. è®¡ç®—å‡çº¿ (åŸºäºå®Œæ•´å†å²æ•°æ®)
            if 'date' in df_hshci.columns:
                 # è®¡ç®—å‡çº¿
                 hshci_ma_list = utils.calculate_ma(df_hshci)
                 if hshci_ma_list:
                     ma_data_dict["general"].extend(hshci_ma_list)
                     print(f"âœ… {hshci_key} å‡çº¿è®¡ç®—å®Œæˆ")
                 
                 # 4. æ•°æ®åˆ‡ç‰‡ (ä»…ä¿ç•™æœ€è¿‘ 20 å¤©ï¼Œä¿®å¤"æ—¶é—´åŒºé—´å¤ªé•¿"é—®é¢˜)
                 cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=REPORT_DAYS)
                 df_slice = df_hshci[df_hshci['date'] >= cutoff_date].copy()
                 df_slice['date'] = df_slice['date'].dt.strftime('%Y-%m-%d')
                 
                 # æ›¿æ¢ combined_macro ä¸­çš„é•¿æ•°æ®ä¸ºåˆ‡ç‰‡åçš„çŸ­æ•°æ®
                 sliced_records = df_slice.to_dict(orient='records')
                 combined_macro['hk'][hshci_key] = sliced_records
                 print(f"âœ‚ï¸ {hshci_key} æ•°æ®å·²åˆ‡ç‰‡ (ä¿ç•™æœ€è¿‘ {len(sliced_records)} æ¡)")

        except Exception as e_ma:
             print(f"âš ï¸ {hshci_key} å‡çº¿è®¡ç®—æˆ–åˆ‡ç‰‡å¤±è´¥: {e_ma}")

    print("\n[Step 4/4] è·å–è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•° (Investing.com)...")
    try:
        vni_data, vni_err = fetch_data.fetch_vietnam_index_klines()
        if vni_data:
            if "data" not in kline_data_dict or kline_data_dict["data"] is None:
                kline_data_dict["data"] = {}
                
            kline_data_dict["data"]["è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"] = vni_data
            
            try:
                df_vni = pd.DataFrame(vni_data)
                df_vni['name'] = "è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°"
                
                vni_ma_list = utils.calculate_ma(df_vni)
                if vni_ma_list:
                    ma_data_dict["general"].extend(vni_ma_list)
                    print(f"âœ… è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–æˆåŠŸ ({len(vni_data)} æ¡è®°å½•) & å‡çº¿å·²è®¡ç®—")
                else:
                    print(f"âœ… è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–æˆåŠŸ ({len(vni_data)} æ¡è®°å½•) (å‡çº¿è®¡ç®—æ— ç»“æœ)")
                
                all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': None})
                
            except Exception as e_ma:
                print(f"âš ï¸ è¶Šå—æ•°æ®è·å–æˆåŠŸä½†å‡çº¿è®¡ç®—å¤±è´¥: {e_ma}")
                all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': True, 'error': f"MA Error: {e_ma}"})
            
        else:
            all_status_logs.append({'name': 'è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°', 'status': False, 'error': vni_err})
            print(f"âŒ è¶Šå—èƒ¡å¿—æ˜æŒ‡æ•°è·å–å¤±è´¥: {vni_err}")
    except Exception as e:
        print(f"âŒ è¶Šå—æŒ‡æ•°æ¨¡å—å¼‚å¸¸: {e}")
        all_status_logs.append({'name': 'vni_module', 'status': False, 'error': str(e)})

    print("\n[Step 5] æ•´åˆæ•°æ®å¹¶æ¸…æ´—...")
    final_data = merge_final_report(combined_macro, kline_data_dict, ma_data_dict)
    final_data = clean_and_round(final_data)

    success_names = set(log['name'] for log in all_status_logs if log.get('status'))
    cleaned_logs = []
    for log in all_status_logs:
        if log['status']:
            cleaned_logs.append(log)
        else:
            if log['name'] not in success_names:
                cleaned_logs.append(log)
    
    write_status_log(cleaned_logs, LOG_FILENAME)
    
    # ç”ŸæˆæŠ€æœ¯ä¿¡å·æ‘˜è¦
    signal_summary = generate_signals_summary(ma_data_dict)
    print(signal_summary)

    if save_compact_json(final_data, OUTPUT_FILENAME):
        try:
            email_subject = f"MarketRadarå…¨é‡æ—¥æŠ¥_{datetime.now(TZ_CN).strftime('%Y-%m-%d')}"
            base_body = f"ç”Ÿæˆæ—¶é—´: {datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}\nåŒ…å«: å®è§‚(Selenium), æ±‡ç‡/å›½å€º(Online), Kçº¿(Stock/VNI/ç§‘åˆ›50), ä¿¡å·æ‰«æ(MyTT)\n\n"
            
            email_body = generate_email_body_summary(cleaned_logs, signal_summary)
            email_body = base_body + email_body
            
            attachments = [OUTPUT_FILENAME, LOG_FILENAME]
            
            MarketRadar.send_email(email_subject, email_body, attachments)
        except Exception as e:
            print(f"âš ï¸ é‚®ä»¶å‘é€è·³è¿‡æˆ–å¤±è´¥: {e}")

    print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")

if __name__ == "__main__":
    main()